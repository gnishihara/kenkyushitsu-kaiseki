[
  {
    "objectID": "fish-demo01.html",
    "href": "fish-demo01.html",
    "title": "有川湾の藻場で観測した魚類",
    "section": "",
    "text": "パッケージの読み込み\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ purrr::%||%()   masks base::%||%()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nデータの読み込み\nデータはこちらからダウンロードできます。\nfish_data - fish_data.csv\n\nfilename = \"fish_data - fish_data.csv\"\nfish = read_csv(filename)\n\n\nfish"
  },
  {
    "objectID": "standardize_figures.html",
    "href": "standardize_figures.html",
    "title": "研究室の作図法",
    "section": "",
    "text": "研究室の図を評価化してほしいので次のコードを紹介します。 ではパッケージの読み込みと作図環境を整えましょう。",
    "crumbs": [
      "研究室用",
      "研究室の作図法"
    ]
  },
  {
    "objectID": "standardize_figures.html#load-packages",
    "href": "standardize_figures.html#load-packages",
    "title": "研究室の作図法",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ purrr::%||%()   masks base::%||%()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(patchwork)\nlibrary(ggpubr)\nlibrary(ggtext)\nlibrary(lemon)\n\n\nAttaching package: 'lemon'\n\nThe following object is masked from 'package:purrr':\n\n    %||%\n\nThe following object is masked from 'package:base':\n\n    %||%\n\nlibrary(ggrepel)\nlibrary(showtext)\n\nLoading required package: sysfonts\nLoading required package: showtextdb\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.11.60\nEnabled features: fontconfig, freetype, fftw, heic, lcms, pango, webp, x11\nDisabled features: cairo, ghostscript, raw, rsvg\nUsing 32 threads\n\n\nこれで Google fonts からフォントをダウンロードできます。 このとき、ネットへつなげる必要があります。 Noto Sans は英語のフォントですが、 Noto Sans JP は日本語のフォントです。 図に日本語を入れたい場合は、日本語フォントの読み込みが必要です。\n\nfont_add_google(name = \"Noto Sans\",\n                family = \"notosans\")\nfont_add_google(name = \"Noto Sans JP\",\n                family = \"notosansjp\")\n\nフォントを読み込んだら、図のデフォルトの設定を変えます。 ここでは ggpubr のパッケージから theme_pubr() をデフォルト に設定しますが、そのときに、 フォントサイズ base_size と 使用するフォント base_family を していします。 theme_set() はデフォルトテーマ設定用関数です。\n\ntheme_pubr(base_size = 12,\n           base_family = \"notosans\") |&gt; \n  theme_set()\n\n次に theme_pubr() をちょっとだけ修正します。 図の背景を白に設定し、図に黒い枠をつけます。 黒い枠を追加したので、軸の線を外します。 凡例の背景とタイトルも外します。\n\ntheme_pubr() |&gt; \n  theme_replace(\n    panel.background = element_rect(\n      fill = \"white\",\n      color = \"black\",\n      linewidth = 1\n    ),\n    axis.line = element_blank(),\n    legend.background = element_blank(),\n    legend.title = element_blank()\n  )\n\nテーマの設定を適応するためには、次のコードを実行します。\n\nshowtext_auto()",
    "crumbs": [
      "研究室用",
      "研究室の作図法"
    ]
  },
  {
    "objectID": "standardize_figures.html#データの読み込み",
    "href": "standardize_figures.html#データの読み込み",
    "title": "研究室の作図法",
    "section": "データの読み込み",
    "text": "データの読み込み\n今回の図は形上湾の予備データを用います。 データは研究室のサーバにあるので、次のようコードを実行して、 データを読み込みましょう。 read_rds() の後の print() は読み込んだデータの最初の３行を コンソールに表示するために追加したが、なくてもいいです。\n\nfname = \"~/Lab_Data/standardized_figures/allcarbondata.rds\"\ndset = read_rds(fname) |&gt; print(n = 3)\n\n# A tibble: 107 × 8\n  Core_length   LOI Mapping  Type    dry_mud    OC carbon_density carbon_content\n        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1        52.5  7.97 Seagrass Seagra…    29.6  2.38         0.0171          0.704\n2        52.5  9.43 Seagrass Seagra…    30.3  2.94         0.0217          0.890\n3        52.5  7.29 Seagrass Seagra…    30.9  2.13         0.0160          0.658\n# ℹ 104 more rows",
    "crumbs": [
      "研究室用",
      "研究室の作図法"
    ]
  },
  {
    "objectID": "standardize_figures.html#作図例その-1",
    "href": "standardize_figures.html#作図例その-1",
    "title": "研究室の作図法",
    "section": "作図例その 1",
    "text": "作図例その 1\n図の軸にはタイトルと関係する単位を必ず追加しましょう。 もっと合理的な方法として、軸タイトルのオブジェクトをつくことです。 ここでは、xlabel と ylabel のオブジェクトを作りました。 x 軸に当てた変数には単位はありません。 でも、y 軸の変数には単位が必要です。\nまずは箱ひげ図を作ります。 ところが Figure 1 の見た目がわるい。 y 軸に範囲を調整して、全てのデータを囲むようにしましょう。 2群のデータしかないので、わざわざいろで分ける必要はないですが、 せっかくなので、色分けもしましょう (Figure 2)。\n\nxlabel = \"State\"\nylabel = \"Organic carbon content (%)\"\nggplot() + \n  geom_boxplot(\n    aes(\n      x = Mapping,\n      y = OC,\n    ),\n    data = dset\n  ) +\n  scale_x_discrete(name = xlabel) +\n  scale_y_continuous(name = ylabel)\n\n\n\n\n\n\n\nFigure 1: 初期の図\n\n\n\n\n\nここで Figure 1 を修正しました。 軸と凡例が重複しているので、どちらかを外してもいいとおもいます。\n\nxlabel = \"State\"\nylabel = \"Organic carbon content (%)\"\nylimits = c(0, 6)\nggplot() + \n  geom_boxplot(\n    aes(\n      x = Mapping,\n      y = OC,\n      color = Mapping\n    ),\n    data = dset\n  ) +\n  scale_x_discrete(name = xlabel) +\n  scale_y_continuous(name = ylabel,　limits = ylimits) +\n  scale_color_viridis_d(end = 0.9) \n\n\n\n\n\n\n\nFigure 2: 色の追加、縦軸範囲の調整\n\n\n\n\n\nFigure 3 は Figure 2 から凡例を外しました。 geom_boxplot() に show.legend = F を渡せば、 その geom の凡例を隠すようにできます。\n\nxlabel = \"State\"\nylabel = \"Organic carbon content (%)\"\nylimits = c(0, 6)\nggplot() + \n  geom_boxplot(\n    aes(\n      x = Mapping,\n      y = OC,\n      color = Mapping\n    ),\n    data = dset,\n    show.legend = F\n  ) +\n  scale_x_discrete(name = xlabel) +\n  scale_y_continuous(name = ylabel,　limits = ylimits) +\n  scale_color_viridis_d(end = 0.9) \n\n\n\n\n\n\n\nFigure 3: 凡例を外した。\n\n\n\n\n\nでは Figure 3 が完成したので、この図をファイルに保存します。 ここで重要なのは保存方法です。 できる限りフォントの大きさを変えずに図を保存しましょう。\n最もいい方法は、pdf ファイルへの保存です。 その後、png ファイルに変換すれば、とても整った使いやすい 画像ができあがります。\nまず保存したい図をオブジェクトに書き込みます。 この図は plot1 にいれました。\n\nxlabel = \"State\"\nylabel = \"Organic carbon content (%)\"\nylimits = c(0, 6)\nplot1 =\n  ggplot() +\n  geom_boxplot(aes(x = Mapping,\n                   y = OC,\n                   color = Mapping),\n               data = dset,\n               show.legend = F) +\n  scale_x_discrete(name = xlabel) +\n  scale_y_continuous(name = ylabel,　limits = ylimits) +\n  scale_color_viridis_d(end = 0.9)\n\n保存先のファイル名を指定します。\n\npdfname = \"figure01.pdf\"\npngname = str_replace(pdfname, \"pdf\", \"png\")\n\n次に図の寸法を指定します。 研究室用パッケージ gnnlab に aseries() の関数があります。 この関数は A判の用紙サイズの寸法を返してくれます。\n例えば A4 なら次のように関数をつかいます。\n\ngnnlab::aseries(4)\n\n width height \n   210    297 \n\n\nレポートなどを作るなら、A7 (74 mm × 105 mm) はちょうどいいサイズです。 まず、PDF ファイルとして保存します。\n\nwh = gnnlab::aseries(7)\n\nggsave(filename = pdfname, \n       plot = plot1,\n       width = wh[1],\n       height = wh[1],\n       units = \"mm\")\n\nその次に、PDF ファイルを PNG ファイルに変換します。 このとき、PDF ファイルを読み込んだときに、解像度を指定できます。 デフォルトは density = 300 ですが、もしも density = 150 の PNG ファイルに保存したいのであれば、次のように実行しましょう。\n\nimage_read_pdf(pdfname, density = 150) |&gt; \n  image_write(pngname)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: PNG ファイルに保存した図 (density = 150)。",
    "crumbs": [
      "研究室用",
      "研究室の作図法"
    ]
  },
  {
    "objectID": "standardize_figures.html#作図その-2",
    "href": "standardize_figures.html#作図その-2",
    "title": "研究室の作図法",
    "section": "作図その 2",
    "text": "作図その 2\n図に特殊な記号や文字の書式を工夫したいなら、ggtext のパッケージをつかいましょう。 次の図は 有機炭素の面積あたりの量とその t 検定の結果を示す図を作ります。\n軽いデータ処理をします。\n\ndset = dset |&gt; \n  mutate(ccm2 = carbon_content / ((1.5 / 100)^2 * pi)) |&gt; \n  mutate(ccm2 = ccm2 / 1000)\n\n次は t 検定を実施します。\n\nt.test(ccm2 ~ Mapping, data = dset) \n\n\n    Welch Two Sample t-test\n\ndata:  ccm2 by Mapping\nt = -3.2519, df = 47.81, p-value = 0.002105\nalternative hypothesis: true difference in means between group Sand and group Seagrass is not equal to 0\n95 percent confidence interval:\n -0.4762400 -0.1123041\nsample estimates:\n    mean in group Sand mean in group Seagrass \n              1.050607               1.344879 \n\n\nt検定の結果を後で使いたいので、次のように結果を tibble 化をします。\n\ntout = \n  t.test(ccm2 ~ Mapping, data = dset) |&gt; \n  broom::tidy() |&gt; \n  print()\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   -0.294      1.05      1.34     -3.25 0.00211      47.8   -0.476    -0.112\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nt 検定の結果の文字列を作って、tibble にします。\n\ntextout = \n  sprintf(\"t&lt;sub&gt;(%0.1f)&lt;/sub&gt; = %0.2f (P = %0.4f)\",\n          tout$parameter,\n          abs(tout$statistic),\n          tout$p.value) |&gt; print()\n\n[1] \"t&lt;sub&gt;(47.8)&lt;/sub&gt; = 3.25 (P = 0.0021)\"\n\ntextdf1 = \n  tibble(x = \"Seagrass\",\n         y = 3,\n         l = textout)\n\n\n\n\n\n\n\n出力文字列の書式設定について\n\n\n\nsprintf() は文字列を出力するための関数ですが、変数を渡して 他の文字列と組み合わせることができます。 このとき % と変換指定子を使って書式の設定をします。\n構造は次のよう通りです： %(フラグ)(フィールド幅.精度)変換指定子\nフラグ\n\n- 左詰め（デフォルトは右詰め）\n+ 符号付き（デフォルトは負のときのみ）\n0 フィールド幅未満の場合はゼロで埋める（）\n\n変更指定子\n\nd 整数\nf 実数\ns 文字列\n\nちなみに % を出力したいときは、 %% を記述しましょう。\n\n\n\nxlabel = \"State\"\nylabel = \"Organic carbon density (g m&lt;sup&gt;-2&lt;/sup&gt;)\"\nplot2 = \n  ggplot() + \n  geom_boxplot(\n    aes(\n      x = Mapping,\n      y = ccm2,\n      color = Mapping\n    ),\n    data = dset,\n    show.legend = F\n  ) +\n  geom_point(\n    aes(\n      x = Mapping,\n      y = ccm2,\n      fill = Mapping\n    ),\n    data = dset,\n    shape = 21,\n    size = 3,\n    alpha = 0.25,\n    stroke = 0.1,\n    color = \"white\",\n    position = position_jitter(0.2),\n    show.legend = F\n  ) +\n  geom_richtext(\n    aes(\n      x = x, \n      y = y,\n      label = l\n    ), \n    data = textdf1,\n    family = \"notosans\",\n    color = \"transparent\",\n    fill = \"transparent\",\n    text.color = \"black\",\n    size = 3, \n    nudge_x = -0.5,\n    vjust = 1,\n    hjust = 0.5\n  ) +\n  scale_x_discrete(name = xlabel) +\n  scale_y_continuous(name = ylabel,\n                     limits = c(0, 3)) +\n  scale_color_viridis_d(end = 0.9) +\n  scale_fill_viridis_d(end = 0.9) +\n  theme(\n    axis.title.y = element_markdown()\n  )\n\n\n\n\n\n\n\n図の中の文字列の書式について\n\n\n\nggtext パッケージを読み込めば、 ggplot に markdown, HTML, CSS の一部の機能を使えます。 ここでは文字を上付きや下付きにするために使っています。\n\n下付き CO&lt;sub&gt;2&lt;/sub&gt; は CO2 になります。\n上付き m&lt;sup&gt;-2&lt;/sup&gt; は m-2 になります。\nイタリック体 &lt;i&gt;Sargassum horneri&lt;/i&gt;　は Sargassum horneri\n改行は &lt;br&gt; のタグでできます。\n\n\n\n図を保存して、解像度 300 で読みここ見ます。\n\npdfname = \"figure02.pdf\"\npngname = str_replace(pdfname, \"pdf\", \"png\")\n\ngnnlab::aseries(4) # A4 サイズ\n\n width height \n   210    297 \n\nwh = gnnlab::aseries(7)\n\nggsave(filename = pdfname, \n       plot = plot2,\n       width = wh[1],\n       height = wh[1],\n       units = \"mm\")\n\nimage_read_pdf(pdfname, density = 300) |&gt; \n  image_write(pngname)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: PNG ファイルに保存した図 (density = 300)。\n\n\n\n図を発表スライドに合わせたいなら、wh = c(192.0, 108.0) にします、\n\npdfname = \"figure03.pdf\"\npngname = str_replace(pdfname, \"pdf\", \"png\")\n\nwh = c(192.0, 108,0)\n\nggsave(filename = pdfname, \n       plot = plot2,\n       width = wh[1],\n       height = wh[2],\n       units = \"mm\")\n\nimage_read_pdf(pdfname, density = 300) |&gt; \n  image_write(pngname)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: スライド１面に合わせた PNG ファイルに保存した図 (density = 300)。\n\n\n\nスライドの半面に合わせた図。\n\npdfname = \"figure04.pdf\"\npngname = str_replace(pdfname, \"pdf\", \"png\")\n\nwh = c(192.0, 108,0)\n\nggsave(filename = pdfname, \n       plot = plot2,\n       width = wh[1]/2,\n       height = wh[2],\n       units = \"mm\")\n\nimage_read_pdf(pdfname, density = 300) |&gt; \n  image_write(pngname)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: スライド半面に合わせた PNG ファイルに保存した図 (density = 300)。",
    "crumbs": [
      "研究室用",
      "研究室の作図法"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#mean-and-expectation",
    "href": "descriptive_statistics-slide.html#mean-and-expectation",
    "title": "記述統計量",
    "section": "Mean and expectation",
    "text": "Mean and expectation\nMean, average (平均値) or Expectation（期待値）\nAdd all values \\((x_i)\\) and divide by the total number of samples \\((i)\\). The population mean is \\(\\mu\\) and the sample mean is \\(\\overline{X}\\).\n\\[\n\\mu \\equiv \\overline{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\] \\[\n\\text{平均値} = \\frac{\\text{資料の変量の総和}}{\\text{資料の個数}}\n\\]",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#deviation-residual",
    "href": "descriptive_statistics-slide.html#deviation-residual",
    "title": "記述統計量",
    "section": "Deviation, residual",
    "text": "Deviation, residual\n\nDeviation: 偏差\nResidual: 残渣\n\nData: x = {12, 19, 3, 8, 9, 10, 16, 9, 13, 7}\nMean or average\n\nmean(x)\n\n[1] 10.6\n\n\nDeviation or residual\n\nx - mean(x)\n\n [1]  1.4  8.4 -7.6 -2.6 -1.6 -0.6  5.4 -1.6  2.4 -3.6",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#linearity-of-the-expectation線形性",
    "href": "descriptive_statistics-slide.html#linearity-of-the-expectation線形性",
    "title": "記述統計量",
    "section": "Linearity of the expectation（線形性）",
    "text": "Linearity of the expectation（線形性）\n\\(a\\) is a constant (定数) and \\(X\\) is the random variable (確率変数).\n\\[\nE[aX] = aE[X]\n\\]\n\n\\(X\\): x = {12, 19, 3, 8, 9, 10, 16, 9, 13, 7}\n\\(aX = 5X\\): ax = 5 × {12, 19, 3, 8, 9, 10, 16, 9, 13, 7} = {90, 55, 20, 90, 55, 10, 30, 30, 40, 70}",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#linearity-of-the-expectation線形性-1",
    "href": "descriptive_statistics-slide.html#linearity-of-the-expectation線形性-1",
    "title": "記述統計量",
    "section": "Linearity of the expectation（線形性）",
    "text": "Linearity of the expectation（線形性）\nMultiplying a random variable by a constant shifts the mean of the random variable by the same magnitude (constant).\n\\[\nE[aX] = aE[X]\n\\]\n\nmean(x)\n\n[1] 10.6\n\n5 * mean(x)\n\n[1] 53\n\nmean(5 * x)\n\n[1] 53",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#monotonicity-単調性",
    "href": "descriptive_statistics-slide.html#monotonicity-単調性",
    "title": "記述統計量",
    "section": "Monotonicity (単調性)",
    "text": "Monotonicity (単調性)\n\\[\nX \\leq Y \\rightarrow E[X] \\leq E[Y]\n\\]\n\nx = {2,5,4}\ny = {9,7,9}\n3.6666667 ≤ 8.3333333",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#non-multiplicativity-非乗法性",
    "href": "descriptive_statistics-slide.html#non-multiplicativity-非乗法性",
    "title": "記述統計量",
    "section": "Non-multiplicativity (非乗法性)",
    "text": "Non-multiplicativity (非乗法性)\nIf the random variables \\(X\\) and \\(Y\\) are not independent (非独立性) of each other,\n\\[\nE[XY] \\neq E[X]\\cdot E[Y]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean(x * y)\n\n[1] 210.6681\n\nmean(x) * mean(y)\n\n[1] 206.8307",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#non-multiplicativity-非乗法性-1",
    "href": "descriptive_statistics-slide.html#non-multiplicativity-非乗法性-1",
    "title": "記述統計量",
    "section": "Non-multiplicativity (非乗法性)",
    "text": "Non-multiplicativity (非乗法性)\nIf the random variables are independent (独立性) of each other,\n\\[\nE[XY] = E[X]\\cdot E[Y]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean(x * y)\n\n[1] 199.3228\n\nmean(x) * mean(y)\n\n[1] 199.2973",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#variance-分散",
    "href": "descriptive_statistics-slide.html#variance-分散",
    "title": "記述統計量",
    "section": "Variance (分散)",
    "text": "Variance (分散)\nThe variance (分散) is the square of the difference of the random variable and \\(E[X]\\).\n\\[\n\\sigma^2\\equiv Var(X) = E[(X - E[X])^2] = E[X^2] - E[X]^2\n\\]\nIt is a measure of scale (スケール) and decribes the amount of scatter in the data.",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#variance分散",
    "href": "descriptive_statistics-slide.html#variance分散",
    "title": "記述統計量",
    "section": "Variance（分散）",
    "text": "Variance（分散）\n\\[\n\\sigma^2\\equiv Var(X) = E[(\\underbrace{X - E[X]}_{\\text{deviation}})^2] = E[X^2] - E[X]^2\n\\]",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#properties-of-the-variance分散の一般的な性質",
    "href": "descriptive_statistics-slide.html#properties-of-the-variance分散の一般的な性質",
    "title": "記述統計量",
    "section": "Properties of the variance（分散の一般的な性質）",
    "text": "Properties of the variance（分散の一般的な性質）\nVariance of a constant (定数) is zero\n\\[\nVar(a) = 0\n\\]\nScale invariance (スケール普遍性)\n\\[\nVar(aX+b) = a^2Var(X)\n\\]\nAdditivity of independent variables（独立な確立変数の和の分散）\n\\[\nVar(X+Y) = Var(X) + Var(Y)\n\\]",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#population-variance-母分散",
    "href": "descriptive_statistics-slide.html#population-variance-母分散",
    "title": "記述統計量",
    "section": "Population variance (母分散)",
    "text": "Population variance (母分散)\nWhen the population mean \\((\\mu)\\) is known, then the population variance (\\(\\sigma^2\\), 母分散) is\n\\[\n\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n\\left(x_i - \\mu\\right)^2\n\\] However, we usually do not know the population mean. So, we must calculate the sample variance (標本分散).",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#sample-variance-and-the-unbiased-sample-variance",
    "href": "descriptive_statistics-slide.html#sample-variance-and-the-unbiased-sample-variance",
    "title": "記述統計量",
    "section": "Sample variance and the unbiased sample variance)",
    "text": "Sample variance and the unbiased sample variance)\nThere are two ways to calculate the sample variance.\n\n\nSample variance (標本分散)\n\\[\n\\widehat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n\\left(x_i - \\overline{x}\\right)^2\n\\] The value of the sample variance \\((\\widehat{\\sigma}^2)\\) is smaller than the population variance (母分散). In otherwords, if \\(n\\) is small, then \\(\\widehat{\\sigma}^2 \\ll \\sigma^2\\).\n\nUnbiased sample variance (不偏標本分散)\n\\[\ns^2=\\frac{1}{n-1}\\sum_{i=1}^n\\left(x_i - \\overline{x}\\right)^2\n\\] When \\(n\\) is small, use the unbiased sample variance (不偏標本分散 \\((s^2)\\)).",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#unbiased-sample-variance-不偏標本分散",
    "href": "descriptive_statistics-slide.html#unbiased-sample-variance-不偏標本分散",
    "title": "記述統計量",
    "section": "Unbiased sample variance (不偏標本分散)",
    "text": "Unbiased sample variance (不偏標本分散)\n\nx = {4, 6, 2, 9, 3}\nmean: 4.8\ndeviation: x = {-0.8, 1.2, -2.8, 4.2, -1.8}\nn: 5\n\n\nz = x -  mean(x)\nn = length(z)\nsum(z^2) / (n - 1) # 数式で求めた値\n\n[1] 7.7\n\nvar(x)             # Rの固有関数,必ず不遍分散を計算する\n\n[1] 7.7",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#variance-is-a-measure-of-variation",
    "href": "descriptive_statistics-slide.html#variance-is-a-measure-of-variation",
    "title": "記述統計量",
    "section": "Variance is a measure of variation",
    "text": "Variance is a measure of variation\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nVar(aX+b) = a^2Var(X)\n\\]\n\\[\n\\begin{aligned}\nVar(X) &= 1\\\\\nVar(aX) &= 0.5\\\\\na^2Var(X) &= a^2 1 = 0.5\\\\\na &= \\sqrt{0.5} = \\frac{\\sqrt{2}}{2}\\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#variance-is-a-measure-of-variation-1",
    "href": "descriptive_statistics-slide.html#variance-is-a-measure-of-variation-1",
    "title": "記述統計量",
    "section": "Variance is a measure of variation",
    "text": "Variance is a measure of variation\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nVar(X+b) = Var(X)\n\\]\n\\[\n\\begin{aligned}\nVar(X) &= 1\\\\\nVar(X+b) &= 1\\\\\nVar(X) &= Var(X+b)\n\\end{aligned}\n\\]",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#standard-deviation-標準偏差",
    "href": "descriptive_statistics-slide.html#standard-deviation-標準偏差",
    "title": "記述統計量",
    "section": "Standard deviation (標準偏差)",
    "text": "Standard deviation (標準偏差)\nVariance (分散) describes how much the data is scattered around the expectation (mean). Since the sample variance is \\(\\sim\\sum(x - \\overline{x})^2\\), it cannot be directly compared with the mean.\nStandard deviation (標準偏差) (Std. Dev., S.D.)\n\\[\n\\sigma = \\sqrt{\\sigma^2} \\equiv \\sqrt{Var(X)}\n\\]\nThe standard deviation is the positive root of the variance. Both variance and standard deviation describe the scatter of the data. Which variance to use? \\(\\widehat{\\sigma}^2\\) or \\(s^2\\)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#use-the-unbiased-sample-variance",
    "href": "descriptive_statistics-slide.html#use-the-unbiased-sample-variance",
    "title": "記述統計量",
    "section": "Use the unbiased sample variance",
    "text": "Use the unbiased sample variance\n\\[\n\\text{Std. Dev.}=\\sqrt{s^2}=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_i -\\overline{x})^2}\n\\]",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#standard-error",
    "href": "descriptive_statistics-slide.html#standard-error",
    "title": "記述統計量",
    "section": "Standard error",
    "text": "Standard error\nThe standard error (標準誤差) describes the precision of a statistic. All statistics have a standard error.\n\\[\nS.E. = \\frac{s}{\\sqrt{n}}\n\\] The S.E. decreases when sample size increase!\n\\[\n\\lim_{n\\rightarrow\\infty} \\frac{s}{\\sqrt{n}} = 0\n\\]",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#median-中央値メディアン",
    "href": "descriptive_statistics-slide.html#median-中央値メディアン",
    "title": "記述統計量",
    "section": "Median (中央値・メディアン)",
    "text": "Median (中央値・メディアン)\n\n\nThe median (中央値・メディアン) is another statistic to describe data. It is the midpoint of data that is sorted from small to large values. When the number of data is odd, the median is the value at the midpoint. When the number of data is even, the median is the average of the two values nearest to the middle.\n\nset.seed(2020)\nx = sample(1:9, size = 5, replace = TRUE)\nsort(x)\n\n[1] 1 1 6 7 8\n\n\nThe value in the middle is 6 so the median is 6.\n\nmedian(x)\n\n[1] 6\n\n\n\n\nset.seed(2020)\nx = sample(1:9, size = 4, replace = TRUE)\nsort(x)\n\n[1] 1 6 7 8\n\nmedian(x)\n\n[1] 6.5\n\n\nThe two values near the middle are 6 and 7, so the median is \\((6 + 7) / 2 = 6.5\\).",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#mode-最頻値モード",
    "href": "descriptive_statistics-slide.html#mode-最頻値モード",
    "title": "記述統計量",
    "section": "Mode (最頻値・モード)",
    "text": "Mode (最頻値・モード)\nThe mode is the most common value in a dataset.\n\nset.seed(2020)\nx = sample(1:9, size = 100, replace = TRUE)\nz = table(x) %&gt;% as_tibble() %&gt;% arrange(desc(n))\nz\n\n# A tibble: 9 × 2\n  x         n\n  &lt;chr&gt; &lt;int&gt;\n1 8        15\n2 1        14\n3 2        14\n4 4        13\n5 6        12\n6 7        10\n7 3         8\n8 5         8\n9 9         6\n\n\nThe value 8 occurs 15 times, so it is the mode.",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#median-absolute-deviation-中央絶対偏差",
    "href": "descriptive_statistics-slide.html#median-absolute-deviation-中央絶対偏差",
    "title": "記述統計量",
    "section": "Median Absolute Deviation (中央絶対偏差)",
    "text": "Median Absolute Deviation (中央絶対偏差)\nThe median absolute deviation (MAD, 中央絶対偏差) is another measure of variation.\n\\[\nMAD = \\text{median}(|x_i - \\tilde{x}|)\n\\]\n\\(\\tilde{x}\\) is the median.\n\nmad = function(x) {\n  xtilde = median(x)\n  median(abs(x - xtilde))\n}\nx = rnorm(100)\nlist(mad = mad(x), sd = sd(x))\n\n$mad\n[1] 0.6588682\n\n$sd\n[1] 0.9770429",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#quantile-四分位数クォンタイル",
    "href": "descriptive_statistics-slide.html#quantile-四分位数クォンタイル",
    "title": "記述統計量",
    "section": "Quantile (四分位数・クォンタイル)",
    "text": "Quantile (四分位数・クォンタイル)\n\n\nThere are many ways to define the quantile (四分位数・クォンタイル). In any case, we first need to sort the values from smallest to largest. Then we separate the values in to four groups. The value used to separate the groups are called the quantile.\n\nset.seed(2020)\nx = sample(20:40, size = 9, replace = TRUE)\nz = sort(x)\nz\n\n[1] 20 23 25 27 29 31 32 36 36\n\n\n\n# 文科省の定義：\nN = length(z)\nQ1 = median(z[1:(floor(N/2))])\nQ2 = median(z)\nQ3 = median(z[(ceiling(N/2)+1):N])\nc(min(z), Q1, Q2, Q3, max(z))\n\n[1] 20 24 29 34 36\n\n\n\n# R では, Tukey の定義で計算します。\nquantile(z)\n\n  0%  25%  50%  75% 100% \n  20   25   29   32   36 \n\n\n\n\n\n\n\n\n\n\n\n\nIn this boxplot, the whiskers indicate the minimum and maximum values. The line in the center of the box is the median (i.e., second quantile, 第2四分位数). The bottom edge of the box is the first quantile (第1四分位数) and the top edge of the box is the third quantile (第3四分位数). The distance between the first and third quantile is called the Inter-Quantile Range (IQR, 四分位範囲).",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#quantile-in-r",
    "href": "descriptive_statistics-slide.html#quantile-in-r",
    "title": "記述統計量",
    "section": "Quantile in R",
    "text": "Quantile in R\n\n\n\nset.seed(2021)\nz = rpois(100, 10) \nquantile(z)\n\n  0%  25%  50%  75% 100% \n   3    7    9   12   25 \n\n\n\n\n\n\n\n\n\n\n\n\nIn the standard boxplot, the dots beyond the whiskers indicate outliers. The whiskers extend to the largest value within 1.5 times the IQR from the each edge.",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#data",
    "href": "descriptive_statistics-slide.html#data",
    "title": "記述統計量",
    "section": "Data",
    "text": "Data\n\n\n\n\n\n\n\n\n\n\n\n\nDuarte et al. 2022. Global estimates of the extent and production of macroalgal forests. Global Ecology and Biogeography 31 (7): 1422 - 1439. https://doi.org/10.1111/geb.13515",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#散布図-scatter-plot",
    "href": "descriptive_statistics-slide.html#散布図-scatter-plot",
    "title": "記述統計量",
    "section": "散布図 (scatter plot)",
    "text": "散布図 (scatter plot)\n\nggplot(dset) + \n  geom_point(aes(x = habitat, y = npp))",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#散布図-scatter-plot-output",
    "href": "descriptive_statistics-slide.html#散布図-scatter-plot-output",
    "title": "記述統計量",
    "section": "散布図 (scatter plot)",
    "text": "散布図 (scatter plot)\n\n\n\n\n横軸は因子 (factor)、または離散変数 (discrete variable)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#散布図とジッター-scatter-plot-with-jitter",
    "href": "descriptive_statistics-slide.html#散布図とジッター-scatter-plot-with-jitter",
    "title": "記述統計量",
    "section": "散布図とジッター (scatter plot with jitter)",
    "text": "散布図とジッター (scatter plot with jitter)\n\nggplot(dset) + \n  geom_point(aes(x = habitat, y = npp),\n             position = position_jitter(0.2))\n\n\nこの図の完成度は低い",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#散布図とジッター-scatter-plot-with-jitter-output",
    "href": "descriptive_statistics-slide.html#散布図とジッター-scatter-plot-with-jitter-output",
    "title": "記述統計量",
    "section": "散布図とジッター (scatter plot with jitter)",
    "text": "散布図とジッター (scatter plot with jitter)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#散布図とジッター-scatter-plot-with-jitter-1",
    "href": "descriptive_statistics-slide.html#散布図とジッター-scatter-plot-with-jitter-1",
    "title": "記述統計量",
    "section": "散布図とジッター (scatter plot with jitter)",
    "text": "散布図とジッター (scatter plot with jitter)\n\nxlabel = \"Habitat\"\nylabel = \"NPP (kg C m&lt;sup&gt;-2&lt;/sup&gt; yr&lt;sup&gt;-1&lt;/sup&gt;)\"\nybreaks = seq(0, 5, by = 1)\nggplot(dset) + \n  geom_point(aes(x = habitat, y = npp),\n             position = position_jitter(0.2),\n             alpha = 0.5,\n             size = 3,\n             stroke = 0) +\n  scale_x_discrete(xlabel) + \n  scale_y_continuous(ylabel, \n                   breaks = ybreaks, \n                   limits = range(ybreaks) + c(-0.25, 0)) +\n  theme(axis.title.y = element_markdown())",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#散布図とジッター-scatter-plot-with-jitter-1-output",
    "href": "descriptive_statistics-slide.html#散布図とジッター-scatter-plot-with-jitter-1-output",
    "title": "記述統計量",
    "section": "散布図とジッター (scatter plot with jitter)",
    "text": "散布図とジッター (scatter plot with jitter)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#箱ひげ図-box-plot",
    "href": "descriptive_statistics-slide.html#箱ひげ図-box-plot",
    "title": "記述統計量",
    "section": "箱ひげ図 (box plot)",
    "text": "箱ひげ図 (box plot)\n\nggplot(dset) + \n  geom_boxplot(aes(x = habitat, y = npp))",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#箱ひげ図-box-plot-output",
    "href": "descriptive_statistics-slide.html#箱ひげ図-box-plot-output",
    "title": "記述統計量",
    "section": "箱ひげ図 (box plot)",
    "text": "箱ひげ図 (box plot)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar",
    "href": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar",
    "title": "記述統計量",
    "section": "点とエラーバー (point and error bar)",
    "text": "点とエラーバー (point and error bar)\n\n# Calculate the mean, standard deviation (sd),\n# then number of samples (length), and the standard error.\n\ndset2 = \n  dset |&gt; \n  group_by(habitat) |&gt; \n  summarise(across(npp, \n                   list(m = mean, sd = sd, n = length))) |&gt; \n  mutate(npp_se = npp_sd / sqrt(npp_n - 1))\n\nggplot(dset2) + \n  geom_point(aes(x = habitat, y = npp_m)) +\n  geom_errorbar(aes(x = habitat, \n                    ymin = npp_m - npp_sd,\n                    ymax = npp_m + npp_sd),\n                width = 0.25)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-output",
    "href": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-output",
    "title": "記述統計量",
    "section": "点とエラーバー (point and error bar)",
    "text": "点とエラーバー (point and error bar)\n\n\n\n\nエラーバーは１標準偏差",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-1",
    "href": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-1",
    "title": "記述統計量",
    "section": "点とエラーバー (point and error bar)",
    "text": "点とエラーバー (point and error bar)\n\n# Calculate the mean, standard deviation (sd),\n# then number of samples (length), and the standard error.\n\ndset2 = \n  dset |&gt; \n  group_by(habitat) |&gt; \n  summarise(across(npp, \n                   list(m = mean, sd = sd, n = length))) |&gt; \n  mutate(npp_se = npp_sd / sqrt(npp_n - 1))\n\nggplot(dset2) + \n  geom_point(aes(x = habitat, y = npp_m)) +\n  geom_errorbar(aes(x = habitat, \n                    ymin = npp_m - npp_se,\n                    ymax = npp_m + npp_se),\n                width = 0.25)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-1-output",
    "href": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-1-output",
    "title": "記述統計量",
    "section": "点とエラーバー (point and error bar)",
    "text": "点とエラーバー (point and error bar)\n\n\n\n\nエラーバーは１標準誤差",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#棒グラフ-bar-graph",
    "href": "descriptive_statistics-slide.html#棒グラフ-bar-graph",
    "title": "記述統計量",
    "section": "棒グラフ (bar graph)",
    "text": "棒グラフ (bar graph)\n\ndset2 = \n  dset |&gt; \n  group_by(habitat) |&gt; \n  summarise(across(npp, \n                   list(m = mean, sd = sd, n = length))) |&gt; \n  mutate(npp_se = npp_sd / sqrt(npp_n - 1))\n\nggplot(dset2) + \n  geom_col(aes(x = habitat, y = npp_m))",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#棒グラフ-bar-graph-output",
    "href": "descriptive_statistics-slide.html#棒グラフ-bar-graph-output",
    "title": "記述統計量",
    "section": "棒グラフ (bar graph)",
    "text": "棒グラフ (bar graph)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#棒グラフとエラーバー-candle-stick-graph",
    "href": "descriptive_statistics-slide.html#棒グラフとエラーバー-candle-stick-graph",
    "title": "記述統計量",
    "section": "棒グラフとエラーバー (candle stick graph)",
    "text": "棒グラフとエラーバー (candle stick graph)\n\ndset2 = \n  dset |&gt; \n  group_by(habitat) |&gt; \n  summarise(across(npp, \n                   list(m = mean, sd = sd, n = length))) |&gt; \n  mutate(npp_se = npp_sd / sqrt(npp_n - 1))\n\nggplot(dset2) + \n  geom_col(aes(x = habitat, y = npp_m),\n           fill = \"grey25\") + \n  geom_errorbar(aes(x = habitat,\n                    ymin = npp_m,\n                    ymax = npp_m + npp_sd), \n                width = 0,\n                linewidth = 2, \n                color = \"grey25\")",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#棒グラフとエラーバー-candle-stick-graph-output",
    "href": "descriptive_statistics-slide.html#棒グラフとエラーバー-candle-stick-graph-output",
    "title": "記述統計量",
    "section": "棒グラフとエラーバー (candle stick graph)",
    "text": "棒グラフとエラーバー (candle stick graph)\n\n\n\n\nエラーバーは１標準偏差",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#棒グラフとエラーバー-candle-stick-graph-1",
    "href": "descriptive_statistics-slide.html#棒グラフとエラーバー-candle-stick-graph-1",
    "title": "記述統計量",
    "section": "棒グラフとエラーバー (candle stick graph)",
    "text": "棒グラフとエラーバー (candle stick graph)\n\ndset2 = \n  dset |&gt; \n  group_by(habitat) |&gt; \n  summarise(across(npp, \n                   list(m = mean, sd = sd, n = length))) |&gt; \n  mutate(npp_se = npp_sd / sqrt(npp_n - 1))\n\nggplot(dset2) + \n  geom_col(aes(x = habitat, y = npp_m),\n           fill = \"grey25\") + \n  geom_errorbar(aes(x = habitat,\n                    ymin = npp_m,\n                    ymax = npp_m + npp_se), \n                width = 0,\n                linewidth = 2, \n                color = \"grey25\")",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#棒グラフとエラーバー-candle-stick-graph-1-output",
    "href": "descriptive_statistics-slide.html#棒グラフとエラーバー-candle-stick-graph-1-output",
    "title": "記述統計量",
    "section": "棒グラフとエラーバー (candle stick graph)",
    "text": "棒グラフとエラーバー (candle stick graph)\n\n\n\n\nエラーバーは１標準誤差",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#棒グラフとエラーバー-candle-stick-graph-2",
    "href": "descriptive_statistics-slide.html#棒グラフとエラーバー-candle-stick-graph-2",
    "title": "記述統計量",
    "section": "棒グラフとエラーバー (candle stick graph)",
    "text": "棒グラフとエラーバー (candle stick graph)\n\ndset2 = \n  dset |&gt; \n  group_by(habitat) |&gt; \n  summarise(across(npp, \n                   list(m = mean, sd = sd, n = length))) |&gt; \n  mutate(npp_se = npp_sd / sqrt(npp_n - 1)) |&gt; \n  mutate(habitat = fct_reorder(habitat, npp_m, .desc = TRUE))\n\nggplot(dset2) + \n  geom_col(aes(x = habitat, y = npp_m),\n           fill = \"grey25\") + \n  geom_errorbar(aes(x = habitat,\n                    ymin = npp_m,\n                    ymax = npp_m + npp_se), \n                width = 0,\n                linewidth = 2, \n                color = \"grey25\")",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#棒グラフとエラーバー-candle-stick-graph-2-output",
    "href": "descriptive_statistics-slide.html#棒グラフとエラーバー-candle-stick-graph-2-output",
    "title": "記述統計量",
    "section": "棒グラフとエラーバー (candle stick graph)",
    "text": "棒グラフとエラーバー (candle stick graph)\n\n\n\n\nエラーバーは１標準偏差、データを降順に並べ替えた",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#散布図-scatter-plot-1",
    "href": "descriptive_statistics-slide.html#散布図-scatter-plot-1",
    "title": "記述統計量",
    "section": "散布図 (scatter plot)",
    "text": "散布図 (scatter plot)\n\nggplot(iris) + \n  geom_point(aes(x = Petal.Width, y = Petal.Length))",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#散布図-scatter-plot-1-output",
    "href": "descriptive_statistics-slide.html#散布図-scatter-plot-1-output",
    "title": "記述統計量",
    "section": "散布図 (scatter plot)",
    "text": "散布図 (scatter plot)\n\n\n\n\n横軸も連続変数(continuous variable)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#散布図-scatter-plot-2",
    "href": "descriptive_statistics-slide.html#散布図-scatter-plot-2",
    "title": "記述統計量",
    "section": "散布図 (scatter plot)",
    "text": "散布図 (scatter plot)\n\nggplot(iris) + \n  geom_point(aes(x = Petal.Width, y = Petal.Length, \n                 color = Species))",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#散布図-scatter-plot-2-output",
    "href": "descriptive_statistics-slide.html#散布図-scatter-plot-2-output",
    "title": "記述統計量",
    "section": "散布図 (scatter plot)",
    "text": "散布図 (scatter plot)\n\n\n\n\n横軸も連続変数(continuous variable)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#折れ線グラフ-line-graph",
    "href": "descriptive_statistics-slide.html#折れ線グラフ-line-graph",
    "title": "記述統計量",
    "section": "折れ線グラフ (line graph)",
    "text": "折れ線グラフ (line graph)\n\nse = function(x) {sd(x) / sqrt(length(x) -1)}\niris |&gt; \n  group_by(Species,\n           Petal.Width) |&gt; \n  summarise(across(Petal.Length,\n                   list(m = mean, sd = sd, se = se))) |&gt; \n  ggplot() + \n  geom_point(aes(x = Petal.Width, \n                 y = Petal.Length_m, \n                 color = Species)) +\n  geom_line(aes(x = Petal.Width,\n                y = Petal.Length_m, \n                color = Species))",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#折れ線グラフ-line-graph-output",
    "href": "descriptive_statistics-slide.html#折れ線グラフ-line-graph-output",
    "title": "記述統計量",
    "section": "折れ線グラフ (line graph)",
    "text": "折れ線グラフ (line graph)\n\n\n\n\n横軸も連続変数(continuous variable)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#折れ線グラフ-line-graph-1",
    "href": "descriptive_statistics-slide.html#折れ線グラフ-line-graph-1",
    "title": "記述統計量",
    "section": "折れ線グラフ (line graph)",
    "text": "折れ線グラフ (line graph)\n\nse = function(x) {sd(x) / sqrt(length(x) -1)}\niris |&gt; \n  group_by(Species,\n           Petal.Width) |&gt; \n  summarise(across(Petal.Length,\n                   list(m = mean, sd = sd, se = se))) |&gt; \n  ggplot() + \n  geom_line(aes(x = Petal.Width,\n                y = Petal.Length_m, \n                color = Species)) +\n  geom_errorbar(aes(x = Petal.Width,\n                    ymin = Petal.Length_m - Petal.Length_se,\n                    ymax = Petal.Length_m + Petal.Length_se,\n                    color = Species),\n                linewidth = 2,\n                width = 0.0) +\n  geom_errorbar(aes(x = Petal.Width,\n                    ymin = Petal.Length_m - Petal.Length_sd,\n                    ymax = Petal.Length_m + Petal.Length_sd,\n                    color = Species),\n                width = 0.0)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#折れ線グラフ-line-graph-1-output",
    "href": "descriptive_statistics-slide.html#折れ線グラフ-line-graph-1-output",
    "title": "記述統計量",
    "section": "折れ線グラフ (line graph)",
    "text": "折れ線グラフ (line graph)\n\n\n\n\n１標準偏差（細線）と１標準誤差（太線）を示した",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-2",
    "href": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-2",
    "title": "記述統計量",
    "section": "点とエラーバー (point and error bar)",
    "text": "点とエラーバー (point and error bar)\n\niris |&gt; \n  group_by(Species) |&gt; \n  summarise(across(matches(\"Petal\"),\n                   list(m = mean, sd = sd, n = length))) |&gt; \n  mutate(Petal.Width_se = Petal.Width_sd / sqrt(Petal.Width_n - 1)) |&gt; \n  mutate(Petal.Length_se = Petal.Length_sd / sqrt(Petal.Length_n - 1)) |&gt; \n  ggplot() + \n  geom_point(aes(x = Petal.Width_m, \n                 y = Petal.Length_m, \n                 color = Species)) +\n  geom_errorbarh(aes(y = Petal.Length_m,\n                     xmin = Petal.Width_m - Petal.Width_sd,\n                     xmax = Petal.Width_m + Petal.Width_sd,\n                     color = Species),\n                 height = 0.0)+\n  geom_errorbar(aes(x = Petal.Width_m,\n                    ymin = Petal.Length_m - Petal.Length_sd,\n                    ymax = Petal.Length_m + Petal.Length_sd,\n                    color = Species),\n                width = 0.0)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-2-output",
    "href": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-2-output",
    "title": "記述統計量",
    "section": "点とエラーバー (point and error bar)",
    "text": "点とエラーバー (point and error bar)\n\n\n\n\n横軸も連続変数(continuous variable)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-3",
    "href": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-3",
    "title": "記述統計量",
    "section": "点とエラーバー (point and error bar)",
    "text": "点とエラーバー (point and error bar)\n\niris |&gt; \n  group_by(Species) |&gt; \n  summarise(across(matches(\"Petal\"),\n                   list(m = mean, sd = sd, n = length))) |&gt; \n  mutate(Petal.Width_se = Petal.Width_sd / sqrt(Petal.Width_n - 1)) |&gt; \n  mutate(Petal.Length_se = Petal.Length_sd / sqrt(Petal.Length_n - 1)) |&gt; \n  ggplot() + \n  geom_point(aes(x = Petal.Width,\n                 y = Petal.Length,\n                 color = Species),\n             data = iris,\n             stroke = 0,\n             alpha = 0.5) +\n  geom_point(aes(x = Petal.Width_m, \n                 y = Petal.Length_m, \n                 color = Species)) +\n  geom_errorbarh(aes(y = Petal.Length_m,\n                     xmin = Petal.Width_m - Petal.Width_sd,\n                     xmax = Petal.Width_m + Petal.Width_sd,\n                     color = Species),\n                 height = 0.0)+\n  geom_errorbar(aes(x = Petal.Width_m,\n                    ymin = Petal.Length_m - Petal.Length_sd,\n                    ymax = Petal.Length_m + Petal.Length_sd,\n                    color = Species),\n                width = 0.0) +\n  scale_color_viridis_d(end = 0.9)",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-3-output",
    "href": "descriptive_statistics-slide.html#点とエラーバー-point-and-error-bar-3-output",
    "title": "記述統計量",
    "section": "点とエラーバー (point and error bar)",
    "text": "点とエラーバー (point and error bar)\n\n\n\n\n変数ごとの平均値と１標準偏差も示した",
    "crumbs": [
      "基礎統計学用",
      "記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#サクマドロップスのサンプル表",
    "href": "sakuma-drops.html#サクマドロップスのサンプル表",
    "title": "データの可視化と記述統計量",
    "section": "サクマドロップスのサンプル表",
    "text": "サクマドロップスのサンプル表\n一回目の講義には、キャンバスバッグからサクマドロップスを採取しました。 キャンバスバッグにあるサクマドロップスの種類ごと数を知ることが目的です。 本来知りたい集団全体のことを母集団 (population)とよび、母集団から採取したデータは標本 (sample)といいます。\n\n\n\n\n\n\n\n\nStudent\nイチゴ\nオレンジ\nスモモ\nハッカ\nパイン\nメロン\nリンゴ\nレモン\n\n\n\n\nA\n4\n3\n8\n2\n4\n10\n5\n5\n\n\nB\n8\n6\n5\n3\n4\n3\n3\n3\n\n\nC\n8\n5\n7\n3\n4\n8\n5\n6\n\n\nD\n8\n6\n4\n3\n7\n6\n5\n5\n\n\nE\n5\n8\n4\n3\n3\n6\n8\n4\n\n\n\n\n\n\n\n学生ごとに一度だけ標本をとったので、表には５つの標本を示している。\n標本から母数団の情報を推定するので、標本の代表的な値を求めます。",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#データを代表する値",
    "href": "sakuma-drops.html#データを代表する値",
    "title": "データの可視化と記述統計量",
    "section": "データを代表する値",
    "text": "データを代表する値\n平均値 (mean, average)： 総和を標本数で割った値\n\\[\n\\bar{x} = \\frac{1}{N} \\sum_{n=1}^{N} x_n\n\\]\n中央値 (median)： 標本を上順に並べたときに、データの中央に位置する値\n\\[\nM = \\cases{\nx_{[\\frac{1}{2}(N + 1)]} & $N$ が奇数 \\\\\n\\frac{1}{2}\\left(x_{[\\frac{N}{2}]} + x_{[\\frac{N}{2} + 1]}\\right) & $N$ が偶数\n}\n\\]\n最頻値 (mode)： 標本で最も頻繁に現れる値",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#平均値の求め方",
    "href": "sakuma-drops.html#平均値の求め方",
    "title": "データの可視化と記述統計量",
    "section": "平均値の求め方",
    "text": "平均値の求め方\nオレンジサクマドロップスのサンプルをまとめます。\n\\[\n\\bar{x} = \\frac{1}{5} (3 + 6 + 5 + 6 + 8) = 5.6\n\\]",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#中央値の求め方",
    "href": "sakuma-drops.html#中央値の求め方",
    "title": "データの可視化と記述統計量",
    "section": "中央値の求め方",
    "text": "中央値の求め方\nオレンジサクマドロップスの中央値を求めるなら、サンプルを上順に並べる必要があります。\n\\[\n(3, 5, 6, 6, 8)\n\\]\n\\(n = 5\\) なので、中央値は、\n\\[\nx_{[\\frac{1}{2}(5 + 1)]} = x_{[3]} = 6\n\\]",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#サクマドロップスの統計量",
    "href": "sakuma-drops.html#サクマドロップスの統計量",
    "title": "データの可視化と記述統計量",
    "section": "サクマドロップスの統計量",
    "text": "サクマドロップスの統計量\n全種類のサクマドロップスの平均値、中央値、最頻値を求めます。 少量のデータなら、表でまとめることもあるが、データ数が増えると図のほうが見やすい。\n\n\n\n\n\n\n\n\nStudent\nイチゴ\nオレンジ\nスモモ\nハッカ\nパイン\nメロン\nリンゴ\nレモン\n\n\n\n\nA\n4.0\n3.0\n8.0\n2.0\n4.0\n10.0\n5.0\n5.0\n\n\nB\n8.0\n6.0\n5.0\n3.0\n4.0\n3.0\n3.0\n3.0\n\n\nC\n8.0\n5.0\n7.0\n3.0\n4.0\n8.0\n5.0\n6.0\n\n\nD\n8.0\n6.0\n4.0\n3.0\n7.0\n6.0\n5.0\n5.0\n\n\nE\n5.0\n8.0\n4.0\n3.0\n3.0\n6.0\n8.0\n4.0\n\n\nMean\n6.6\n5.6\n5.6\n2.8\n4.4\n6.6\n5.2\n4.6\n\n\nMedian\n8.0\n6.0\n5.0\n3.0\n4.0\n6.0\n5.0\n5.0\n\n\nMode\n8.0\n6.0\n4.0\n3.0\n4.0\n6.0\n5.0\n5.0",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#棒グラフ",
    "href": "sakuma-drops.html#棒グラフ",
    "title": "データの可視化と記述統計量",
    "section": "棒グラフ",
    "text": "棒グラフ\n\nStacked bar chart",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#棒グラフ-1",
    "href": "sakuma-drops.html#棒グラフ-1",
    "title": "データの可視化と記述統計量",
    "section": "棒グラフ",
    "text": "棒グラフ\n\nGrouped bar chart",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#散布図",
    "href": "sakuma-drops.html#散布図",
    "title": "データの可視化と記述統計量",
    "section": "散布図",
    "text": "散布図\n\nScatter plot",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#散布図-1",
    "href": "sakuma-drops.html#散布図-1",
    "title": "データの可視化と記述統計量",
    "section": "散布図",
    "text": "散布図\n\nJittered scatter plot",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#箱ひげ図",
    "href": "sakuma-drops.html#箱ひげ図",
    "title": "データの可視化と記述統計量",
    "section": "箱ひげ図",
    "text": "箱ひげ図\n\nBox-and-whisker plot",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#箱ひげ図について",
    "href": "sakuma-drops.html#箱ひげ図について",
    "title": "データの可視化と記述統計量",
    "section": "箱ひげ図について",
    "text": "箱ひげ図について\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedian: 中央値または第2分位点\n箱の上下の辺は第1 (25%) と第3四分位点 (75%)を示す。50%のサンプルは箱内にある。\n上下のヒゲ (Upper & Lower) は、それぞれの四分位点の位置から、極値までの間を示す。 極値とは、第1または第3四分位点から箱の高さの 1.5 倍以内にあるサンプルのうちの最大値と最小値です。\nひげの範囲外のサンプルは点として示される。",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#データの可視化の重要なポイント",
    "href": "sakuma-drops.html#データの可視化の重要なポイント",
    "title": "データの可視化と記述統計量",
    "section": "データの可視化の重要なポイント",
    "text": "データの可視化の重要なポイント\n\n複雑なデータの関係を明確に、簡潔に、雑味のないように示す\n相手に重要なポイントをすぐに把握できるように示す\n適切な視覚的要素を用いて、データに含まれる情報やアイデアを効果的に伝える\nフォント、フォントサイズ、色、記号の種類を相手にあわせる\n読みやすいフォントを選ぶ\n色覚異常を意識すること\nシンプルでわかりやすい図を作図する・不必要な要素をいれない。\n信頼できる正確な、最新なデータを用いる",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#区別しづらい色の組み合わせ",
    "href": "sakuma-drops.html#区別しづらい色の組み合わせ",
    "title": "データの可視化と記述統計量",
    "section": "区別しづらい色の組み合わせ",
    "text": "区別しづらい色の組み合わせ\n\n参考：(https://www.morisawa.co.jp/blogs/MVP/5369)",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#サンプル数の総和で表す",
    "href": "sakuma-drops.html#サンプル数の総和で表す",
    "title": "データの可視化と記述統計量",
    "section": "サンプル数の総和で表す",
    "text": "サンプル数の総和で表す",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#サンプル数の総和で表す-1",
    "href": "sakuma-drops.html#サンプル数の総和で表す-1",
    "title": "データの可視化と記述統計量",
    "section": "サンプル数の総和で表す",
    "text": "サンプル数の総和で表す\n\n色を加えるなら、資料と合わせる。",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#サンプル数の総和で表す-2",
    "href": "sakuma-drops.html#サンプル数の総和で表す-2",
    "title": "データの可視化と記述統計量",
    "section": "サンプル数の総和で表す",
    "text": "サンプル数の総和で表す\n\n場合によって、図に数値情報も加えるといい。ここでは、棒の上に総数を追加した。",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#平均値で表す",
    "href": "sakuma-drops.html#平均値で表す",
    "title": "データの可視化と記述統計量",
    "section": "平均値で表す",
    "text": "平均値で表す\n\n学生が採取したサンプルの学生ごとの合計は異なる。 5回分の情報を総数で示すのはあまりよろしくない。Why?",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#平均値で表す-1",
    "href": "sakuma-drops.html#平均値で表す-1",
    "title": "データの可視化と記述統計量",
    "section": "平均値で表す",
    "text": "平均値で表す",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#データの分布",
    "href": "sakuma-drops.html#データの分布",
    "title": "データの可視化と記述統計量",
    "section": "データの分布",
    "text": "データの分布",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#データの見せ方",
    "href": "sakuma-drops.html#データの見せ方",
    "title": "データの可視化と記述統計量",
    "section": "データの見せ方",
    "text": "データの見せ方\n\n図はサンプル数に合わせて、見せ方を工夫する。",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#データのばらつき",
    "href": "sakuma-drops.html#データのばらつき",
    "title": "データの可視化と記述統計量",
    "section": "データのばらつき",
    "text": "データのばらつき\n標準偏差 (standard deviation)：\n\\[\ns = \\sqrt{\\frac{1}{N-1} \\sum_{n=1}^N\\left(x - \\bar{x}\\right)^2}\n\\]\n\\(x - \\bar{x}\\) は残渣と呼ぶ。\n平均絶対偏差 (mean absolute deviation) & 中央絶対残渣 (median absolute deviation)\n\\[\n\\text{MAD} = \\frac{1}{N} \\sum_{n = 1}^{N} |x - m(x)|\n\\]\n\\[\n\\text{MAD} = median(|x - \\tilde{x}|)\n\\] \\(m(x)\\) は平均値または中央値、\\(\\tilde{x}\\) は中央値。一般的には MAD が諸略なので、何が計算されたのかをよく調べること。",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#標準偏差の求め方",
    "href": "sakuma-drops.html#標準偏差の求め方",
    "title": "データの可視化と記述統計量",
    "section": "標準偏差の求め方",
    "text": "標準偏差の求め方\n\\[\n\\bar{x} = 5.6\n\\]\n\\[\ns = \\sqrt{\\frac{1}{5-1} (3-5.6)^2 + (6-5.6)^2 + (5-5.6)^2 + (6-5.6)^2 + (8-5.6)^2}\n\\] \\[\ns =  1.8165902\n\\]",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#標準偏差付き散布図",
    "href": "sakuma-drops.html#標準偏差付き散布図",
    "title": "データの可視化と記述統計量",
    "section": "標準偏差付き散布図",
    "text": "標準偏差付き散布図",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#平均の制度",
    "href": "sakuma-drops.html#平均の制度",
    "title": "データの可視化と記述統計量",
    "section": "平均の制度",
    "text": "平均の制度\n\\[\n\\text{S.E.} = \\frac{s}{N}\n\\]\n\\(s\\) は標準偏差、\\(\\text{S.E.}\\) は Standard Error の諸略（標準誤差）。",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#標準誤差の求め方",
    "href": "sakuma-drops.html#標準誤差の求め方",
    "title": "データの可視化と記述統計量",
    "section": "標準誤差の求め方",
    "text": "標準誤差の求め方\n\\[\n\\bar{x} = 5.6\n\\]\n\\[\n\\text{S.E.} = \\frac{1}{5}\\sqrt{\\frac{1}{5-1} (3-5.6)^2 + (6-5.6)^2 + (5-5.6)^2 + (6-5.6)^2 + (8-5.6)^2}\n\\] \\[\n\\text{S.E.} = 0.363318\n\\]",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#標準誤差付き散布図",
    "href": "sakuma-drops.html#標準誤差付き散布図",
    "title": "データの可視化と記述統計量",
    "section": "標準誤差付き散布図",
    "text": "標準誤差付き散布図",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#母集団と平均値の比較",
    "href": "sakuma-drops.html#母集団と平均値の比較",
    "title": "データの可視化と記述統計量",
    "section": "母集団と平均値の比較",
    "text": "母集団と平均値の比較",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "sakuma-drops.html#割合の比較",
    "href": "sakuma-drops.html#割合の比較",
    "title": "データの可視化と記述統計量",
    "section": "割合の比較",
    "text": "割合の比較",
    "crumbs": [
      "基礎統計学用",
      "データの可視化と記述統計量"
    ]
  },
  {
    "objectID": "nhst-slide.html#度数-frequency",
    "href": "nhst-slide.html#度数-frequency",
    "title": "大数の法則と\n中心極限定理",
    "section": "度数 (frequency)",
    "text": "度数 (frequency)\n\n\n\n\n\n\n度数（頻度）とは\n\n\nある標本で特定のデータの値が得られた回数。 相対度数は標本数に対する度数の割合です。\n\n\n\n{3, 1, 5, 1, 4, 2, 5, 2, 3, 2, 6, 3, 3, 1, 3, 4, 1, 4, 4, 2, 1, 6, 1, 1, 1, 2, 1, 2, 6, 4} に対する 6 の度数は 3 です。\n相対度数は\n\nです。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#コイン投げの例",
    "href": "nhst-slide.html#コイン投げの例",
    "title": "大数の法則と\n中心極限定理",
    "section": "コイン投げの例",
    "text": "コイン投げの例\n\n\nフェア (fair) なコインを投げた場合、表が出る確率は 0.50 です。 では、コインを 100 回投げた場合、 表がでる相対度数は 0.50 でしょうか。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#コイン投げの例-1",
    "href": "nhst-slide.html#コイン投げの例-1",
    "title": "大数の法則と\n中心極限定理",
    "section": "コイン投げの例",
    "text": "コイン投げの例\n\n\n今回の実験では、 表が出る度数が 0.50 に収束するためには、 おおよそ 15,000 回投げました。\nコイン投げの回数が少なければ（小数） 表が出る確率と相対度数に大きな違いがありましたが、 投げる回数が増えると（大数）相対度数と確率の違いは小さくなりました。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#大数の弱法則",
    "href": "nhst-slide.html#大数の弱法則",
    "title": "大数の法則と\n中心極限定理",
    "section": "大数の弱法則",
    "text": "大数の弱法則\nWeak law of large numbers (大数の弱法則)\n\n平均値 \\(\\mu\\)、 分散 \\(\\sigma^2\\) の分布にお互いに独立に従う 確率変数 \\(X_1, X_2, \\cdots, X_n\\) と、任意の \\(\\varepsilon &gt; 0\\) の場合、\n\n\\[\n\\lim_{n\\rightarrow\\infty} P \\left(\\left |\\frac{X_1+X_2+\\cdots+X_n}{n}-\\mu\\right | &lt; \\varepsilon\\right) = 1\n\\]\n\nつまり、標本平均と母平均の差が \\(\\varepsilon\\) 以下になる確率は 試行回数 \\((n)\\) を増やせば \\(1\\) に確率収束 (stochastic convergence) すると意味します。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#大数の強法則",
    "href": "nhst-slide.html#大数の強法則",
    "title": "大数の法則と\n中心極限定理",
    "section": "大数の強法則",
    "text": "大数の強法則\nStrong law of large numbers (大数の強法則)\n\\[\nP \\left(\\lim_{n\\rightarrow\\infty} \\frac{X_1+X_2+\\cdots+X_n}{n}=\\mu \\right) = 1\n\\]\n観測回数が増えるにつれて、標本平均は概収束（ほとんど確実に収束; almost sure convergence）に従って母平均に収束します。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#平均値のデモ",
    "href": "nhst-slide.html#平均値のデモ",
    "title": "大数の法則と\n中心極限定理",
    "section": "平均値のデモ",
    "text": "平均値のデモ\n\n\n標本 (y) は 0 から 1 の一様分布に従う確率変数です。 点は y の観測値、線は y の累積平均値（標本平均）です。 標本数が増えれるにつれて、y の標本平均（線）は母平均 (μ = 0.50) に収束する。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#clt-dice-example1",
    "href": "nhst-slide.html#clt-dice-example1",
    "title": "大数の法則と\n中心極限定理",
    "section": "",
    "text": "一回の実験に２個のサイコロを４９回投げました。 実験は 215 回繰り返して行い、実験ごとに２個のサイコロの和の平均値を求めました。 標準平均から母平均（７）を引いたあと、標本の標準偏差で割りました \\((Z = \\frac{x_i - \\overline{x}}{s})\\)。 もとのデータの分布は正規分布ではないが、平均値は正規分布に従っています。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#clt-dice-example2",
    "href": "nhst-slide.html#clt-dice-example2",
    "title": "大数の法則と\n中心極限定理",
    "section": "",
    "text": "一回の実験に 1 個のサイコロを４９回投げました。 実験は 215 回繰り返して行い、 実験ごとに 1 個のサイコロの出目の平均値を求めました。 標準平均から母平均（3.5）を引いたあと、標本の標準偏差で割りました \\((Z = \\frac{x_i - \\overline{x}}{s})\\)。 もとのデータの分布は正規分布ではないが、平均値は正規分布に従っています。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#central-limit-theorem-中心極限定理",
    "href": "nhst-slide.html#central-limit-theorem-中心極限定理",
    "title": "大数の法則と\n中心極限定理",
    "section": "Central limit theorem (中心極限定理)",
    "text": "Central limit theorem (中心極限定理)\n中心極限定理とは、多数の独立かつ同一分布に従う確率変数の 平均値の分布が、一定の条件下では、正規分布に近似することを示す。\nデータの分布に関わらず、データの平均値を大量に求めると、 その平均値自体が正規分布に近似することが期待できる。 平均値は、正規分布に従うことで、母集団の平均値や分散についての 推論ができるようになる。\n統計学において非常に重要な定理です。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#中心極限定理",
    "href": "nhst-slide.html#中心極限定理",
    "title": "大数の法則と\n中心極限定理",
    "section": "中心極限定理",
    "text": "中心極限定理\n\\[\n\\lim_{n\\rightarrow \\infty}\\sqrt{n}\\left(\\frac{\\overline{X}_n - \\mu}{\\sigma}\\right) \\xrightarrow{d} N(0, \\sigma^2)\n\\]\n標本数 \\(n\\) が無限大に近づくにつれて、\\(\\sqrt{n}\\) と \\(\\frac{\\overline{X}_n - \\mu}{\\sigma}\\) の積は、平均 \\(0\\)、分散 \\(\\sigma^2\\) の正規分布に近づくことを意味します。\n\n\\(\\overline{X}_n\\) は標本 n の標本平均値です。\n\\(\\xrightarrow{d}\\) は法則収束または分布収束 (converges in distribution)　と意味します。\n\n\nThis equation expresses that as the sample size \\(n\\) approaches infinity, the standardized sample mean \\(\\frac{\\overline{X}_n - \\mu}{\\sigma}\\) multiplied by \\(\\sqrt{n}\\) approaches a normal distribution with mean \\(0\\) and variance \\(\\sigma^2\\).",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#scientific-method-科学的方法",
    "href": "nhst-slide.html#scientific-method-科学的方法",
    "title": "大数の法則と\n中心極限定理",
    "section": "Scientific Method (科学的方法)　",
    "text": "Scientific Method (科学的方法)　\n合理的に研究をするためには、科学的方法を用います。 科学的方法には6つのステップがあり、切り返して行うことが一般的です。\n\n観察：まず、自然界を観察します。\n質問：観察した自然現象について、科学的に検証できる（実験）課題を考えます。\n仮説：仮説は、質問に対する合理的な推測です。実験で検証できるものです。\n実験：実験を行い、考えた仮説が正しいかどうかを確認します。ここでデータ収集します。\n分析：実験で得たデータを統計学的に解析します。データは、考え上げた仮説を支持するか否定するかを検証します。\n結論：仮説が支持された場合は、考え上げた仮説が現段階で正しいと結論つけられますが、否定された場合は、仮説を修正するかまたは新たな仮説を開発します。\n\n\n科学的方法は周期的なプロセスです。実験を終了した後、最初に戻って新しい観察を行うか、新しい質問をする必要が生じる場合があります。これは、科学的方法が自然界について学ぶ方法だからです。より多くのことを学ぶにつれて、仮説や理論を変更する必要があるかもしれません。\n科学的方法は、自然界を理解するための強力なツールです。科学者はこの方法を使用して、原子から銀河までのあらゆるものを理解するための大きな進歩を遂げました。科学的方法は、継続的な学習と発見のプロセスです。それは、自然界の真実を見つける方法です",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#hypothesis-仮説",
    "href": "nhst-slide.html#hypothesis-仮説",
    "title": "大数の法則と\n中心極限定理",
    "section": "Hypothesis (仮説)",
    "text": "Hypothesis (仮説)\n\n仮説は実験を行う前に決めるもの。データ見てから仮説を決めません。\n一般的には、仮説を証明（採択）したいが、統計学の視点から考えると、仮説は棄却するものです。\n\n仮説を証明（採択）することはできません。仮説は棄却するものです。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#model-モデル",
    "href": "nhst-slide.html#model-モデル",
    "title": "大数の法則と\n中心極限定理",
    "section": "Model (モデル)",
    "text": "Model (モデル)\n\nモデルはデータを見てから考えるもの。\nモデルも検証する必要があります。\nモデルは予測に使用できます。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#有意性検定および仮説検定",
    "href": "nhst-slide.html#有意性検定および仮説検定",
    "title": "大数の法則と\n中心極限定理",
    "section": "有意性検定および仮説検定",
    "text": "有意性検定および仮説検定\n\n\nSignificance testing (有意性検定論)\n\n1920年代に、R.A. Fisher が提案した。\n帰無仮説 \\(H_0\\)　に対して集めたデータを得られるエビデンス（証拠）。\nP値が小さいとき、エビデンスは弱い\n\n\\(\\text{P-value} = P(T(X) \\ge T_0(X)|H_0)\\)\n\n仮説の棄却や採択はしない\n\n\nHypothesis testing (仮説検定論)\n\n1930年代に、 J. Neyman と E.S. Pearson が提案した。\n最も使われている手法\n帰無仮説 \\(H_0\\) と対立仮説 \\(H_A\\)を定義します\n有意水準 (significance level \\(\\alpha\\)) を基準にして、 \\(H_A\\) の採択または棄却をする手法。\n\n\n\n\nSir Ronald Aylmer Fisher was a British statistician and geneticist. His work win agricultural experiments led to the development of the Analysis of Variance. In 1925 he published the book Statistical Methods for Research Workers, that introduced the concept of the P-value.\nJerzy Neyman was a Polish mathematician and statician, and Egon Sharpe Pearson was a British statician. They co-invented and popularized the hypothesis testing techniques and the rejection and acceptance of the null hypothesis or the alternative hypothesis.",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#what-is-the-p-value-p値とは",
    "href": "nhst-slide.html#what-is-the-p-value-p値とは",
    "title": "大数の法則と\n中心極限定理",
    "section": "What is the P-value (P値とは)?",
    "text": "What is the P-value (P値とは)?\n統計解析をすることで、P値を求めることは当たり前のようになりました。\n\\[\n\\text{P-value} = P(T(X) \\ge T_0(X)|H_0)\n\\]\n\n帰無仮説 \\(H_0\\) にたいして、\\(T(X) &gt; T_0(X)\\) がおきる確率\n帰無仮説に対して、収集したデータの整合性を指標科した値\nP値は 0 ~ 1 (\\(0 \\leq \\text{P-value} \\leq 1\\)) の範囲をとる。\n帰無仮説が正しいとき、P値は一様分布に従う。\n帰無仮説が正しくとき：\\(\\displaystyle \\lim_{n \\rightarrow \\infty} P \\rightarrow 0\\)。\n\n\n\nThe P-value only makes sense when the null hypothesis is true. This is why we assume a true null, so that we can get a P-value.\nThe original definition of the P-value: the chance of obtaining an effect equal to or more extreme than the one observed considering the null hypothesis is true.\n\nLinks to StackExchange: Cross Validated Questions.S\n\nIntepretation of p-value in hypothesis testing.\nWhat is the relationship between p values and type I errors\nHow do I find the probability of type II errors",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#帰無仮説は正しいときのp値",
    "href": "nhst-slide.html#帰無仮説は正しいときのp値",
    "title": "大数の法則と\n中心極限定理",
    "section": "帰無仮説は正しいときのP値",
    "text": "帰無仮説は正しいときのP値\n\n\n帰無仮説が正しいとき \\(((\\mu_0=50) = (\\mu_A=50))\\)、 P値は一様分布に従います。 実験を10万回繰り返し実施たとき、 \\(P(\\text{P-value}&lt;0.05) = 0.0501\\)でした。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#帰無仮説は正しくないときのp値",
    "href": "nhst-slide.html#帰無仮説は正しくないときのp値",
    "title": "大数の法則と\n中心極限定理",
    "section": "帰無仮説は正しくないときのP値",
    "text": "帰無仮説は正しくないときのP値\n\n\n帰無仮説は正しくないとき \\(((\\mu_0=50) \\ne (\\mu_A=51))\\)、 P値は一様分布に従いません。 実験を10万回繰り返し実施たとき、 \\(P(\\text{P-value}&lt;0.05) = 0.5604\\)でした。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#仮説検定",
    "href": "nhst-slide.html#仮説検定",
    "title": "大数の法則と\n中心極限定理",
    "section": "仮説検定",
    "text": "仮説検定\n仮説検定は客観的に意思決定をするために使います。\n\nP値は帰無仮説と対立仮説を比較するために使います。\n帰無仮説を棄却するルールは有意水準を用いて行います。\n\\(\\text{P-value} \\leq \\alpha\\)　のとき帰無仮説を棄却します。\n帰無仮説を棄却・採択することで、誤りを起こすことになる\n\n第１種の誤り, α過誤 (Type-I error)\n第２種の誤り, β過誤 (Type-II error)",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#帰無仮説に注意",
    "href": "nhst-slide.html#帰無仮説に注意",
    "title": "大数の法則と\n中心極限定理",
    "section": "帰無仮説に注意",
    "text": "帰無仮説に注意\nNeyman-Pearson の仮説検定法は Null Hypothesis Significance Test (NHST) (帰無仮説の有意性検定) といいます。 意思決定をするための手法なので、誤りを起こすこともある\n\nType-I Error (第１種の誤り): 帰無仮説が正しいのに、帰無仮説を棄却する誤り。誤る確率は \\(\\alpha\\)　（有意水準と同じ値）\nType-II Error (第２種の誤り): 帰無仮説が正しくないのに、帰無仮説を棄却しない誤り。誤る確率は \\(\\beta\\)（単純に求められない）",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#帰無仮説に注意-1",
    "href": "nhst-slide.html#帰無仮説に注意-1",
    "title": "大数の法則と\n中心極限定理",
    "section": "帰無仮説に注意",
    "text": "帰無仮説に注意\n\n\\(\\alpha\\) 有意水準および第1種の誤り\n\\(\\beta\\) 第2種の誤り\n\\(1-\\beta\\) 検定の検出力、正しくない帰無仮説を棄却する確率\n第1種の誤りを厳しくすると、第2種の誤りはあまくなる (お互いに反比例する)\n\n\\[\n\\alpha \\propto 1/\\beta\n\\]",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#第１種と第２種の誤り",
    "href": "nhst-slide.html#第１種と第２種の誤り",
    "title": "大数の法則と\n中心極限定理",
    "section": "第１種と第２種の誤り",
    "text": "第１種と第２種の誤り\n\n\nBiau et al. 2010. Clin. Orthop. Relat. Res. 468: 885-892.\nFig. 2 of Biau et al. 2010 is wrong, since if the Type-I error rate decreases, then the Type-II error rate increases.\nThere is a false impression that if experiments are conducted with a low Type I error, then significant results almost always corresponds to a true effect.\nOut of 1000 null hypothesis, 10% are actually false. In otherwords, the null hypothesis is rarely false. So, 900 are true null hypothesis and 100 are false null hypothesis. However, we do not know this. Suppose that we perform tests that have an α of 0.10 and a β of 0.24, Among the true null hypothesis, since our Type-I error rate is 0.10, then 90 experiments will be erroneously rejected. Among the false null hypothesis, since our Type-II error rate is 0.24, then 24 experiments will be erroneously not rejected and 80 will be correctly rejected.\nTherefore, a total of 90+76 null hypotheses will be rejected, but only 76 / (90+76) = 0.46 of these were correctly rejected.\nI am not sure if this is a good example, since the null hypothesis is only rare false. What is the point of this?",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#有意性検定と仮説検定の違い",
    "href": "nhst-slide.html#有意性検定と仮説検定の違い",
    "title": "大数の法則と\n中心極限定理",
    "section": "有意性検定と仮説検定の違い",
    "text": "有意性検定と仮説検定の違い\n\n\nSignificance testing (有意性検定)\n\nFisher の手法\nP値の値は重要\n帰無仮説を棄却するために使う\nデータを見てから計算する\n解析したデータのみに有効\nSubjective decision (主観的な判断)\nエビデンスによる意思決定\n\n\nNHST (仮説検定)\n\nNeyman and Pearson の手法\n第1種の誤りが重要\n第1種と第2種の誤りを最低限にする\nデータを見てから、α と β を選ぶ\n実験は十分に反復されており、今後も似たようなデータを期待できる\nObjective decision (客観的)\nルールに基づいた意思決定\n\n\n\n\n\nBiau et al. 2010. Clin. Orthop. Relat. Res. 468: 885-892.\n\nSee Fig. 1A-B in this paper. Since Fisher’s P-value is valid for each experiment individually, some experiments will provide evidence for rejecting the null and some won’t.\n\nThe p value is not the probability of the null hypothesis being true; it is the probability of observing these data, or more extreme data, if the null is true.\nHowever, for Neyman-Pearson method, there is only a chance of making an error of rejecting or accepting a hypothesis based on an α and β value.",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#example",
    "href": "nhst-slide.html#example",
    "title": "大数の法則と\n中心極限定理",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  observation by experiment\nt = -1.4885, df = 31.452, p-value = 0.1466\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -1.3181200  0.2055077\nsample estimates:\nmean in group A mean in group B \n       9.897632       10.453938 \n\n\n\n\n\n\n\\(H_0: \\overline{\\mu_A}=\\overline{\\mu_B}~:\\text{Null hypothesis}\\)\n\\(H_1: \\overline{\\mu_A}\\neq\\overline{\\mu_B}~:\\text{Alternative hypothesis}\\)\nTrue standard deviation：\\(\\sigma_A=\\sigma_B\\)\nTrue mean: \\(\\overline{\\mu_A}=10\\) and \\(\\overline{\\mu_B}=12\\)\nIn this case, \\(P = 0.1466  \\nless \\alpha = 0.05\\)\n\n\n\nThe true means are different, yet the P-value for the Welch’s two sample t-test is greater than 0.05. What does this mean?\nDo we accept the alternative hypothesis?\nDo we reject the null hypothesis?",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "nhst-slide.html#amerhein-et-al.-2019.-nature-567-305-307",
    "href": "nhst-slide.html#amerhein-et-al.-2019.-nature-567-305-307",
    "title": "大数の法則と\n中心極限定理",
    "section": "Amerhein et al. 2019. Nature 567: 305-307",
    "text": "Amerhein et al. 2019. Nature 567: 305-307\n\n\n\nLet’s be clear about what must stop: we should never conclude there is ‘no difference’ or ‘no association’ just because a P-value is larger than a threshold such as 0.05 … –Amerhein et al. 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\nP値は 0.05 より大きい場合、「違いはない」、「実験の影響はない」、 「関係性はない」のような解釈は誤りです。\n\\(P&gt;0.05\\) は、帰無仮説を棄却するほどの情報量がないだけを意味します。 決定的に実験の効果がないまでは言えませんが、効果がなかったことについては丁寧に考察する必要はあるでしょう。 帰無仮説を棄却したときも同じように疑いながら結果の考察は重要です。 たまたま棄却できたときもあります（第２種誤り）。",
    "crumbs": [
      "基礎統計学用",
      "大数の法則と中心極限定理"
    ]
  },
  {
    "objectID": "glm.html",
    "href": "glm.html",
    "title": "線形モデル",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(car)\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nlibrary(patchwork)",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#必要なパッケージ",
    "href": "glm.html#必要なパッケージ",
    "title": "線形モデル",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(car)\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nlibrary(patchwork)",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#線形モデルとは",
    "href": "glm.html#線形モデルとは",
    "title": "線形モデル",
    "section": "線形モデルとは？",
    "text": "線形モデルとは？\n\n線形モデルの 線型 (linear) は直線 (straight line) と全く関係ないです。\n線型モデルの線型は 線型結合 (linear combination) を意味しています。\n\n\ny = b_1 x_1 + b_2 x_2 + b_3 x_3 + \\cdots + b_n x_n\n b_i は係数，x_i はベクトル（変数, 説明変数）です。y が x_i の線型結合です。",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#線型モデルは直線じゃなくてもいい",
    "href": "glm.html#線型モデルは直線じゃなくてもいい",
    "title": "線形モデル",
    "section": "線型モデルは直線じゃなくてもいい",
    "text": "線型モデルは直線じゃなくてもいい\n線形モデル\n\n\\mu = \\beta_0 + \\beta_1 x_1\n\\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2^2\n\\mu = \\beta_0 + \\beta_1\\exp(x_1)\n\n非線形モデル\n\n\\mu = \\frac{\\beta_1 x_1}{\\beta_2 + x_1}\n\\mu = \\beta_1 \\left(1-\\exp(\\beta_2 x_1)\\right) + \\beta_3\n\\mu = \\beta_0 + \\beta_1\\exp(\\beta_2 x_1)\n\n線形モデルは直線じゃなくてもいい。 ただし，変数 (x_i) は線型結合であることが重要なポイントです。 では、検討しているモデルは線形モデルかどうかを確認したいなら、パラメータに対してモデルの偏微分方程式を求めてください。\nたとえば、モデルパラメータ (\\beta_i) に対して，\\muを微分します。\n\n\\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2^2\n\nパラメータの数ほど微分方程式があります。\n\n\\frac{\\partial \\mu}{\\partial \\beta_0}  = 1 \\qquad\n\\frac{\\partial \\mu}{\\partial \\beta_1}  = x_1 \\qquad\n\\frac{\\partial \\mu}{\\partial \\beta_2}  = x_2\n\n\\beta_i はどの微分方程式に残らないので，このモデルは線形モデルでしょう。\n次のモデルは非線形モデルです。\n\n\\mu = \\frac{\\beta_1 x_1}{\\beta_2 + x_1}\n\nモデルパラメータは \\beta_1, \\beta_2 ですね。 ではパラメータに対する偏微分方程式は次の通りです。\n\n\\begin{aligned}\n\\frac{\\partial \\mu}{\\partial \\beta_1}  &= \\frac{x_1}{\\beta_2 x_1} \\\\\n\\frac{\\partial \\mu}{\\partial \\beta_2}  &= -\\frac{x_1}{(\\beta_2 x_2)^2}\n\\end{aligned}\n\nパラメータはお互いの方程式に残るので，このモデルは線形モデルではないですね。",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#線形モデルの仮定",
    "href": "glm.html#線形モデルの仮定",
    "title": "線形モデル",
    "section": "線形モデルの仮定",
    "text": "線形モデルの仮定\n線形モデルを用いるときに守るべき仮定は分散分析と同じです。\n\n残差が正規分布に従うこと。\n残差またはデータはお互いに独立し，同一分布から発生していること。\n\nモデルに対する注意する点\n\n説明変数もお互いに関係性（相関関係）が低いこと（関係ないほうがいい）。\n\n関係性が高いとき，モデルパラメータの推定量の精度 (precision) と正確度 (accuracy) が落ちます。\nこの現象は多重共線性 (multicollinearity) とよびます。\n\n説明変数はお互いに関係がなければ，直交性 (orthogonal) のあるモデルとなり，変数の推定量はお互いとの相関性がないです。\n\n野外データは上の条件を満たすことは珍しいが、すべての条件を満たさなくても解析はできます。 ただし、結果の解釈と考察には気をつけましょう。",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#一般線形モデルの例-general-linear-model",
    "href": "glm.html#一般線形モデルの例-general-linear-model",
    "title": "線形モデル",
    "section": "一般線形モデルの例 (General Linear Model)",
    "text": "一般線形モデルの例 (General Linear Model)\n解析例にはアヤメのデータをつかっています。 ちなみに、一般線形モデルは一般化線形モデル (Generalized Linear Model, GLM) の特例です。\n\niris = iris |&gt; as_tibble()\niris |&gt; print(n = 3)\n\n# A tibble: 150 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n# … with 147 more rows\n\n\n検討する線型モデルは次の通りです。\n\nE(\\text{Petal Length}) = b_0 + b_1\\text{Petal Width} + b_2\\text{Sepal Length} + b_3\\text{Sepal Width}\n\nE(\\text{Petal Length}) は花びらの長さの期待値 (expected value) といいます。\n線形モデルなので，lm() 関数で解析できます。 さらに、分散分析表も存在します。\n\nm1 = lm(Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width, data = iris)\n\n分散分析表は summary.aov() でだします。\n\nm1 |&gt; summary.aov() |&gt; print(signif.stars = F)\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)\nPetal.Width    1  430.5   430.5 4231.49 &lt;2e-16\nSepal.Length   1    9.9     9.9   97.74 &lt;2e-16\nSepal.Width    1    9.0     9.0   88.95 &lt;2e-16\nResiduals    146   14.9     0.1               \n\n\nモデルに入れた全ての説明変数のP値は &lt; 0.0001 ですね。\nでは、線形モデルの係数表を出してみましょう。\n\nm1 |&gt; summary() |&gt; print(signif.stars = F)\n\n\nCall:\nlm(formula = Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width, \n    data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.99333 -0.17656 -0.01004  0.18558  1.06909 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  -0.26271    0.29741  -0.883    0.379\nPetal.Width   1.44679    0.06761  21.399   &lt;2e-16\nSepal.Length  0.72914    0.05832  12.502   &lt;2e-16\nSepal.Width  -0.64601    0.06850  -9.431   &lt;2e-16\n\nResidual standard error: 0.319 on 146 degrees of freedom\nMultiple R-squared:  0.968, Adjusted R-squared:  0.9674 \nF-statistic:  1473 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n\nPetal.Length に対して，Petal.Width と Sepal.Length は正の効果があり，Sepal.Width とは負の効果がありました。 つまり， Petal.Width と Sepal.Length が上昇すると， Petal.Length も上昇します。\n分散分析表の場合は、それぞれの変数に対するF値がでたが、係数表の場合は t値 がでました。 当然それぞれの値も異なった。\n何が違うのか？\nまず、分散分析の平方和って、さまざま求め方がるあることに気づきましょう。\n\nType-I Sum-of-squares\n\nSS(A), SS(B|A), SS(AB|B, A)\n\nType-II Sum-of-squares\n\nSS(A|B), SS(B|A)\n\nType-III Sum-of-squares\n\nSS(A|B), SS(B|A), SS(AB|B,A)\n\n\n他にもありますが、この 3 つが一般的にです。\nType-I の場合、結果は変数の順序に依存します。 Type-II の場合、順序に依存しないが、相互作用はない。 Type-III の場合、順序に依存しないが、相互作用の影響も計算されます。\nType-I はRのデフォルトなので、Type-II または Type-II を使いたいなら、 car パッケージの Anova() 関数が必要です。\nといくおとで、説明変数の順序をかえるた、次のように異なる結果が返ってきます。\n\nm1a  = lm(Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width, data = iris)\nm1b = lm(Petal.Length ~ Petal.Width + Sepal.Width + Sepal.Length, data = iris)\nm1c = lm(Petal.Length ~ Sepal.Width + Petal.Width + Sepal.Length, data = iris)\nm1a |&gt; summary.aov()\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \nPetal.Width    1  430.5   430.5 4231.49 &lt;2e-16 ***\nSepal.Length   1    9.9     9.9   97.74 &lt;2e-16 ***\nSepal.Width    1    9.0     9.0   88.95 &lt;2e-16 ***\nResiduals    146   14.9     0.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm1b |&gt; summary.aov()\n\n              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nPetal.Width    1  430.5   430.5 4231.49  &lt; 2e-16 ***\nSepal.Width    1    3.1     3.1   30.37 1.57e-07 ***\nSepal.Length   1   15.9    15.9  156.31  &lt; 2e-16 ***\nResiduals    146   14.9     0.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm1c |&gt; summary.aov()\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSepal.Width    1   85.2    85.2   837.8 &lt;2e-16 ***\nPetal.Width    1  348.3   348.3  3424.1 &lt;2e-16 ***\nSepal.Length   1   15.9    15.9   156.3 &lt;2e-16 ***\nResiduals    146   14.9     0.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nこのとき、car パッケージの Anova() 関数を使って平方和の求め方を指定します。 相互作用なしのモデルなので、Type-II を指定しました。 相互作用も調べたいなら、type=\"III\" を渡してください。\n\nm1a |&gt; Anova(type = \"II\")\n\nAnova Table (Type II tests)\n\nResponse: Petal.Length\n             Sum Sq  Df F value    Pr(&gt;F)    \nPetal.Width  46.584   1 457.905 &lt; 2.2e-16 ***\nSepal.Length 15.902   1 156.312 &lt; 2.2e-16 ***\nSepal.Width   9.049   1  88.947 &lt; 2.2e-16 ***\nResiduals    14.853 146                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm1b |&gt; Anova(type = \"II\")\n\nAnova Table (Type II tests)\n\nResponse: Petal.Length\n             Sum Sq  Df F value    Pr(&gt;F)    \nPetal.Width  46.584   1 457.905 &lt; 2.2e-16 ***\nSepal.Width   9.049   1  88.947 &lt; 2.2e-16 ***\nSepal.Length 15.902   1 156.312 &lt; 2.2e-16 ***\nResiduals    14.853 146                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm1c |&gt; Anova(type = \"II\")\n\nAnova Table (Type II tests)\n\nResponse: Petal.Length\n             Sum Sq  Df F value    Pr(&gt;F)    \nSepal.Width   9.049   1  88.947 &lt; 2.2e-16 ***\nPetal.Width  46.584   1 457.905 &lt; 2.2e-16 ***\nSepal.Length 15.902   1 156.312 &lt; 2.2e-16 ***\nResiduals    14.853 146                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n係数表は t値の結果は Wald’s test と呼びます。\n\nm1 |&gt; summary()\n\n\nCall:\nlm(formula = Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width, \n    data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.99333 -0.17656 -0.01004  0.18558  1.06909 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.26271    0.29741  -0.883    0.379    \nPetal.Width   1.44679    0.06761  21.399   &lt;2e-16 ***\nSepal.Length  0.72914    0.05832  12.502   &lt;2e-16 ***\nSepal.Width  -0.64601    0.06850  -9.431   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.319 on 146 degrees of freedom\nMultiple R-squared:  0.968, Adjusted R-squared:  0.9674 \nF-statistic:  1473 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n\nこのとき、\\beta = 0 の帰無仮説を検証しています。 つまり、係数の値は 0 か 0 じゃないかですね。\nWald’s test と F-test のこちらもモデルの解析に使えます。 ところが、一般的には、モデル選択に F-test を使いますが、 係数が 0 か 0 じゃないかの検証には Wald’s test を使います。 ちなみに、このモデルの場合、(t_\\nu)^2 = F_{(1,\\nu)} なので、 係数表のt値の2乗は Type-II 分散分析のF値と同じです。",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#モデル診断",
    "href": "glm.html#モデル診断",
    "title": "線形モデル",
    "section": "モデル診断",
    "text": "モデル診断\nモデルを当てはめたあと、最も重要な解析はモデル残渣の確認です。 モデル残渣に異常があると、モデルとデータの相性が悪いです。 モデルとデータが合わない場合、求めた係数を信用できません。\n一般線形モデルのとき、標準化残渣 (standardized residuals) をもとめて、モデルの良さを目で確認します。 つまり、標準化残渣を様々形の図を作ります。\nまず、残渣を持てめて、図に使うデータを準備します。\n\nfiris = fortify(m1) |&gt; as_tibble()\nfiris\n\n# A tibble: 150 × 10\n   Petal…¹ Petal…² Sepal…³ Sepal…⁴   .hat .sigma .cooksd .fitted  .resid .stdr…⁵\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1     1.4     0.2     5.1     3.5 0.0205  0.320 3.73e-4    1.48 -0.0842 -0.267 \n 2     1.4     0.2     4.9     3   0.0212  0.319 3.71e-3    1.66 -0.261  -0.828 \n 3     1.3     0.2     4.7     3.2 0.0201  0.320 3.84e-4    1.39 -0.0864 -0.274 \n 4     1.5     0.2     4.6     3.1 0.0221  0.320 8.46e-4    1.38  0.122   0.387 \n 5     1.4     0.2     5       3.6 0.0230  0.320 1.68e-4    1.35  0.0533  0.169 \n 6     1.7     0.4     5.4     3.9 0.0327  0.320 9.86e-5    1.73 -0.0339 -0.108 \n 7     1.4     0.3     4.6     3.4 0.0255  0.320 3.34e-4    1.33  0.0711  0.226 \n 8     1.5     0.2     5       3.4 0.0189  0.320 2.81e-5    1.48  0.0241  0.0763\n 9     1.4     0.2     4.4     2.9 0.0293  0.320 1.14e-4    1.36  0.0386  0.123 \n10     1.5     0.1     4.9     3.1 0.0225  0.320 1.32e-4    1.45  0.0479  0.152 \n# … with 140 more rows, and abbreviated variable names ¹​Petal.Length,\n#   ²​Petal.Width, ³​Sepal.Length, ⁴​Sepal.Width, ⁵​.stdresid\n\n\n\n残渣の正規性\nまずは残渣は正規分布に従うかを評価します。 正規性はヒストグラムとQQプロットでします。 残渣が正規分布に従うなら、ヒストグラムは正規分布に似ていて、 QQプロットでは残渣を示す点が赤色の直線上に並びます。\n\nggplot(firis) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(x = \"Standardized residual\", y = \"\") \n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n残渣のヒストグラムと正規分布\n\n\n\n\n\nggplot(firis) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(x = \"Theoretical Quantile\", y = \"Standardized residual\") \n\n\n\n\nQQプロット\n\n\n\n\nどちらの図を確認しても、標準化残渣は正規分布に従っているように見えます。\n\n\n残渣のばらつき\n正規性に問題がなければ、次は標準化残渣のばらつきを確認したいです。 すべての変数に対して残渣の図をつくります。 また、求めた期待値に対しても残渣を確認します。 残渣になんかしらのパタンがあると、問題です。\n\nfiris |&gt; \n  select(matches(\"Petal|Sepal\"), .stdresid) |&gt; \n  pivot_longer(matches(\"Petal|Sepal\")) |&gt; \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  facet_wrap(vars(name), scales = \"free\")\n\n\n\n\n標準化残渣と応答変数、それぞれの説明変数との関係\n\n\n\n\n残渣は 0 をまたいで均等にばらついていることが確認できました。 さらに、ばらつきに明確なパタンがないので、この点についてモデルには問題なさそうです。 次は残渣と期待値の関係を確認します。\n\nfiris |&gt; \n  select(.fitted, .stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") \n\n\n\n\n標準化残渣と期待値の関係\n\n\n\n\n期待値と標準化残渣の関係を確認すると、特に問題はないですね。 場合によって、標準化残渣の絶対値の平方根で確認しやすい場合もあります。 この解析の場合は不必要ですが、コードと結果は次の通りです。\n\nfiris |&gt; \n  select(.fitted, .stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid)))) \n\n\n\n\n標準化残渣の平方根と期待値の関係\n\n\n\n\n最後に、クックの距離を確認します。クックの距離 (Cook’s distance) はモデルの当てはめに強く影響する値を検出してくれます。 データ点のクックの距離が P(F_{(n, n-p)}=0.5) を超えた場合、ちょっと怪しいかもしれないです。\n\ndof = summary(m1) |&gt; pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\nfiris |&gt; \n  mutate(n = 1:n()) |&gt; \n  mutate(above = ifelse(.cooksd &gt; thold, T, F)) |&gt; \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\n診断図から問題ないと判断できたら、説明変数の多重鏡線性を確認しましょう。 まず，説明変数がお互いに相関があるかを確認します。\n\niris |&gt; select(-Species, - Petal.Length) |&gt; cor()\n\n             Sepal.Length Sepal.Width Petal.Width\nSepal.Length    1.0000000  -0.1175698   0.8179411\nSepal.Width    -0.1175698   1.0000000  -0.3661259\nPetal.Width     0.8179411  -0.3661259   1.0000000\n\n\nPetal.Width と Sepal.Length の相関は高いですが，他は 0 に近いので相関関係は低いです。 多重共線性をしっかりと確認したければ，分散拡大係数 (Variance Inflation Factor; VIF) をもとめます。\nVIF は決定係数 (R_i^2) をつかって計算するので，方程式は次の通りです。\n\n\\text{VIF} = \\frac{1}{1-R_i^2}\n\nVIF を計算するには，説明変数 x_i を他の説明変数 x_{j \\neq i} との線形モデル組み立てて，決定係数を求める必要があります。\nつまり，先ほどの相関係数の結果をつかうと，\n\nx1 = lm(Petal.Width ~ Sepal.Length + Sepal.Width, \n        data = iris) |&gt; summary() |&gt; pluck(\"r.squared\")\nx2 = lm(Sepal.Length ~ Petal.Width + Sepal.Width, \n        data = iris) |&gt; summary() |&gt; pluck(\"r.squared\")\nx3 = lm(Sepal.Width ~ Sepal.Length + Petal.Width, \n        data = iris) |&gt; summary() |&gt; pluck(\"r.squared\")\n1 / (1-c(x1,x2,x3)) # VIF\n\n[1] 3.889961 3.415733 1.305515\n\n\ncar パッケージの vif() 関数の方が便利です。\n\ncar::vif(m1)\n\n Petal.Width Sepal.Length  Sepal.Width \n    3.889961     3.415733     1.305515 \n\n\nVIF(\\beta_i) &gt; 10 であれば，多重共線性の問題は大きいと考えられます。 このときの決定係数は 1-1/10 = 0.90 です。",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#ガラパゴス諸島における種数の解析",
    "href": "glm.html#ガラパゴス諸島における種数の解析",
    "title": "線形モデル",
    "section": "ガラパゴス諸島における種数の解析",
    "text": "ガラパゴス諸島における種数の解析\niris データの解析に大きな問題がなかったので、あまり参考にならなかったので、 ガラパゴス諸島のデータを解析してみましょう。\nfaraway パッケージの gala を解析します。 gala にはガラパゴス諸島の生態系に対しての 7 つの変数があります。Species は植物の種数，Endemics は植物の固有種， Area は島の平面積 (m2)，Elevation は島の最も高い場所 (m)，Nearest は最も近い島からの距離 (km)，Scruz はサンタクルス島からの距離 (km)，Adjacent は最も近い島の面積 (m2)です。\n\ndata(gala, package = \"faraway\")\ngala = gala |&gt; as_tibble() # tibble に変換\ngala |&gt; print(n = 3) # 最初の 3 行を表示\n\n# A tibble: 30 × 7\n  Species Endemics  Area Elevation Nearest Scruz Adjacent\n    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1      58       23 25.1        346     0.6   0.6     1.84\n2      31       21  1.24       109     0.6  26.3   572.  \n3       3        3  0.21       114     2.8  58.7     0.78\n# … with 27 more rows\n\n\n解析する前に、データの可視化をします。\n\n\n\n\n\n\n\n\n\n説明変数 (explanatory variable)、または予測子 (Predictor) に対する種数の変動は予測子によって変わることがわかります。\n\ngala_out = gala |&gt; select(-Endemics) |&gt; gather(Variable, Predictor, -Species)\nggplot(gala_out) + geom_point(aes(x=Predictor, y=Species)) + facet_wrap(\"Variable\", scales = \"free_x\")\n\nでは、モデルと帰無仮設を決めます。\n「種数の増減は予測子に依存する」を作業仮説にしたとき，モデルは次のようになります。\n\nE(\\text{Species}) = b_0 + b_1\\text{Area}+b_2\\text{Elevation}+b_3\\text{Nearest}+b_4\\text{Scruz}+b_5\\text{Adjacent}\n\nネイマン＝ピアソン (Neyman-Pearson) の枠組みの中で解析するなら，帰無仮設と対立仮設を建てなければなりません。\n\n帰無仮設： b_0 = b_1 = b_2 = b_3 = b_4 = b_5\n対立仮説： b_0 \\neq b_1 \\neq b_2 \\neq b_3 \\neq b_4 \\neq b_5\n\n解析は次の通りです。 この度、帰無仮説のモデルもわさわさくみました。\n\nH0 = lm(Species ~ 1, data = gala)  # これが帰無仮設のモデルです。\nHF = lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala) # これが対立仮説のモデルです。\n\n相互作用を入れていないが、Type-III SS を求めます。\n\n# anova(H0, HF) # Type-I SS\nAnova(H0, HF, type = \"III\")   # Type=III SS\n\nAnova Table (Type III tests)\n\nResponse: Species\n            Sum Sq Df F value    Pr(&gt;F)    \n(Intercept) 217942  1  58.618 6.805e-08 ***\nResiduals    89231 24                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nP値は &lt; 0.0001 なので，0.05 より小さいです。帰無仮設は棄却できます。 帰無仮設を棄却したら，採択するモデル（モデルは採択できるが，仮説は採択できません）の 診断図を作図して，残差のばらつきや正規性などを確認します。\nでは、HF の診断図を確認しましょう。\n\nfgala = fortify(HF)\np1 = ggplot(fgala) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(x = \"Standardized residual\", y = \"\") \np2 = ggplot(fgala) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(x = \"Theoretical Quantile\", y = \"Standardized residual\") \np1 | p2\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nデータ数は少ないが，残差は 0 を中心にしていて左右対称です。 殆どの残差はQQプロットの直線に沿っています。 よって，残差の正規性に大きな問題はなさそうです。\n\nfgala |&gt; \n  select(Species, Area, Elevation, Nearest, Scruz, Adjacent, .stdresid) |&gt; \n  pivot_longer(-.stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  facet_wrap(vars(name), scales = \"free\")\n\n\n\n\n\n\n\n\n予測子に対する残差の分布を確認すると，均等に分布していないように見えます。 例えば Residual vs. Nearest の場合，波状を描いているように見えます。",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#モデルの診断図期待値に対する残差のばらつき",
    "href": "glm.html#モデルの診断図期待値に対する残差のばらつき",
    "title": "線形モデル",
    "section": "モデルの診断図：期待値に対する残差のばらつき",
    "text": "モデルの診断図：期待値に対する残差のばらつき\n\np1 = fgala |&gt; \n  select(.fitted, .stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") \np2 = fgala |&gt; \n  select(.fitted, .stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid)))) \np1 | p2\n\n\n\n\n\n\n\n\n残差のばらつきは期待値と関係性が有るように見えます (Residual vs. Fitted Values Plot)。 Scale–Location Plot では，その関係が明確です。\n次は異常値を探してみましょう。\n\ndof = summary(HF) |&gt; pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\nfgala |&gt; \n  mutate(n = 1:n()) |&gt; \n  mutate(above = ifelse(.cooksd &gt; thold, T, F)) |&gt; \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\nクックの距離が P(F_{(p, n-p)}=0.5) を超えれば，影響力の高い点だと考えられます。 このとき，16番目のデータが明らかに超えています。\nP(F_{(p, n-p)}=0.5)は自由度 p (パラメータの数) と n-p (データ数からパラメータ数の差) のときのF値の中央値です。\n\nwhich(cooks.distance(HF) &gt; thold)\n\n16 \n16 \n\n\nいろいろと問題があったので、モデルの改良は必要ですね。 ところが、モデルを改良するときの問題点を意識してください。\n帰無仮設は棄却できたが，診断図を確認すると多数の問題点がありました。 このようなとき，モデルを改良する必要があります。 ただし，ネイマン=ピアソンの帰無仮設検定法のとき，検討するモデルが増えれば増えるほど第１種の誤りを起こす確率も上がります。\n5 つの予測子（変数）があるので，交互作用ありの一次式のモデルの場合，パラメータの数は32 です。 検証できるパラメータ数はデータ数に制限されるので， パラメータ数とデータ数が等しいときのモデルは飽和モデルとよびます。 ちなみに，32 パラメータのときの第 1 種の誤りを起こす確率は 0.8063 です。 飽和モデルのとき，分散を推定することができないので，飽和モデルは理論上のモデルです。",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#モデル構築の考え方",
    "href": "glm.html#モデル構築の考え方",
    "title": "線形モデル",
    "section": "モデル構築の考え方",
    "text": "モデル構築の考え方\nモデルを設計するときに最も大事なことは：\n\n統計解析の仮定を守る\n科学的・統計学的にありえる\n説明しやすい\nシンプル・単純である\n\nモデルの改良点：\n\n応答変数を変換する\n説明変数を変換する\n誤差項の分布を変える\nモデルパラメータを増やす（二次関数・三次関数・交互作用など）\n\n\n応答変数の変換\n残差に問題があるとき，応答変数を変換することが一般的に行われています。 応答変数の変換は，残差を正規分布に従わせるためにします。\nたとえば個体数はつねに y\\ge 0 です。負の個体数は存在しません。 このとき，正規分布を仮定したら，0 近辺の値の 95% 信頼区間は負の値をとることもあります。 実データとの整合性がとれなくなります。 または，0 から 1 の値しか取れないデータのときでも正規性に問題がでます。\n応答変数が個体数のような正の整数のとき，ログ変換することが多いです。log(x) または，0 から 1 の比率の様なデータのとき，アークサイン変換をします。 asin(x)",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#応答変数変換した解析",
    "href": "glm.html#応答変数変換した解析",
    "title": "線形モデル",
    "section": "応答変数変換した解析",
    "text": "応答変数変換した解析\n応答変数をログ変換した解析と変換していない解析の結果はつぎの通りです。\nログ変換後の分散分析表\n\ngala = gala |&gt; mutate(logSpecies = log(Species))\nlogHF = lm(logSpecies~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala)\nlogHF |&gt; Anova(type = \"III\") |&gt; print(signif.stars = F)\n\nAnova Table (Type III tests)\n\nResponse: logSpecies\n            Sum Sq Df F value    Pr(&gt;F)\n(Intercept) 53.956  1 48.7039 3.238e-07\nArea         3.998  1  3.6088   0.06955\nElevation   26.168  1 23.6211 5.927e-05\nNearest      0.918  1  0.8289   0.37164\nScruz        1.217  1  1.0984   0.30505\nAdjacent     7.597  1  6.8575   0.01505\nResiduals   26.588 24                  \n\n\n変換なしの分散分析表\n\nHF |&gt; Anova(type = \"III\") |&gt; print(signif.stars = F)\n\nAnova Table (Type III tests)\n\nResponse: Species\n            Sum Sq Df F value    Pr(&gt;F)\n(Intercept)    506  1  0.1362 0.7153508\nArea          4238  1  1.1398 0.2963180\nElevation   131767  1 35.4404 3.823e-06\nNearest          0  1  0.0001 0.9931506\nScruz         4636  1  1.2469 0.2752082\nAdjacent     66406  1 17.8609 0.0002971\nResiduals    89231 24                  \n\n\n変数（パラメータ）に対するP値はが変わりました。 変換なしの解析に比べて，ログ変換後のElevation と Nearest のP値は下がりましたが，その他のP値は上がりました。\n\nfgala = fortify(logHF)\np1 = ggplot(fgala) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(x = \"Standardized residual\", y = \"\") \np2 = ggplot(fgala) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(x = \"Theoretical Quantile\", y = \"Standardized residual\") \np1 | p2\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nログ変換なしの結果よりちょっと良くなったと思います。\n\nfgala |&gt; \n  select(logSpecies, Area, Elevation, Nearest, Scruz, Adjacent, .stdresid) |&gt; \n  pivot_longer(-.stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  facet_wrap(vars(name), scales = \"free\")\n\n\n\n\n\n\n\n\n説明変数が上昇すると残差のばらつきが減少する傾向があるので，残差のばらつきに問題があります。\n\np1 = fgala |&gt; \n  select(.fitted, .stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") \np2 = fgala |&gt; \n  select(.fitted, .stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid)))) \np1 | p2\n\n\n\n\n\n\n\n\n期待値に対しても，残差のばらつきの均一性が問題です。さらに Scale - Location Plot には明らか傾向があります。\n\ndof = summary(logHF) |&gt; pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\nfgala |&gt; \n  mutate(n = 1:n()) |&gt; \n  mutate(above = ifelse(.cooksd &gt; thold, T, F)) |&gt; \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\nwhich(cooks.distance(logHF) &gt; thold)\n\n16 \n16 \n\n\n16番目のデータに対して，クックの距離は下がったが，PF_{(n, n-p)}=0.5) 以上のままなので，課題としてのこります。\n\n応答変数以外の解決手法\n応答変数をログ変換しても問題は解決できませんでした。 次に検討することは説明変数の変換または、削除です。 まずは，VIF の高い変数から外してみみます。\n\ncar::vif(logHF)\n\n     Area Elevation   Nearest     Scruz  Adjacent \n 2.928145  3.992545  1.766099  1.675031  1.826403 \n\n\nElevation の値が一番高かったので，Elevation なしのモデルを再解析します。\n\nlogHF2 = lm(logSpecies~ Area + Nearest + Scruz + Adjacent, data = gala)\ncar::vif(logHF2)\n\n    Area  Nearest    Scruz Adjacent \n1.047496 1.669984 1.658583 1.073529 \n\n\n診断図を確認したら，結果は良くなかったので，Nearest か Scruz も外します。 種数は島と島の間の距離に依存するかもしれないが，一つの島（Santa Cruz島）との距離の影響は考えにくいので，Scruz を外します。",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#単純化したモデル",
    "href": "glm.html#単純化したモデル",
    "title": "線形モデル",
    "section": "単純化したモデル",
    "text": "単純化したモデル\n結果として，つぎのモデルを解析することになりました。\n\nE(\\log(\\text{Species})) = b_0 + b_1\\text{Area}+b_2\\text{Nearest}+b_3\\text{Adjacent}\n\n\nlogSF = lm(logSpecies~ Area + Nearest + Adjacent, data = gala)\nlogSF |&gt; Anova(type = \"III\") |&gt; print(signif.stars = F)\n\nAnova Table (Type III tests)\n\nResponse: logSpecies\n             Sum Sq Df F value    Pr(&gt;F)\n(Intercept) 159.281  1 74.7879 3.983e-09\nArea         13.201  1  6.1981   0.01951\nNearest       2.372  1  1.1139   0.30095\nAdjacent      0.180  1  0.0847   0.77335\nResiduals    55.374 26                  \n\n\nArea 以外の変数のP値は 0.05 より高いです。",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#診断図",
    "href": "glm.html#診断図",
    "title": "線形モデル",
    "section": "診断図",
    "text": "診断図\n\nfgala = fortify(logSF)\np1 = ggplot(fgala) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(title = \"Histogram\", x = \"Standardized residual\", y = \"\") \np2 = ggplot(fgala) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(title = \"QQ plot\", x = \"Theoretical Quantile\", y = \"Standardized residual\") \np3 = fgala |&gt; \n  select(logSpecies, Area, Nearest, Adjacent, .stdresid) |&gt; \n  pivot_longer(-.stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Resid. vs. predictor\", x = \"Predictor value\", y = \"Standardized residual\") +\n  facet_wrap(vars(name), scales = \"free\")\np4 = fgala |&gt; \n  select(.fitted, .stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")  +\n  labs(title = \"Resid. vs. fit\", x = \"Fitted value\", y = \"Standardized residual\")\np5 = fgala |&gt; \n  select(.fitted, .stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid))))  +\n  labs(title = \"Scale - location\", x = \"Fitted value\", y = \"Standardized residual\")\ndof = summary(logSF) |&gt; pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\np6 = fgala |&gt; \n  mutate(n = 1:n()) |&gt; \n  mutate(above = ifelse(.cooksd &gt; thold, T, F)) |&gt; \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Cook's distance\", x = \"Index\", y = \"Cook's distance\")\n\np1 + p2 + p3 + p4 + p5 + p6\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n残差のばらつきや正規性は良くなったが，クックの距離の問題が残っています。",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#説明変数も変換する",
    "href": "glm.html#説明変数も変換する",
    "title": "線形モデル",
    "section": "説明変数も変換する",
    "text": "説明変数も変換する\nこんどは，説明変数も変換します。\n\nE(\\log(\\text{Species})) = b_0 + b_1\\log(\\text{Area})+b_2\\text{Nearest}+b_3\\log(\\text{Adjacent})\n\n\ngala = gala |&gt; mutate(logSpecies = log(Species), logArea = log(Area), logAdjacent = log(Adjacent))\nlogSF2 = lm(logSpecies~ logArea + Nearest + logAdjacent, data = gala)\nlogSF2 |&gt; Anova(type = \"III\") |&gt; print(signif.stars = F)\n\nAnova Table (Type III tests)\n\nResponse: logSpecies\n             Sum Sq Df  F value    Pr(&gt;F)\n(Intercept) 151.442  1 239.3574 1.247e-14\nlogArea      52.429  1  82.8653 1.445e-09\nNearest       0.618  1   0.9764    0.3322\nlogAdjacent   0.159  1   0.2512    0.6204\nResiduals    16.450 26                   \n\n\n\nfgala = fortify(logSF2)\np1 = ggplot(fgala) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(title = \"Histogram\", x = \"Standardized residual\", y = \"\") \np2 = ggplot(fgala) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(title = \"QQ plot\", x = \"Theoretical Quantile\", y = \"Standardized residual\") \np3 = fgala |&gt; \n  select(logSpecies, logArea, Nearest, logAdjacent, .stdresid) |&gt; \n  pivot_longer(-.stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") + \n  labs(title = \"Resid. vs. predictor\", x = \"Predictor value\", y = \"Standardized residual\") +\n  facet_wrap(vars(name), scales = \"free\")\np4 = fgala |&gt; \n  select(.fitted, .stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")  +\n  labs(title = \"Resid. vs. fit\", x = \"Fitted value\", y = \"Standardized residual\")\np5 = fgala |&gt; \n  select(.fitted, .stdresid) |&gt; \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid))))  +\n  labs(title = \"Scale - location\", x = \"Fitted value\", y = \"Standardized residual\")\ndof = summary(logSF) |&gt; pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\np6 = fgala |&gt; \n  mutate(n = 1:n()) |&gt; \n  mutate(above = ifelse(.cooksd &gt; thold, T, F)) |&gt; \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Cook's distance\", x = \"Index\", y = \"Cook's distance\")\n\np1 + p2 + p3 + p4 + p5 + p6\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n残差のばらつき，正規性，クックの距離の問題は解決できました。\n結果\n\nlogSF2 |&gt; Anova(type = \"III\") |&gt; print(signif.stars = F)\n\nAnova Table (Type III tests)\n\nResponse: logSpecies\n             Sum Sq Df  F value    Pr(&gt;F)\n(Intercept) 151.442  1 239.3574 1.247e-14\nlogArea      52.429  1  82.8653 1.445e-09\nNearest       0.618  1   0.9764    0.3322\nlogAdjacent   0.159  1   0.2512    0.6204\nResiduals    16.450 26                   \n\n\n帰無仮設の有意性検定によると，logArea と Nearest の効果は有意ですが，logAdjacent の効果は有意じゃない。\nところが，この結果まで導くには，5 種類のモデルを検証しました。\\alpha_\\text{fwer} は 1 - (1-0.05)^5 = 0.2262 ですので，\\alpha_\\text{fwer}=0.05 にしたければ，\\alpha = 0.0102 に設定しなければなりません。このとき，logArea 以外の要因の効果は有意ではありません。",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#一般化線形モデル",
    "href": "glm.html#一般化線形モデル",
    "title": "線形モデル",
    "section": "一般化線形モデル",
    "text": "一般化線形モデル\n一般化線形モデル (GLM) は今までの線型モデルと同じように，線型結合した変数から成り立っています。\nさらに，\n\n正規分布を仮定した線型モデルのとき，予測残差は Residuals (残差) とよびましたが，GLMの予測残差は Deviance (尤離度・逸脱度・デビアンス) とよびます。\n正規分布以外の指数型分布族にぞくする分布も使えます。\n一般化線形モデルに 3 つの成分が存在します。\n\n誤差構造またはランダム成分 (random component)\n線型予測子 (linear predictor)または系統成分(systematic component)\n連結関数またはリンク関数 (link function)",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#指数型分布族誤差項",
    "href": "glm.html#指数型分布族誤差項",
    "title": "線形モデル",
    "section": "指数型分布族・誤差項",
    "text": "指数型分布族・誤差項\n\nf(y|\\theta, \\phi) = \\exp\\left(\\frac {y\\theta - b(\\theta)} {a(\\phi)} + c(y, \\phi)\\right)\n\n\\theta は正準パラメータ (canonical parameter)，\\phi はばらつきのパラメータ (dispersion parameter, 分散パラメータ)，a(\\cdot), b(\\cdot), c(\\cdot) は存知の関数です。さらに，平均値は \\mu = E(y) = b'(\\theta)，分散は var(y) = a(\\phi) b''(\\theta) です。つまり，平均値は \\theta の関数できまるが，分散は二つの関数の積です。a(\\phi) = \\phi/w であれば，v(y) = \\phi b''(\\theta)/w であり，分散関数とよびます。正規分布の場合，分散と平均値は独立しているのと，\\phi=1 なので，var(y) = 1/w です。w は存知の重みです。\n\n線形予測子または系統成分\n応答変数における予測子の影響は次のように書けます。\n\n\\eta = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n\n\nこの線形予測子は一般線形モデルと同じ用に構築します。\n\n\n連結関数 (リンク関数)\n応答変数の期待値と予測子の関係はリンク関数を通して関係づけます。\n\ng(\\mu) = \\eta\n\nリンク関数は単調な微分可能な関数です。\n\n\n一般化線形モデルの推定方法\n一般化線形モデルのパラメータ推定には最尤推定 (maximum likelihood estimation; maximum likelihood method; 最尤推定法) を用います。\ny_1, \\dots, y_n は n 数の独立な確率変数とします。さらに，y_i は同じパラメータの正規分布に従うとしたら，同時確立分布はつぎの通りです。\n\nP(y_1, \\dots, y_n | \\mu, \\sigma^2) = \\prod_{i=1}^{N} \\exp\\left(y_i\\frac{\\mu}{\\sigma^2} -\\frac{\\mu^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{y_i^2}{2\\sigma^2}\\right) = \\mathcal{L}(\\mu, \\sigma^2 | y_1, \\dots, y_n)\n\nここの \\mathcal{L}(\\cdots) が尤度関数 (log-likelihood function)。両側の自然対数を求めれば，対数尤度関数 (log-likelihood function) に導きます。\n\n\\log(\\mathcal{L}(\\mu,\\sigma^2)) = - \\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(y_i - \\mu)^2\n\nこの対数尤度関数が最大または負の対数尤度関数 (negative log-likelihood function) が最小になるパラメータをさがすことが目的です。\n\n\n一般的な誤差項\n連続型確率分布\n\n正規分布\nガンマ分布\nベータ分布\n指数分布\n\n離散型確率分布\n\nポアソン分布\n二項分布\n負の二項分布\ncategorical 分布\n\n他にありますが，上記の分布が一般的につかわれています。",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#ガラパゴス諸島のデータのglm解析例",
    "href": "glm.html#ガラパゴス諸島のデータのglm解析例",
    "title": "線形モデル",
    "section": "ガラパゴス諸島のデータのGLM解析例",
    "text": "ガラパゴス諸島のデータのGLM解析例\nまず，帰無仮設の気無モデル（ヌルモデル）を組み立てます。\n\n\\begin{aligned}\n\\text{Species} &\\sim Pois(\\mu)\\\\\nE(\\text{Species}) &= \\mu \\\\\nE(\\text{Species}) &= \\exp(\\eta) \\\\\n\\eta &= b_0  \\\\\n\\end{aligned}\n\n\\eta = b_0 は切片のみのモデルです。\n\nポアソンGLM解析の出力\n\nH0_poisson = glm(Species ~ 1, data = gala, family = poisson(link = \"log\"))\nH0_poisson |&gt; summary() |&gt; print(signif.stars = F)\n\n\nCall:\nglm(formula = Species ~ 1, family = poisson(link = \"log\"), data = gala)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-12.307   -9.782   -5.200    1.142   27.351  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  4.44539    0.01978   224.8   &lt;2e-16\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3510.7  on 29  degrees of freedom\nResidual deviance: 3510.7  on 29  degrees of freedom\nAIC: 3673.6\n\nNumber of Fisher Scoring iterations: 6\n\n\nパラメータの推定量の表の後に，(Dispersion parameter for poisson family taken to be 1) の記述があります。 これは，ポアソン分布のばらつきのパラメータ (\\phi) を 1 に設定したと意味します。 ヌルモデルを当てはめたので，ヌル逸脱度 (Null Deviance) と残渣逸脱度 (Residual Deviance) は同じです。\n\n\n対立モデル\n次は対立モデルです。\n\n\\begin{aligned}\n\\text{Species} &\\sim Pois(\\mu)\\\\\nE(\\text{Species}) &= \\mu \\\\\nE(\\text{Species}) &= \\exp(\\eta) \\\\\n\\eta &= b_0 + b_1\\text{Area}+b_2\\text{Elevation}+b_3\\text{Nearest}+b_4\\text{Scruz}+b_5\\text{Adjacent}\\\\\n\\end{aligned}\n\n\n\n対立モデルの出力\n\nHF_poisson = glm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, \n                 data = gala, family = poisson(link = \"log\"))\nHF_poisson |&gt; summary() |&gt; print(signif.stars = F)\n\n\nCall:\nglm(formula = Species ~ Area + Elevation + Nearest + Scruz + \n    Adjacent, family = poisson(link = \"log\"), data = gala)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-8.2752  -4.4966  -0.9443   1.9168  10.1849  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  3.155e+00  5.175e-02  60.963  &lt; 2e-16\nArea        -5.799e-04  2.627e-05 -22.074  &lt; 2e-16\nElevation    3.541e-03  8.741e-05  40.507  &lt; 2e-16\nNearest      8.826e-03  1.821e-03   4.846 1.26e-06\nScruz       -5.709e-03  6.256e-04  -9.126  &lt; 2e-16\nAdjacent    -6.630e-04  2.933e-05 -22.608  &lt; 2e-16\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  716.85  on 24  degrees of freedom\nAIC: 889.68\n\nNumber of Fisher Scoring iterations: 5\n\n\n最初に当てはめた線形モデルの結果と全く違います。",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#尤度比検定とaicにおけるモデル比較",
    "href": "glm.html#尤度比検定とaicにおけるモデル比較",
    "title": "線形モデル",
    "section": "尤度比検定とAICにおけるモデル比較",
    "text": "尤度比検定とAICにおけるモデル比較\n今までは，正規分布を仮定した解析手法をつかっていたので，比較はF検定で行いました。 ポアソンGLMののとき，尤度比検定で，帰無モデル（ヌルモデル）と対立モデルを比較します。\n\nanova(H0_poisson, HF_poisson, test = \"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: Species ~ 1\nModel 2: Species ~ Area + Elevation + Nearest + Scruz + Adjacent\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        29     3510.7                          \n2        24      716.8  5   2793.9 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAICでも比較できます。\n\nAIC(H0_poisson, HF_poisson)\n\n           df       AIC\nH0_poisson  1 3673.5596\nHF_poisson  6  889.6767\n\n\n尤度比検定のとき，NHSTの検証になるので，帰無仮設を棄却することになります。 AICのとき，モデル選択するので，AICの最も低いモデルを採択します。 正規分布やガンマ分布などの他の分布をつかったときでも，尤度比検定やAICを用いてもいいです。 もちろん，正規分布の場合，F検定でも問題はありません (このとき，test = \"F\")。\n\nポアソン分布のときに必ず確認する値\nポアソン分布のGLMを実施したあと，必ず確認しなければならい値は過分散 (over-dispersion)です。 過分散があるとき，パラメータの標準誤差，信頼区間，モデルの予測値は誤っています。 過分散をパラメータとして推定するモデル（正規分布やガンマ分布など）の場合は，過分散の推定を無視できます。 この点については https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html を参考にしてください。\n\noverdisp_fun = function(model) {\n    rdf = df.residual(model)\n    rp = residuals(model,type=\"pearson\")\n    Pearson.chisq = sum(rp^2)\n    prat = Pearson.chisq/rdf\n    pval = pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)\n    mu = mean(predict(model, type = \"response\"))\n    c(mu=mu, chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)\n}\noverdisp_fun(HF_poisson)\n\n           mu         chisq         ratio           rdf             p \n 8.523333e+01  7.619792e+02  3.174914e+01  2.400000e+01 2.187190e-145 \n\n\n感覚的に考えると，モデル結果の Residual Deviance の値が degrees of freedom の値と同じであれば，過分散は存在しません。 ただし，どの手法をつかっても，平均値は 5 以上じゃなければなりません。 上記のP値はとても小さいので，過分散が存在すると考えられます。 または，Residual Deviance (761.98) は自由度 (24) より大きいので，検定をしなくても過分散の問題は明らかです 過分散はデータがポアソン分布に従わないときとモデル変数が不十分なときに起こります。過小分散も存在しますが，過分散ほどの問題ではありません。\n\n\nモデルの改良\n過分散の問題があったので，診断図を確認するよりも，他のモデルを当てはめてみます。 ここでは説明変数のログ変換したモデルを解析します。\n\nH2_poisson = glm(Species ~ logArea + Nearest + logAdjacent, \n                 data = gala, family = poisson(link = \"log\"))\nH2_poisson |&gt; summary() |&gt; print(signif.stars = F)\n\n\nCall:\nglm(formula = Species ~ logArea + Nearest + logAdjacent, family = poisson(link = \"log\"), \n    data = gala)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-5.6281  -3.4817  -0.3344   2.7419   8.2056  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  3.361898   0.046784  71.860  &lt; 2e-16\nlogArea      0.371771   0.007961  46.698  &lt; 2e-16\nNearest     -0.006244   0.001312  -4.759 1.94e-06\nlogAdjacent -0.097529   0.006106 -15.974  &lt; 2e-16\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  371.78  on 26  degrees of freedom\nAIC: 540.61\n\nNumber of Fisher Scoring iterations: 5\n\n\nResidual deviance と degrees of freedom を確認すると，371.78\\; /\\; 26 &gt; 1 なので，過分散が残っています。 ガラパゴス諸島の種数のデータに対して，どのように変換を変換しても，過分散は &gt;1 です。\n\n\n過分散をパラメータとして扱う\n何をしても過分散が残るとき，過分散をパラメータ化することができます。 ポアソン分布で説明できなかった分散を負の二項分布で説明できるかもしれません。\n【重要】MASS パッケージのselect() 関数は tidyverse の select() 関数と同じ名前なので、 あとに読み込んだパッケージの関すが優先されます。 つまり、MASS パッケージは tidyverse の先に読み込みましょう。 あるいは、不要になったらディタッチ (detach) しましょう。\n\ndetach(name = package:MASS)\n\n\nH2_negbinom = MASS::glm.nb(Species ~ logArea + Nearest + logAdjacent, data = gala, link = \"log\")\nH2_negbinom |&gt; summary() |&gt; print(signif.stars = F)\n\n\nCall:\nMASS::glm.nb(formula = Species ~ logArea + Nearest + logAdjacent, \n    data = gala, link = \"log\", init.theta = 2.83714993)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2174  -0.9672  -0.3006   0.5165   2.0350  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  3.369739   0.157235  21.431   &lt;2e-16\nlogArea      0.364309   0.035028  10.400   &lt;2e-16\nNearest     -0.012389   0.008211  -1.509    0.131\nlogAdjacent -0.034399   0.036005  -0.955    0.339\n\n(Dispersion parameter for Negative Binomial(2.8371) family taken to be 1)\n\n    Null deviance: 144.45  on 29  degrees of freedom\nResidual deviance:  32.82  on 26  degrees of freedom\nAIC: 284.88\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.837 \n          Std. Err.:  0.830 \n\n 2 x log-likelihood:  -274.884 \n\n\n誤差項の分布は負の二項分布にしたので，分散は Variance = \\mu + \\frac{\\mu}{\\theta} として推定されます。 このモデルの\\theta は 2.8371 です。 このモデルの解析を進めたいので，次は診断図の確認です。\n\n\n負の二項分布GLMの診断図\n診断図はよくつくるので、診断図用の関数を定義します。\n\n\n診断図関数：残差のヒストグラム\n\n# 残差のヒストグラム\ngg_resid_hist = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) |&gt; as_tibble()\n  if(class(fitted.model)[1] != \"lm\") {\n    data = data |&gt; mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data |&gt; mutate(residual = qresid)\n    xlabel = \"Randomized Quantile Residuals\"\n  } else {\n    data = data |&gt; mutate(residual = .resid)\n    xlabel = \"Standardized Residuals\"\n  }\n  ggplot(data) +\n    geom_histogram(aes(x = residual)) +\n    labs(x = xlabel) + ggtitle(\"Histogram of Residuals\")\n}\n\n\n\n診断図関数：残差のQQプロット\n\n# 残差のQQプロット\ngg_qq = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) |&gt; as_tibble()\n   if(class(fitted.model)[1] != \"lm\") {\n    data = data |&gt; mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data |&gt; mutate(residual = qresid)\n    ylabel = \"Randomized Quantile Residuals\"\n  } else {\n    data = data |&gt; mutate(residual = .stdresid)\n    ylabel = \"Standardized Residuals\"\n  }\n  ggplot(data) +\n    geom_qq(aes(sample =residual)) +\n    geom_abline(color = \"red\") +\n    labs(x = \"Theoretical Quantile\", y = ylabel) +\n    ggtitle(\"Normal-QQ Plot\")\n}\n\n\n\n診断図関数：変数に対する残差のばらつき\n\n# 変数に対する残差のプロット\ngg_resX = function(fitted.model, ncol=NULL, ...) {\n  require(tidyvers)\n  require(statmod)\n  residlab = function(string) {\n    sprintf(\"Residuals vs. %s\", string)\n  }\n  data = fortify(fitted.model) |&gt; as_tibble()\n  varnames = as.character(formula(fitted.model)) |&gt; pluck(3)\n  varnames = str_split(varnames, \" \\\\+ \") |&gt; pluck(1)\n  if(class(fitted.model)[1] != \"lm\") {\n    data = data |&gt; mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data |&gt; mutate(residual = qresid)\n    ylabel = \"Randomized Quantile Residuals\"\n  } else {\n    data = data |&gt; mutate(residual = .resid)\n    ylabel = \"Standardized Residuals\"\n  }\n  varnames = names(data)[names(data) %in% varnames]\n  data = data |&gt; dplyr::select(varnames, residual) |&gt; gather(var, value, varnames)\n  ggplot(data) + \n    geom_point(aes(x = value, y = residual)) +\n    geom_hline(yintercept=0, color = \"red\", linetype = \"dashed\") +\n    labs(x=\"Value\", y = ylabel) +\n    facet_wrap(\"var\", labeller=labeller(var=residlab), scales = \"free_x\",\n               ncol = ncol)\n}\n\n\n\n診断図関数：期待値に対する残差のばらつき\n\n# 期待値に対する残差のプロット\ngg_resfitted = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) |&gt; as_tibble()\n  if(class(fitted.model)[1] != \"lm\") {\n    data = data |&gt; mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data |&gt; mutate(residual = qresid)\n    ylabel = \"Randomized Quantile Residuals\"\n  } else {\n    data = data |&gt; mutate(residual = .resid)\n    ylabel = \"Standardized Residuals\"\n  }\n  ggplot(data) + \n    geom_point(aes(x = .fitted, y = residual)) +\n    geom_hline(yintercept=0, color = \"red\", linetype = \"dashed\") +\n    labs(x=\"Fitted Values\", y = ylabel) +\n    ggtitle(\"Residuals vs. Fitted Values\")\n}\n\n\n\n診断図関数：スケール・ロケーションプロット\n\n# スケール・ロケーションプロット\ngg_scalelocation = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) |&gt; as_tibble()\n  if(class(fitted.model)[1] != \"lm\") {\n    data = data |&gt; mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data |&gt; mutate(residual = qresid)\n    ylabel = expression(sqrt(\"|RQR|\"))\n  } else {\n    data = data |&gt; mutate(residual = .resid)\n    ylabel = expression(sqrt(\"|Standardized Residuals|\"))\n  }\n  ggplot(data) + \n    geom_point(aes(x = .fitted, y = sqrt(abs(residual)))) +\n    geom_smooth(aes(x = .fitted, y = sqrt(abs(residual))), \n                se = F, color = \"red\", linetype = \"dashed\") +\n    labs(x=\"Fitted Values\", y = ylabel) +\n    ggtitle(\"Scale - Location Plot\")\n}\n\n\n\n診断図関数：クックの距離\n\n# クックの距離\ngg_cooksd = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) |&gt; as_tibble()\n  data = data |&gt; mutate(n = seq_along(.cooksd))\n  dof = summary(fitted.model) |&gt; pluck(\"df\")\n  thold = qf(0.5, dof[1], dof[2])\n  data2 = data |&gt; mutate(above = ifelse(.cooksd &gt; thold, T, F)) |&gt; filter(above)\n  ggplot(data) + \n    geom_point(aes(x = n, y = .cooksd)) +\n    geom_segment(aes(x = n, y = 0, xend = n, yend = .cooksd)) +\n    geom_hline(yintercept=thold, color = \"red\", linetype = \"dashed\") +\n    geom_text(aes(x = n, y = .cooksd, label = n), data = data2, nudge_x=3, color = \"red\") +\n    labs(x=\"Sample\", y = \"Cook's Distance\") +\n    ggtitle(\"Cook's Distance Plot\")\n}\n\n残差の確認は線形モデルと同じようにしますが，解析に使う残差は ダン=スミス残差 (Dunn-Smyth Residuals; randomized quantile residuals) です。 とくに，ポアソンGLMと疑似ポアソンGLMのダン=スミス残差を求めます。 ダン=スミス残差は残差の累積分布関数を用いて残差のランダム化を行います。 ランダム化した残差は正規分布に近似するので，QQプロットで使用した分布のよさを評価できます。 ]ランダム化残差が直線に沿っていたら，分布に問題がないと示唆します。 左側はポアソン分布GLMのランダム化残差のQQプロットです。右側は負の二項分布GLMのランダム化残差のQQプロットです。\n\np1 = gg_qq(H2_poisson)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\n\nLoading required package: statmod\n\np2 = gg_qq(H2_negbinom)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\np1 | p2\n\n\n\n\n\n\n\n\n\n\nη に対するランダム化残差のばらつき\nランダム化残差と \\eta の間に明確な関係はありません。 \\sqrt{\\left(|\\text{Randomized Quantile Residuals}~\\right)} は若干山形な形をしています。 ところが，期待値の両端のデータが点線を引っ張っているように見えるので， 明確ではない。\n\np1 = gg_resfitted(H2_negbinom)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\np2 = gg_scalelocation(H2_negbinom)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\np1 | p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n全てクックの距離はP(F_{(n, n-p)}=0.5) より低いので，モデルを引っ張る点はありません。\n\ngg_cooksd(H2_negbinom)",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "glm.html#結果",
    "href": "glm.html#結果",
    "title": "線形モデル",
    "section": "結果",
    "text": "結果\n診断図と過分散の確認から，次のモデルにたどり着きました。\n\n\\begin{aligned}\n\\text{Species} &\\sim NegBin(\\mu)\\\\\nE(\\text{Species}) &= \\mu \\\\\nE(\\text{Species}) &= \\exp(\\eta) \\\\\nVariance &= \\mu + \\frac{\\mu}{\\theta} \\\\\n\\eta &= b_0 + b_1\\log(\\text{Area})+b_3\\text{Nearest}+b_5\\log(\\text{Adjacent})\\\\\n\\end{aligned}\n\n\nAIC(H2_poisson, H2_negbinom)\n\n            df      AIC\nH2_poisson   4 540.6134\nH2_negbinom  5 284.8838\n\n\nポアソン分布GLMのAICが高いので，負の二項分布GLMを採択します。\nCross Validated の参考資料 (Why is the quasi-Poisson in GLM not treated as a special case of negative binomial?)[https://stats.stackexchange.com/questions/157575/why-is-the-quasi-poisson-in-glm-not-treated-as-a-special-case-of-negative-binomi]\n\n負の二項分布の結果（係数表）\n係数表だけ出力しました。\n\nH2_negbinom |&gt; summary() |&gt; coefficients() |&gt; print(digits = 3)\n\n            Estimate Std. Error z value  Pr(&gt;|z|)\n(Intercept)   3.3697    0.15723  21.431 6.83e-102\nlogArea       0.3643    0.03503  10.400  2.47e-25\nNearest      -0.0124    0.00821  -1.509  1.31e-01\nlogAdjacent  -0.0344    0.03600  -0.955  3.39e-01\n\n# H2_negbinom |&gt; summary() # 全情報の出力\n\n切片と logArea の効果ははっきりしています (P &lt; 0.0001)。ところが，Nearest と logAdjacent のP値は &gt;0.05 でした。 さらに，変数をへらしてもAICは大きく変わりません。\n\n\nAIC\n\nH2_negbinom2 = MASS::glm.nb(Species ~ logArea + Nearest , data = gala, link = \"log\")\nH2_negbinom3 = MASS::glm.nb(Species ~ logArea +  logAdjacent, data = gala, link = \"log\")\nH2_negbinom4 = MASS::glm.nb(Species ~ logArea , data = gala, link = \"log\")\nAIC(H2_negbinom, H2_negbinom2, H2_negbinom3, H2_negbinom4) |&gt; as_tibble(rownames = \"model\") |&gt; arrange(AIC)\n\n# A tibble: 4 × 3\n  model           df   AIC\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 H2_negbinom2     4  284.\n2 H2_negbinom4     3  284.\n3 H2_negbinom      5  285.\n4 H2_negbinom3     4  285.\n\n\n最も小さいAICは H2_negbinom2 になりましたがAICがの差が 0 ~ 7 の範囲に入るので，どのモデルでもいいです。 このとき，尤度比検定もした方がいい。\n\n\n尤度比検定\n\nanova(H2_negbinom, H2_negbinom2, H2_negbinom3, H2_negbinom4, test = \"LRT\")\n\nWarning in anova.negbin(H2_negbinom, H2_negbinom2, H2_negbinom3, H2_negbinom4,\n: only Chi-squared LR tests are implemented\n\n\nLikelihood ratio tests of Negative Binomial Models\n\nResponse: Species\n                            Model    theta Resid. df    2 x log-lik.   Test\n1                         logArea 2.533343        28       -277.7721       \n2               logArea + Nearest 2.730433        27       -275.7711 1 vs 2\n3           logArea + logAdjacent 2.619569        27       -276.9859 2 vs 3\n4 logArea + Nearest + logAdjacent 2.837150        26       -274.8838 3 vs 4\n     df  LR stat.   Pr(Chi)\n1                          \n2     1  2.000994 0.1571961\n3     0 -1.214826 1.0000000\n4     1  2.102117 0.1470954\n\n\n自由度の高い順からペア毎の尤度比検定を行っています。E(\\text{Species})\\sim b_0 + b_1\\text{logArea} のモデルでいいかもしれないです。\n\n\nモデル診断図\n\np1 = gg_qq(H2_negbinom4)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\np2 = gg_resX(H2_negbinom4, ncol = 2)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(varnames)\n\n  # Now:\n  data %&gt;% select(all_of(varnames))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\np3 = gg_resfitted(H2_negbinom4)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\np4 = gg_cooksd(H2_negbinom4)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\np1 + p2 + p3 + p4\n\n\n\n\n\n\n\n\nモデル診断図を確認したら，残差の問題はないですので，E(\\text{Species})\\sim b_0 + b_1\\text{logArea} のモデルを採択することになりました。\n\n\n採択したモデル\n\nH2_negbinom4 |&gt; summary()\n\n\nCall:\nMASS::glm.nb(formula = Species ~ logArea, data = gala, link = \"log\", \n    init.theta = 2.533342912)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9737  -0.9216  -0.2155   0.5056   1.8969  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.22480    0.13669  23.592   &lt;2e-16 ***\nlogArea      0.34989    0.03543   9.877   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.5333) family taken to be 1)\n\n    Null deviance: 130.161  on 29  degrees of freedom\nResidual deviance:  32.604  on 28  degrees of freedom\nAIC: 283.77\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.533 \n          Std. Err.:  0.719 \n\n 2 x log-likelihood:  -277.772 \n\n\n\nndata = gala |&gt; expand(logArea = seq(min(logArea), max(logArea), length = 21)) \nndata = bind_cols(ndata, predict(H2_negbinom4, newdata = ndata, type = \"link\", se = T) |&gt; as_tibble())\nndata = ndata |&gt; \n  mutate(expect = exp(fit),\n         lower = exp(fit - se.fit),\n         upper = exp(fit + se.fit))\n\n\ngala |&gt; \n  ggplot(aes(x = logArea)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), data = ndata, alpha = 0.5) +\n  geom_point(aes(y = Species)) +\n  geom_line(aes(y = expect), data = ndata)",
    "crumbs": [
      "線形モデル",
      "線形モデル"
    ]
  },
  {
    "objectID": "anova2.html",
    "href": "anova2.html",
    "title": "多数群の比較：二元配置分散分析",
    "section": "",
    "text": "二元配置分散分析 (Two-Way ANOVA) 2 種類の因子（要因）を同時に比較するときに使用する。\n二元配置分散分析の帰無仮説\nフルモデルは:\nx_{ijk} = \\mu_{Ai}+\\mu_{Bj} + \\mu_{ABij} + \\epsilon_{ijk}\n水準 i, j, とサンプル k の値は x_{ijk}。 因子 A の水準 i ごとの平均値は \\mu_{Ai}。 因子 B の水準 i ごとの平均値は \\mu_{Bi}。\n交互作用 AB の i,j 効果の平均値は \\mu_{ABij}。 残渣（誤差項）は \\epsilon_{ijk}。",
    "crumbs": [
      "線形モデル",
      "多数群の比較：二元配置分散分析"
    ]
  },
  {
    "objectID": "anova2.html#two-way-anova-table-二元配置分散分析表",
    "href": "anova2.html#two-way-anova-table-二元配置分散分析表",
    "title": "多数群の比較：二元配置分散分析",
    "section": "Two-Way ANOVA Table (二元配置分散分析表)",
    "text": "Two-Way ANOVA Table (二元配置分散分析表)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFactor\nDegrees-of-freedom (df)\nSum-of-Squares (SS)\nMean-square (MS)\nF-value\nP-value\n\n\n\n\nA\ndf_A = I-1\nSS_A\nMS_A = SS_A / df_A\nMS_A / MS_R\nqf(1-\\alpha, df_A, df_R)\n\n\nB\ndf_B = J-1\nSS_B\nMS_B = SS_B / df_B\nMS_B / MS_R\nqf(1-\\alpha, df_B, df_R)\n\n\nAB\ndf_{AB} = (I-1)(J-1)\nSS_{AB}\nMS_{AB} = SS_{AB} / df_{AB}\nMS_{AB} / MS_R\nqf(1-\\alpha, df_{AB}, df_R)\n\n\ne\ndf_R = IJ(K-1)\nSS_R\nMS_R = SS_R / df_R\n\n\n\n\n\ndf_T = IJK-1\nSS_T\n\n\n\n\n\n\n\n\n\nA と B は主効果、 e は残渣、 I と J は各因子の水準、 K はサンプル数です。 SS_A と SS_B は水準間平方和、 SS_{AB} は相互作用平方和、 SS_R は残渣平方和、 SS_T は総平方和です。 MS_A と MS_B は水準間平均平方、 MS_{AB} は相互作用平均平方、 MS_R は残渣平均平方です。 平均平方の比率はF値です。\n\n\n\n\n\n\n平方和は他にもある\n\n\n\n上述した分散分析表は Type I 平方和 (SS) を求めています。 このとき、SS(A), SS(B|A), SS(AB|A,B) です。 分散分析の結果は因子に順序とに依存し、非釣り合い型データに合わない。\nType II 平方和は、SS(A|B) と SS(B|A) のみです。相互作用はありません。 分散分析の結果は因子に順序とに依存しないが、非釣り合い型データに合わない。\nType III 平方和は、SS(A|B, AB), SS(B|A, AB), SS(AB|A,B) です。 分散分析の結果は因子に順序とに依存しない、非釣り合い型データにも使えるが、 必ずcontr.sum を設定しなければならない。\n\n\n平方和の非釣り合い型データの問題については、Hector, Felten, and Schmid (2010), Langsrud (2003) を参考にしてください。\n\nHector, Andy, Stefanie von Felten, and Bernhard Schmid. 2010. “Analysis of variance with unbalanced data: An update for ecology & evolution.” Journal of Animal Ecology 79: 308–16. https://doi.org/10.1111/j.1365-2656.2009.01634.x.\n\nLangsrud, Øyvind. 2003. “ANOVA for unbalanced data: Use Type II instead.” Statistics and Computing 13: 163–67. https://doi.org/10.1023/A:1023260610025.",
    "crumbs": [
      "線形モデル",
      "多数群の比較：二元配置分散分析"
    ]
  },
  {
    "objectID": "anova2.html#type-i-平方和の方程式",
    "href": "anova2.html#type-i-平方和の方程式",
    "title": "多数群の比較：二元配置分散分析",
    "section": "Type I 平方和の方程式",
    "text": "Type I 平方和の方程式\n\n\\begin{split}\n\\overbrace{\\sum_{i=1}^I\\sum_{j=1}^J\\sum_{k=1}^K(x_{ijk} - \\overline{\\overline{x}})^2 }^{\\text{総平方和}\\;(SS_T)} =\n\\overbrace{JK\\sum_{i=1}^I(\\overline{x}_{i}-\\overline{\\overline{x}})^2}^{\\text{水準間平方和}\\;SS_A} +\n\\overbrace{IK\\sum_{j=1}^J(\\overline{x}_{j}-\\overline{\\overline{x}})^2}^{\\text{水準間平方和}\\;SS_B} \\\\ +\n\\underbrace{K\\sum_{i=1}^I\\sum_{j=1}^J(\\overline{x}_{ij} + \\overline{\\overline{x}})^2}_{\\text{相互作用平方和}\\;SS_{AB}} +\n\\underbrace{\\sum_{i=1}^I\\sum_{j=1}^J\\sum_{k=1}^K(x_{ijk} - \\overline{x}_{ij})^2}_{\\text{残渣平方和}\\;SS_R}\n\\end{split}\n\n\\bar{x}_i is the sample mean (標本平均) and \\bar{\\bar{x}} is the global mean (総平均).",
    "crumbs": [
      "線形モデル",
      "多数群の比較：二元配置分散分析"
    ]
  },
  {
    "objectID": "anova2.html#二元配置分散分析",
    "href": "anova2.html#二元配置分散分析",
    "title": "多数群の比較：二元配置分散分析",
    "section": "二元配置分散分析",
    "text": "二元配置分散分析\n分散分析の結果。\n\nfullmodel_treatment = lm(Weight ~ pH + Calluna + pH:Calluna, data = festuca)\nanova(fullmodel_treatment)\n\nAnalysis of Variance Table\n\nResponse: Weight\n           Df  Sum Sq Mean Sq F value     Pr(&gt;F)    \npH          1 19.9800 19.9800 28.1792 0.00007065 ***\nCalluna     1 10.2102 10.2102 14.4001    0.00159 ** \npH:Calluna  1  5.3976  5.3976  7.6126    0.01397 *  \nResiduals  16 11.3446  0.7090                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nモデル係数の結果。\n\nsummary(fullmodel_treatment)\n\n\nCall:\nlm(formula = Weight ~ pH + Calluna + pH:Calluna, data = festuca)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.156 -0.603 -0.138  0.732  1.272 \n\nCoefficients:\n                       Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)              3.3680     0.3766   8.944 0.000000127 ***\npHpH5.5                  3.0380     0.5326   5.705 0.000032562 ***\nCallunaPresent          -0.3900     0.5326  -0.732       0.475    \npHpH5.5:CallunaPresent  -2.0780     0.7531  -2.759       0.014 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.842 on 16 degrees of freedom\nMultiple R-squared:  0.7583,    Adjusted R-squared:  0.713 \nF-statistic: 16.73 on 3 and 16 DF,  p-value: 0.00003447\n\n\n因子ごとの比較は emmeans パッケージの emmeans() 関数でします。 object 引数に、処理するモデルを渡します。 formula には、A因子 と B因子をモデル式として、渡します。\n\nemmip(object = fullmodel_treatment, \n      formula = pH ~ Calluna)\n\n\n\n\n\n\n\n\n図で確認したと、ペアごとの比較をして、t値を求めます。 object 引数に、処理するモデルを渡します。\nB因子の水準内のペアごとの比較をしたい場合は、specs に pairwise ~ A因子 | B因子 を渡します。\n\nemmeans(object = fullmodel_treatment, specs = pairwise ~ pH | Calluna)\n\n$emmeans\nCalluna = Absent:\n pH    emmean    SE df lower.CL upper.CL\n pH3.5   3.37 0.377 16     2.57     4.17\n pH5.5   6.41 0.377 16     5.61     7.20\n\nCalluna = Present:\n pH    emmean    SE df lower.CL upper.CL\n pH3.5   2.98 0.377 16     2.18     3.78\n pH5.5   3.94 0.377 16     3.14     4.74\n\nConfidence level used: 0.95 \n\n$contrasts\nCalluna = Absent:\n contrast      estimate    SE df t.ratio p.value\n pH3.5 - pH5.5    -3.04 0.533 16  -5.705  &lt;.0001\n\nCalluna = Present:\n contrast      estimate    SE df t.ratio p.value\n pH3.5 - pH5.5    -0.96 0.533 16  -1.803  0.0903\n\n\n全ペア毎の比較は、次の通りです。\n\nemmeans(object = fullmodel_treatment, specs = pairwise ~ pH : Calluna, adjust = \"tukey\") |&gt; \n  summary(infer = T)\n\n$emmeans\n pH    Calluna emmean    SE df lower.CL upper.CL t.ratio p.value\n pH3.5 Absent    3.37 0.377 16     2.57     4.17   8.944  &lt;.0001\n pH5.5 Absent    6.41 0.377 16     5.61     7.20  17.011  &lt;.0001\n pH3.5 Present   2.98 0.377 16     2.18     3.78   7.908  &lt;.0001\n pH5.5 Present   3.94 0.377 16     3.14     4.74  10.457  &lt;.0001\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate    SE df lower.CL upper.CL t.ratio\n pH3.5 Absent - pH5.5 Absent      -3.04 0.533 16   -4.562   -1.514  -5.705\n pH3.5 Absent - pH3.5 Present      0.39 0.533 16   -1.134    1.914   0.732\n pH3.5 Absent - pH5.5 Present     -0.57 0.533 16   -2.094    0.954  -1.070\n pH5.5 Absent - pH3.5 Present      3.43 0.533 16    1.904    4.952   6.437\n pH5.5 Absent - pH5.5 Present      2.47 0.533 16    0.944    3.992   4.634\n pH3.5 Present - pH5.5 Present    -0.96 0.533 16   -2.484    0.564  -1.803\n p.value\n  0.0002\n  0.8827\n  0.7118\n  &lt;.0001\n  0.0014\n  0.3080\n\nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 4 estimates \nP value adjustment: tukey method for comparing a family of 4 estimates",
    "crumbs": [
      "線形モデル",
      "多数群の比較：二元配置分散分析"
    ]
  },
  {
    "objectID": "anova2.html#釣り合い型データと直交性について",
    "href": "anova2.html#釣り合い型データと直交性について",
    "title": "多数群の比較：二元配置分散分析",
    "section": "釣り合い型データと直交性について",
    "text": "釣り合い型データと直交性について",
    "crumbs": [
      "線形モデル",
      "多数群の比較：二元配置分散分析"
    ]
  },
  {
    "objectID": "anova2.html#釣り合い型データ-balanced-data",
    "href": "anova2.html#釣り合い型データ-balanced-data",
    "title": "多数群の比較：二元配置分散分析",
    "section": "釣り合い型データ (balanced data)",
    "text": "釣り合い型データ (balanced data)\n各因子水準のデータ数は同じであること",
    "crumbs": [
      "線形モデル",
      "多数群の比較：二元配置分散分析"
    ]
  },
  {
    "objectID": "anova2.html#直行性-orthogonality",
    "href": "anova2.html#直行性-orthogonality",
    "title": "多数群の比較：二元配置分散分析",
    "section": "直行性 (orthogonality)",
    "text": "直行性 (orthogonality)\n説明変数同士の内積 (inner product) はゼロと意味します。 ベクトルとした場合、ベクトル間の角度が 90°であること。 つまり、説明変数がお互いに相関していないこと。\n\n# Example of calculating the inner product of two 3d vectors.\n#| echo: false\n#| eval: false\nk  = c(rnorm(3))\nx1 = c(rnorm(2),0)\nx1 = x1 - as.vector(x1 %*% k) * k / sqrt(sum(k^2))^2\nx2 = pracma::cross(k,x1)\n\n\nggplot() +\n  geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1)) +\n  geom_segment(aes(x = 0, y = 0, xend = -1, yend = 1)) +\n  geom_text(aes(x = 1, y =1, label = \"(1,1)\"), vjust = 0) +\n  geom_text(aes(x = -1, y =1, label = \"(-1,1)\"), vjust = 0) +\n  geom_label(aes(x = 0, y = sqrt(2*(0.5^2)), label = \"90°\")) +\n  geom_curve(aes(x = 0.5, y = 0.5, xend = -0.5, yend = 0.5),\n             arrow = arrow(ends = \"both\", type = \"closed\")) +\n  scale_x_continuous(expand = expansion(add = 0.2)) +\n  scale_y_continuous(expand = expansion(add = 0.2)) +\n  coord_equal()\n\n\n\n\n著効性のあるベクトルのペア\n\n\n\n\n説明変数の係数 x_1 と x_2 の内積がゼロになること。\n\n\\begin{aligned}\nx_1 &= \\begin{bmatrix}\n-1 & 1\n\\end{bmatrix} \\\\\nx_2 &= \\begin{bmatrix}\n1 & 1\n\\end{bmatrix}\n\\end{aligned}\n\n\nx_1^T \\cdot x_2 = \\begin{bmatrix}\n-1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\ 1\n\\end{bmatrix} = (-1 \\times 1) + (1 \\times 1) = 0",
    "crumbs": [
      "線形モデル",
      "多数群の比較：二元配置分散分析"
    ]
  },
  {
    "objectID": "anova2.html#type-iiii-平方和の分散分析",
    "href": "anova2.html#type-iiii-平方和の分散分析",
    "title": "多数群の比較：二元配置分散分析",
    "section": "Type-IIII 平方和の分散分析",
    "text": "Type-IIII 平方和の分散分析\nType-I 以外の平方を使うとき、car パッケージが必要です。\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nさらに、比較は 必ず contr.sum にすること。\n\ncontrasts(dset$pH) = contr.sum      # Required\ncontrasts(dset$Calluna) = contr.sum # Required\n\n\nfullmodel_1 = lm(Weight ~ pH * Calluna, data = dset)\nfullmodel_2 = lm(Weight ~ Calluna * pH, data = dset)\n\n\nAnova(fullmodel_1, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: Weight\n            Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 683.36  1 1758.218 &lt; 2.2e-16 ***\npH           42.64  1  109.711 1.278e-12 ***\nCalluna      17.94  1   46.163 5.336e-08 ***\npH:Calluna    8.19  1   21.081 4.947e-05 ***\nResiduals    14.38 37                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(fullmodel_2, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: Weight\n            Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 683.36  1 1758.218 &lt; 2.2e-16 ***\nCalluna      17.94  1   46.163 5.336e-08 ***\npH           42.64  1  109.711 1.278e-12 ***\nCalluna:pH    8.19  1   21.081 4.947e-05 ***\nResiduals    14.38 37                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nモデルに入れる因子の順序が変わっても、結果は同じです。",
    "crumbs": [
      "線形モデル",
      "多数群の比較：二元配置分散分析"
    ]
  },
  {
    "objectID": "anova2.html#結果の書き方",
    "href": "anova2.html#結果の書き方",
    "title": "多数群の比較：二元配置分散分析",
    "section": "結果の書き方",
    "text": "結果の書き方\n二元配置分散分析を行った結果、pH (F(1,16) = 32.54, P = 0.00003) と pH と Calluna の相互作用 (F(1,16) = 7.61, P = 0.0140) に対しては有意の効果があった。 Calluna (F(1,16) = 0.54, P = 0.4746) に対しては有意な効果がなかった。",
    "crumbs": [
      "線形モデル",
      "多数群の比較：二元配置分散分析"
    ]
  },
  {
    "objectID": "anova-slides.html#アマモの全長の比較",
    "href": "anova-slides.html#アマモの全長の比較",
    "title": "Comparing multiple groups",
    "section": "アマモの全長の比較",
    "text": "アマモの全長の比較\n3つの沿岸域（A, B, and C）のアマモ (Zostera marina) を採取しました。 作業仮説は「アマモの全長は沿岸域によって，長さがことなる」です。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#t検定で比較する",
    "href": "anova-slides.html#t検定で比較する",
    "title": "Comparing multiple groups",
    "section": "t検定で比較する",
    "text": "t検定で比較する\n3 カ所の全長を比較するのであれば，次のような組み合わせを考えられます。\n\nA – B\nA – C\nB – C\n\nt 検定を 3 回実行して，比較してみたら，結果は次の通りです。\n\n\n\n\n\nTest\nt-value\nd.f.\np-value\n\n\n\n\nA-B\n-2.778\n16.373\n0.0132\n\n\nA-C\n-2.333\n15.532\n0.0334\n\n\nB-C\n0.239\n17.838\n0.8141\n\n\n\n\n\n\n\nNHST （帰無仮説の有意性検定）によると，A–B と A–C の帰無仮説は棄却できますが，B–C の帰無仮説の棄却はできません。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#多重仮説検定の問題",
    "href": "anova-slides.html#多重仮説検定の問題",
    "title": "Comparing multiple groups",
    "section": "多重仮説検定の問題",
    "text": "多重仮説検定の問題\n\\(\\alpha\\) は有意水準ですが，第 1 種の誤りを起こす確率でもあります。つまり，\\(\\alpha\\) は帰無仮説が正しくても誤って棄却する確率です。\nアマモの全長の解析には，t 検定を 3 回しました。1 回の検定にかかる第 1 種の誤りは 0.05 でしたが，3 回も検定したので（多重仮説検定），全体の第 1 種の誤りを起こす確率は次の通りです。\n\\[\nα_{all}=1-(1-α)_{\\text{A-B}}(1-α)_{\\text{A-C}}(1-α)_{\\text{B-C}} = 1-0.95^3 = 0.1426\n\\]",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#多重仮説検定したときの-α-過誤",
    "href": "anova-slides.html#多重仮説検定したときの-α-過誤",
    "title": "Comparing multiple groups",
    "section": "多重仮説検定したときの α 過誤",
    "text": "多重仮説検定したときの α 過誤\n\n\n多重仮説検定をするときに，全ての仮説の中から少なくとも 1 つの正しい帰無仮説が誤って棄却されてしまう確率が \\(\\alpha_{\\text{fwer}}\\) です。\n\\[\n\\alpha_{\\text{fwer}} = 1 - (1-\\alpha)^n\n\\]\n全体の \\(\\alpha\\) 過誤をおさえたければ，各実験の \\(\\alpha\\) を抑えれことが必要です。\n\n\n\n\n\n\n\n\n\n\n\n\nfwer: family-wise error rate （ファミリーワイズエラー率）",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#一元配置分散分析",
    "href": "anova-slides.html#一元配置分散分析",
    "title": "Comparing multiple groups",
    "section": "一元配置分散分析",
    "text": "一元配置分散分析\n一元配置分散分析 (One-way ANOVA) は， 1つの要因（因子, factor）A の複数の水準（群, group, level） A1, A2, …, Ap にたいして，各水準の母平均の差を検証するための検定です。\n\n帰無仮説: \\(\\mu_1 = \\mu_2 = \\cdots = \\mu_p\\)\n全水準の平均値は等しい",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#一元配置分散分析-1",
    "href": "anova-slides.html#一元配置分散分析-1",
    "title": "Comparing multiple groups",
    "section": "一元配置分散分析",
    "text": "一元配置分散分析\n解析に使うモデルの次の方程式で定義します。\n\\[\nx_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}\n\\]\n\\(\\mu\\) は総平均値，\\(x_{ij}\\) は水準 \\(i\\) の指数 \\(j\\) の観測値です。\\(\\alpha_i\\) は\\(\\mu\\) に対する水準 \\(i\\) の効果です。\\(\\epsilon_{ij}\\) はお互いに独立に正規分布 \\((N(0, \\sigma^2))\\) に従う誤差項です。ただし，\\(\\sum_{i=1}^n \\alpha_i = 0\\) とします。このとき，帰無仮説は \\(\\alpha_1 = \\alpha_2 = \\cdots = \\alpha_n\\) です。\n\n\nこのときの α は有意水準ではない。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#一元配置分散分析表",
    "href": "anova-slides.html#一元配置分散分析表",
    "title": "Comparing multiple groups",
    "section": "一元配置分散分析表",
    "text": "一元配置分散分析表\n\n\n\n\n\n要因\n\n\n自由度 (df)\n\n\n平方和 (SS)\n\n\n平均平方 (MS)\n\n\nF値\n\n\nP値\n\n\n\n\n\n\nA\n\n\n\\(df_A = I-1\\)\n\n\n\\(SS_A\\)\n\n\n\\(MS_A = SS_A / df_A\\)\n\n\n\\(MS_A / MS_R\\)\n\n\n\\(qf(1-α, df_A, df_R)\\)\n\n\n\n\ne\n\n\n\\(df_R = I(J-1)\\)\n\n\n\\(SS_R\\)\n\n\n\\(MS_R = SS_R / df_R\\)\n\n\n\n\n\n\n\n\n\n\n\\(df_T =IJ-1\\)\n\n\n\\(SS_T\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(A\\) は要因, \\(e\\) は残渣（誤差項）,\\(I\\) は水準の数, \\(J\\) は標本の数です。\\(SS_A\\) は水準間平方和，\\(SS_R\\) は残渣平方和, \\(SS_T\\)は総平方和です。\\(MS_A\\) は水準間平均平方，\\(MS_R\\) は残渣平均平方です。F値は平均平方の比です。水準間の平均値に対して，誤差（ばらつき，残渣）が大きいと，残渣平均平方も大きくなります。F値の分母が分子より大きくなったら，帰無仮説を棄却しにくくなります。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#平方和の方程式",
    "href": "anova-slides.html#平方和の方程式",
    "title": "Comparing multiple groups",
    "section": "平方和の方程式",
    "text": "平方和の方程式\n\\[\n\\underbrace{\\sum_{i=1}^I\\sum_{j=1}^J(x_{ij} - \\overline{\\overline{x}})^2 }_{\\text{総平方和}\\;(SS_T)} =\n\\overbrace{J\\sum_{i=1}^I(\\overline{x}_{i}+\\overline{\\overline{x}})^2}^{\\text{水準間平方和}\\;SS_A} +\n\\underbrace{\\sum_{i=1}^I\\sum_{j=1}^J(x_{ij} - \\overline{x}_i)^2}_{\\text{残渣平方和}\\;SS_R}\n\\]\n\\(\\bar{x}_i\\) は標本平均，\\(\\bar{\\bar{x}}\\) は総平均です。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#平均方和を分解すると",
    "href": "anova-slides.html#平均方和を分解すると",
    "title": "Comparing multiple groups",
    "section": "平均方和を分解すると",
    "text": "平均方和を分解すると\n\n残渣平均平方は Field site (site 変数) ごとの残渣に影響されます。残渣のばらつきが小さいほど，残渣平均平方が小さくなります。 総平均に対する Field site の効果が大きければ，水準間平均平方が大きくなります。\n\nThe simulated data case, where the MS of the site is larger than the MS of the residuals.",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#平均方和-水準ごとのばらつきが小さいとき",
    "href": "anova-slides.html#平均方和-水準ごとのばらつきが小さいとき",
    "title": "Comparing multiple groups",
    "section": "平均方和: 水準ごとのばらつきが小さいとき",
    "text": "平均方和: 水準ごとのばらつきが小さいとき\n\n水準間平均平方 (site: Mean Sq) はかわらないが，残渣平均平方 (Residuals: Mean Sq) は小さくなった。\n\n\n\n  \n\n\n\n\n\nThe same data but the variation within the sites are made smaller.\nThe variation among sites are not changes.\nIn this case, the MS of the residuals decrease.",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#平均方和-水準ごとのばらつきが大きいとき",
    "href": "anova-slides.html#平均方和-水準ごとのばらつきが大きいとき",
    "title": "Comparing multiple groups",
    "section": "平均方和: 水準ごとのばらつきが大きいとき",
    "text": "平均方和: 水準ごとのばらつきが大きいとき\n水準間平均平方 (site: Mean Sq) はかわらないが，残渣平均平方 (Residuals: Mean Sq) は大きくなった。\n\n\n\n  \n\n\n\n\n\n\nThe same data but the variation within the sites are made larger.\nThe variation among sites are not changes.\nIn this case, the MS of the residuals increase, because the noise is larger in the residuals.",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#分散分析の統計量",
    "href": "anova-slides.html#分散分析の統計量",
    "title": "Comparing multiple groups",
    "section": "分散分析の統計量",
    "text": "分散分析の統計量\n分散分析で求める統計量は F値 とよびます。\n\\[\nF = \\left . \\frac{SS_A}{I-1} \\right / \\frac{SS_R}{I(J-1)}  = \\frac{MS_A}{MS_R}\n\\]\nF値は自由度 \\(I-1, I(J-1)\\) のF分布に従います。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#f値の確率密度関数",
    "href": "anova-slides.html#f値の確率密度関数",
    "title": "Comparing multiple groups",
    "section": "F値の確率密度関数",
    "text": "F値の確率密度関数\n\\[\nP(x|d_1, d_2) = \\frac{1}{\\mathrm{B}\\left(\\frac{d_1}{2}, \\frac{d_2}{2}\\right)}\\left(\\frac{d_1}{d_2}\\right)^{\\left(\\frac{d_1}{2}\\right)}x^{\\left(\\frac{d_1}{2}-1\\right)}\\left(1+\\frac{d_1}{d_2}x\\right)^{\\left(-\\frac{d_1+d_2}{2}\\right)}\n\\]\n\\(\\mathrm{B}(d_1, d_2)=\\int_0^1t^{x-1}(1-t)^{y-1}dt\\) はベータ関数，\\(d_1\\) と \\(d_2\\) は正数です。\\(x\\) が確率変数です。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#一元配置分散分析の仮定",
    "href": "anova-slides.html#一元配置分散分析の仮定",
    "title": "Comparing multiple groups",
    "section": "一元配置分散分析の仮定",
    "text": "一元配置分散分析の仮定\n分散分析に次の仮定が定義されています。\n\n母集団の分散は等しい\n残渣は正規分布する\n任意の水準の応答（応答変数，説明したいデータ，y軸の値）はお互いに独立で同一の分布に従う正規確率変数であること",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#アマモの全長の解析",
    "href": "anova-slides.html#アマモの全長の解析",
    "title": "Comparing multiple groups",
    "section": "アマモの全長の解析",
    "text": "アマモの全長の解析\nRでは，いろんな方法で分散分析ができます。\n\naov(data ~ site, data = X) %&gt;% summary()\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nsite         2  161.7   80.84   3.896 0.0326 *\nResiduals   27  560.2   20.75                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nlm(data ~ site, X) %&gt;% summary.aov()\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nsite         2  161.7   80.84   3.896 0.0326 *\nResiduals   27  560.2   20.75                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n実は，oneway.test(), aov(), と lm() 関数で分散分析ができます。 不等分散のとき，oneway.test() はウェルチの分散分析を実行してくれますが，一般的につかわれていません。 さらに，oneway.test() の結果から解析の診断や多重仮説検定を簡単にできません。 aov() は lm() ラッパー (wrapper) なので，aov() を実行したとき，裏で lm() が実行されます。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#α過誤β過誤検出力",
    "href": "anova-slides.html#α過誤β過誤検出力",
    "title": "Comparing multiple groups",
    "section": "α過誤，β過誤，検出力",
    "text": "α過誤，β過誤，検出力\n\n\n\nlibrary(pwr)\nJ = 3 # 群の数\neffect_size = 1 # 効果\nMSR = 1 # 残渣平均平方\npower = 0.80 # 検出力\nalpha = 0.05 # 有意水準・α過誤\nlambda = effect_size / MSR \npwr.anova.test(k = J, \n               f = lambda, \n               power = power) # pwr パッケージの関数\n\n\n\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 4.381443\n              f = 1\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\n\n\n一元配置分散分析の検出力は 0.80，\\(\\alpha\\) は 0.05， 水準の数は 3，標本平均の最小と最大の差（効果）は 1， 残渣平均平方と効果の比を 1 にしたとき，各群の標本数を求めました。 このときの効果は 1 としましたが，lambda (\\(\\lambda\\)) の方程式は次のとおりです。\n\\[\n\\lambda = \\frac{\\sum_{j-1}^Jn\\beta_j}{\\sigma_e^2}\n\\] \\(\\beta_j\\) は \\(j\\) 水準の効果，\\(n\\) は \\(j\\) 群の標本数，\\(\\sigma_e^2\\) は母分散です。 \\(n\\), \\(\\beta_j\\), \\(\\sigma_e^2\\) は未知なので，客観的に選ぶ必要があります。\n必要な標本数は n = \\(4.38 \\rightarrow 5\\) でした。\n\nWhen the null hypothesis is TRUE, then the F-value follows a central F-distribution. When the null hypothesis is FALSE, then the F-value follows a non-central F-distribution. This requires determining the non-centrality parameter (ncp).\n\\[\n\\displaystyle\n\\lambda = \\frac{\\sum_{j-1}^Jn\\beta_j}{\\sigma_e^2}\n\\]\nwhere \\(\\beta\\) is the effect size (difference between the group mean and the grand mean), \\(n\\) is the number of samples in group \\(j\\), and \\(\\sigma_e^2\\) is population variance.\nhttps://stats.idre.ucla.edu/r/dae/one-way-anova-power-analysis/\nhttps://rpubs.com/davidtnly/411707\nhttp://users.stat.umn.edu/~corbett/classes/5303/ANOVA_power_handout.pdf\nCohen’s f2 is just the VAR(A) / VAR(R)",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#分散分析のp値も一様分布する",
    "href": "anova-slides.html#分散分析のp値も一様分布する",
    "title": "Comparing multiple groups",
    "section": "分散分析のP値も一様分布する",
    "text": "分散分析のP値も一様分布する\n帰無仮説が正しいとき，分散分析のP値は一様分布します。 シミュレーションの水準数は 3，各水準の標本数は 10，等分散には従っています。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#不等分散のときα過誤は少々あがるn10",
    "href": "anova-slides.html#不等分散のときα過誤は少々あがるn10",
    "title": "Comparing multiple groups",
    "section": "不等分散のときα過誤は少々あがる(N=10)",
    "text": "不等分散のときα過誤は少々あがる(N=10)\n\n説明図\n\n\n帰無仮説が正しいとき，そして等分散性が守られていないとき，P値は一様分布しません。 それにして，帰無仮説を棄却しやすくなりません。 実施される F 検定は保守的なので，どちらかというと，有意な結果はでません。 つまり，第１種の過誤を起こす確率はあまりあがりません。 20・30・30 や 10・10・30 は水準間の平均値に対する分散の％割合を示しています。 等分散の群は Control です。このときの標本数は10, 水準数は 3 です。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#標本数をあげればα過誤は若干さがるn100",
    "href": "anova-slides.html#標本数をあげればα過誤は若干さがるn100",
    "title": "Comparing multiple groups",
    "section": "標本数をあげればα過誤は若干さがる(N=100)",
    "text": "標本数をあげればα過誤は若干さがる(N=100)\n\n説明図\n\n\n帰無仮説が正しいとき，標本数を100 にしたら第１種の過誤を起こす確率は若干さがります。 20・30・30 や 10・10・30 は水準間の平均値に対する分散の％割合を示しています。 等分散の群は Control です。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#不等分散のときβ過誤はあがるn10",
    "href": "anova-slides.html#不等分散のときβ過誤はあがるn10",
    "title": "Comparing multiple groups",
    "section": "不等分散のときβ過誤はあがる(N=10)",
    "text": "不等分散のときβ過誤はあがる(N=10)\n\n説明図\n\n\n帰無仮説が正しくないとき，さらに 等分散性が守られていないとき，正しく帰無仮説を棄却しにくくなります。 第2種の過誤が起きやすくなります。 20・30・30 や 10・10・30 は水準間の平均値に対する分散の％割合を示しています。 等分散の群は Control です。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#分散分析の手順",
    "href": "anova-slides.html#分散分析の手順",
    "title": "Comparing multiple groups",
    "section": "分散分析の手順",
    "text": "分散分析の手順\n\n作業仮説，帰無仮説，対立仮説をたてる。\n実験をデザインして，実行する。\n集めたデータの箱ヒゲ図を作図する。\n\n不等分散性が気になるなら，等分散性の検定を行なう。\nBartlett’s test バートレット検定 (bartlett.test())\nLevene’s test ルビーン検定 (car パケージに leveneTest() があります。)\nHartley’s test ハートリー検定\nBrownForsythe test\nFisher Ratio\nConover test\n\n分散分析を実施する。\n図を用いて残渣の正規性と等分散性を確認する。\n問題なければ，結果を報告する。問題があったとき，さらにデータ処理を行い，再び分散分析をする。\n\n\nhttps://stats.stackexchange.com/questions/135232/bartletts-test-vs-levenes-test",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#作業仮説帰無仮説対立仮説の設定",
    "href": "anova-slides.html#作業仮説帰無仮説対立仮説の設定",
    "title": "Comparing multiple groups",
    "section": "0. 作業仮説・帰無仮説・対立仮説の設定",
    "text": "0. 作業仮説・帰無仮説・対立仮説の設定\n最初の紹介した海域ごとのアマモの全長を解析します。\n作業仮説： アマモの全長は沿岸域によって，長さがことなる\n帰無仮説： アマモの海域ごとの全長は等しい \\((\\mu_A = \\mu_B = \\mu_C)\\)\n対立仮説： 一つ以上のアマモの海域ごとの全長は異なる \\((\\mu_A \\neq \\mu_B |\\mu_A \\neq \\mu_C|\\mu_B \\neq \\mu_C)\\)",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#実験をデザインして実行する",
    "href": "anova-slides.html#実験をデザインして実行する",
    "title": "Comparing multiple groups",
    "section": "1. 実験をデザインして実行する",
    "text": "1. 実験をデザインして実行する\n2019年3月1日に沿岸A，沿岸B，沿岸Cにおいて，各沿岸域から無作為に 10 個体のアマモを採取して，定規で全長を測定した。\n帰無仮説を検証するために，一元配置分差分析を実施する。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#集めたデータの箱ヒゲ図",
    "href": "anova-slides.html#集めたデータの箱ヒゲ図",
    "title": "Comparing multiple groups",
    "section": "2. 集めたデータの箱ヒゲ図",
    "text": "2. 集めたデータの箱ヒゲ図\nデータ数が少ないのとき，散布図でも問題ないです。\n\n\nggplot(X) + geom_boxplot(aes(x = site, y = data, fill = site)) +\n  geom_point(aes(x = site, y = data), position = position_jitter(0.05)) +\n  scale_x_discrete(\"Field site\") + scale_y_continuous(\"Length (mm)\") +\n  theme(legend.position=c(0,1), legend.justification=c(0,1), legend.title=element_blank())",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#等分散性の検定",
    "href": "anova-slides.html#等分散性の検定",
    "title": "Comparing multiple groups",
    "section": "3. 等分散性の検定",
    "text": "3. 等分散性の検定\n箱ひげ図を確認すると，不等分散性の問題はないように見えないが，検定をかけて確認できます。site ごとの不偏分散は次の通りです。\n\nX %&gt;% \n  group_by(site) %&gt;% \n  summarise(sigma = var(data))\n\n\n  \n\n\n\nSite A の分散は Site B と Site C と比べると約半分ぐらいです。\nバートレット検定 は正規分布に従わないデータに強く影響されますので，誤った結果になる可能性があります。，\n\nbartlett.test(data ~ site, X)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  data by site\nBartlett's K-squared = 1.5442, df = 2, p-value = 0.462\n\n\nルービン検定 は正規分布に従わないデータに対してロバスト (robust, 頑健) です。；\n\ncar::leveneTest(data ~ site, X)\n\n\n  \n\n\n\nどちらも，帰無仮説を棄却する結果を示していないので，水準ごとの分散は等しいと解釈できます。\n\n第 1 種の過誤を起こす性質： Levene &gt; Bartlett\n第 2 種の過誤を起こす性質： Bartlett &gt; Levene\n\n\nLevene’s test is just a t-test or an ANOVA of the absolute values of the deviations of the data from their group means. So, the assumptions that need to be satisfied are the same as a t-test or an ANOVA. However, since it uses the absolute value, the t-test/ANOVA assumptionare are not truely satsified, since the value being tested can never be less than 0.\nBartlett’s test assumes the data come from a normal distribution.\nNote that there is a Brown-Forsythe test, which is a robust Levene’s test that uses the absolute deviation from the median. There are two packages with a function, onewaytests and lawstat. I think the calculation in onewaytests is wrong, because it appears to be using the mean and not the median.",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#分散分析を実施する",
    "href": "anova-slides.html#分散分析を実施する",
    "title": "Comparing multiple groups",
    "section": "4. 分散分析を実施する",
    "text": "4. 分散分析を実施する\n等分散性に問題なかったので，分散分析をします。\n\nm1 = lm(data ~ site, data = X)\nanova(m1)\n\n\n  \n\n\n\n分散分析表には，自由度 (Df)，平方和 (Sum Sq)，平均平方 (Mean Sq)，F値 (F value)，P値 (Pr(&gt;F)) がでます。 P値は有意水準 (α = 0.05) より低いので，帰無仮説 (\\(\\mu_A = \\mu_B = \\mu_C\\)) を棄却します。帰無仮説が正しかったら，このデータはとても珍しいです。\n\n# 手順はいくつかあります。\nsummary.aov(m1)\nlm(data ~ site, data = X) %&gt;% anova()\nlm(data ~ site, data = X) %&gt;% summary.aov()\naov(data ~ site, data = X) %&gt;% summary()",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#図を用いて残渣の正規性と等分散性を確認する",
    "href": "anova-slides.html#図を用いて残渣の正規性と等分散性を確認する",
    "title": "Comparing multiple groups",
    "section": "5. 図を用いて残渣の正規性と等分散性を確認する",
    "text": "5. 図を用いて残渣の正規性と等分散性を確認する\n\nResidual vs. site と Residual vs. Fitted Value の図は残渣の性質を確認するための図です。 この二つの図で確認するのは，残渣のばらつきとばらつきの性質です。ばらつきが説明変数となんかしらの関係があれば，解析に問題があると示します。Normal-QQ Plot は残渣の正規性を確認するための図です。残渣は正規分布に従えば，赤線とかさなります。この結果をみると，正規性に問題はないが，期待値が高いときの残渣のばらつきは期待値の低いときのばらつきより大きいです。\n\nhttps://towardsdatascience.com/a-comprehensive-list-of-handy-r-packages-e85dad294b3d",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#結果を報告する",
    "href": "anova-slides.html#結果を報告する",
    "title": "Comparing multiple groups",
    "section": "6. 結果を報告する",
    "text": "6. 結果を報告する\n結果の報告について，まずは分散分析表の記述が必要です。\n\n\n\n\n\n文中に記述するなら，次のとおりです。\n\n沿岸域によってアマモの全長が異なるかを，一元配置分散分析によって検討したところ，沿岸域 \\((F_{(2, 27)} = 3.90, P = 0.0326)\\) の効果は有意だった。\n\nF値，自由度，P値を記述することがポイントです。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#分散分析の帰無仮説を棄却したとき",
    "href": "anova-slides.html#分散分析の帰無仮説を棄却したとき",
    "title": "Comparing multiple groups",
    "section": "分散分析の帰無仮説を棄却したとき",
    "text": "分散分析の帰無仮説を棄却したとき\n\n分散分析に帰無仮説を棄却したら，\\(\\mu_1 \\ne \\mu_2 \\ne \\cdots \\ne \\mu_p\\) と考えられるが， 水準ごとの違いは明らかではないです。多重比較はペアごとの平均値を比較するときにつかう手法です。2 種類の多重比較があります。\n\na priori comparisons「事前比較」\n\n分散分析をするまえに，事前に設定した多重比較\n\npost hoc comparisons「事後比較」\n\n分散分析をしたあとにする多重比較\n\n\n事前比較のとき，第 1 種の誤りは分散分析の \\(\\alpha\\) と同じです。 事後比較のとき，第 1 種の誤りを新たに求める必要があります。\n\n\nThis site is completely wrong! http://plaza.umin.ac.jp/~beehappy/stat/com-ph.html",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#線型結合",
    "href": "anova-slides.html#線型結合",
    "title": "Comparing multiple groups",
    "section": "線型結合",
    "text": "線型結合\n\n確率変数 \\(x_1, x_2, \\cdots, x_p\\) の線型結合は次のように定義します。\n\\[\nL = c_1 x_1 + c_2 x_2 + \\cdots + c_p x_p\n\\] このとき，\\(c_i\\) はあらかじめ決めたモデル係数です。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#事前比較",
    "href": "anova-slides.html#事前比較",
    "title": "Comparing multiple groups",
    "section": "事前比較",
    "text": "事前比較\n\n事前比較のとき，あらかじめ決めたモデル係数と平均値の積和（線型結合）はつぎのとおりです。\\(\\Lambda\\) は大文字のラムダ (\\(\\lambda\\)) です。\n\\[\n\\Lambda = \\sum_i c_i \\mu_i\n\\] \n水準 \\(i\\) の平均値 \\(\\mu_i\\) の対比は先ほどの式で表せます。 このとき，\\(c_i\\) はモデルの係数です。\n\\[\n\\sum_i c_i = 0\n\\] 事前比較の線型結合は上の条件を満たさなければなりません。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#事前比較の線型結合の例",
    "href": "anova-slides.html#事前比較の線型結合の例",
    "title": "Comparing multiple groups",
    "section": "事前比較の線型結合の例",
    "text": "事前比較の線型結合の例\n一般的な線型結合の例\n\\[\nL = \\left(\\frac{1}{p}\\right)x_1 +\\left(\\frac{1}{p}\\right)x_2 +\\cdots +\\left(\\frac{1}{p}\\right)x_p\n\\] 事前比較の線型結合の例\n\\[\nL = \\left(\\frac{1}{3}\\right)x_1 +\n\\left(\\frac{1}{3}\\right)x_2 +\n\\left(\\frac{1}{3}\\right)x_3 -\n\\left(\\frac{1}{2}\\right)x_4 -\n\\left(\\frac{1}{2}\\right)x_5\n\\] \\(\\rightarrow\\) 事前比較の係数の和は \\(\\frac{1}{3}+\\frac{1}{3}+\\frac{1}{3}-\\frac{1}{2}-\\frac{1}{2}=0\\).",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#事前比較-1",
    "href": "anova-slides.html#事前比較-1",
    "title": "Comparing multiple groups",
    "section": "事前比較",
    "text": "事前比較\n\n直交対比 (orthogonal contrasts) は特別な線型結合です。\n複数 (\\(k\\)) の対比 (\\(\\Lambda_j = \\sum_i c_{ji}\\mu_i\\), \\(\\Lambda_k = \\sum_i c_{ki}\\mu_i\\)) があった場合，\n\\[\n\\sum_i  \\frac{c_{ji}c_{ki}}{n_i}= 0\n\\] さらに上の条件を満たさなければなりません。\n\n\nA contrast is a linear combination of treatment group means (\\(\\mu_i\\)),\n\\[\n\\Lambda = \\sum_i c_i \\mu_i\n\\] where the coefficients \\(c_i\\) are predetermined values that must satisfy the condition \\(\\sum_i c_i = 0\\).\nIf there are \\(k\\) multiple contrasts (e.g., \\(\\Lambda_j = \\sum_i c_{ji}\\mu_i\\), \\(\\Lambda_k = \\sum_i c_{ki}\\mu_i\\)), and if they satisfy the constraint,\n\\[\n\\sum_i  \\frac{c_{ji}c_{ki}}{n_i}= 0\n\\]\nthen these contrasts are called orthogonal contrasts.\nj and k refer to a contrast, and there are there are \\(J-1\\) orthogonal contrasts. Where \\(J\\) is the number of levels.\nOrthogonal contrasts are comparisons that are statistically independent.\nIf we compare setosa with virginica and setosa with versicolor, we have implicitly compared virginica with versicolor.\nChoose the contrasts depending on your hypothesis. So if we want to compare virginica and versicolor with setosa, we must build a contrast matrix to do so.\nA contrast matrix has contrast coefficients that must sum to zero. But in R the default contrasts do not",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#直交対比の例",
    "href": "anova-slides.html#直交対比の例",
    "title": "Comparing multiple groups",
    "section": "直交対比の例",
    "text": "直交対比の例\n\n\\(\\mu_1\\), \\(\\mu_2\\), \\(\\mu_3\\) の平均値と関係するモデル係数の線型結合は次の用な形をとります。\n\\[\n\\Lambda = \\overbrace{c_1 \\mu_1}^{p=1} + \\overbrace{c_2 \\mu_2}^{p=2} + \\overbrace{c_3 \\mu_3}^{p=3}\n\\] 最大 \\(p-1\\) の直交比較は設定できます。このとき，\\(p=3\\) なので，直交比較の数は \\(2\\) です。つまり，ここで設定できる直交比較は \\[\n\\begin{aligned}\nA &= a_1 \\mu_1 + a_2 \\mu_2 + a_3 \\mu_3 \\\\\nB &= b_1 \\mu_1 + b_2 \\mu_2 + b_3 \\mu_3\n\\end{aligned}\n\\]\n問題は，\\(\\sum_i a_i = 0\\)，\\(\\sum_i b_i = 0\\)，\\(\\sum_i \\frac{a_i b_i}{n_i}=0\\) を満たす係数をきめることです。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#直交比較の例",
    "href": "anova-slides.html#直交比較の例",
    "title": "Comparing multiple groups",
    "section": "直交比較の例",
    "text": "直交比較の例\n\n係数の組み合わせは無限にありますが，実際には仮説をたててきめます。このとき，\\(\\mu_1 = \\mu_2\\) と \\(\\frac{1}{2}(\\mu_1 + \\mu_2) = \\mu_3\\) が多重比較の帰無仮説としたら，直交比較は次のとおりです。\n\\[\n\\begin{aligned}\nA &= 1 \\mu_1 + (-1) \\mu_2 +    0 \\mu_3 \\\\\nB &= 1 \\mu_1 +   1  \\mu_2 + (-2) \\mu_3\n\\end{aligned}\n\\]\n\n\\(\\sum_i a_i \\rightarrow 1 + (-1) = 0\\)\n\\(\\sum_i b_i \\rightarrow 1 + 1 + (-2) = 0\\),\n\\(\\sum_i \\frac{a_i b_i} {n_i} \\rightarrow 1\\times 1 + (-1)\\times 1 + 0\\times(-2) = 0\\)",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#多重比較",
    "href": "anova-slides.html#多重比較",
    "title": "Comparing multiple groups",
    "section": "多重比較",
    "text": "多重比較\n\n\n多重比較（線型比較, linear contrasts）は \\(p\\) 平均値の比較をします。\n\n直交比較を満たさない多重比較の比較は独立ではない。\n\n直交比較は (orthogonal contrasts) も \\(p\\) 平均値の比較をしていますが，独立した比較をしています。\n\n独立した比較は \\(p-1\\) あります。\n\nFamily wise error rate (FWER: ファミリーワイズエラー率) を 0.05 におさえるのであれば，各比較の test wise error rate (TWER: テストワイズエラー率) を下げる必要があります。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#fwer",
    "href": "anova-slides.html#fwer",
    "title": "Comparing multiple groups",
    "section": "FWER",
    "text": "FWER\n\nファミリーワイズエラー率は多重仮説検定をするときに，全ての仮説のなで，少なくとも１つの第１種の過誤を起こす確率です。FWER の決め方に，数種類の手法があります。\n\nBonferroni’s correction\n\nボンフェローニ補正\n\nSidak’s procedure\n\nシダックの手法\n\nHolmes-Bonferroni’s procedure\n\nホルム・ボンフェローニ手法\n\nDunnet’s correction\n\nダネット補正",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#ボンフェローニ補正",
    "href": "anova-slides.html#ボンフェローニ補正",
    "title": "Comparing multiple groups",
    "section": "ボンフェローニ補正",
    "text": "ボンフェローニ補正\n\nBonferroni’s correction\n\n帰無仮説 \\(H_i\\) を検定するための P値を \\(p_i\\) とする。\n\\(p_i \\leq \\alpha / m\\) であれば，\\(H_i\\) を棄却する。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#シダックの手法",
    "href": "anova-slides.html#シダックの手法",
    "title": "Comparing multiple groups",
    "section": "シダックの手法",
    "text": "シダックの手法\n\nSidak’s procedure\n\nTWER は \\(\\alpha_{sidak} = 1 - (1-\\alpha)^\\frac{1}{m}\\)\nボンフェローニ補正より検出力が高い。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#ホルムボンフェローニ手法",
    "href": "anova-slides.html#ホルムボンフェローニ手法",
    "title": "Comparing multiple groups",
    "section": "ホルム・ボンフェローニ手法",
    "text": "ホルム・ボンフェローニ手法\n\nHolmes-Bonferroni’s procedure\n\nP値を最小から最大までにならべ，\\(P_1, P_2, \\cdots, P_n\\)に対して帰無仮説 \\(H_1, H_2, \\cdots, H_n\\)とする。\n\\(P_k \\leq \\frac{\\alpha}{m+1 - k}\\) が最大になったときの \\(k\\) は \\(R\\) とする。\n\\(H_1, \\cdots, H_k\\) までの帰無仮説を棄却する。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#ダネット補正",
    "href": "anova-slides.html#ダネット補正",
    "title": "Comparing multiple groups",
    "section": "ダネット補正",
    "text": "ダネット補正\n\nDunnet’s correction\n\n多数の水準を単一の対象水準と比較したいときにつきます。\nダネット補正のとき，\\(j-1\\)回 の比較しかありません。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#花びらの長さの事前比較直交ではない比較",
    "href": "anova-slides.html#花びらの長さの事前比較直交ではない比較",
    "title": "Comparing multiple groups",
    "section": "花びらの長さの事前比較（直交ではない比較）",
    "text": "花びらの長さの事前比較（直交ではない比較）\n\\(\\mu_\\text{setosa} = \\mu_\\text{versicolor} = \\mu_\\text{virginica}\\) の帰無仮説でなくて，\\(\\mu_\\text{setosa} = \\mu_\\text{versicolor}\\) と \\(\\mu_\\text{setosa} = \\mu_\\text{virginica}\\) が帰無仮説です。\n線型結合は次のとおりです。 \\[\n\\begin{aligned}\nA &= 1 \\mu_\\text{setosa} - 1 \\mu_\\text{versicolor} +    0 \\mu_\\text{virginica} \\\\\nB &= 1 \\mu_\\text{setosa} + 0 \\mu_\\text{versicolor} -    1 \\mu_\\text{virginica} \\\\\n\\end{aligned}\n\\]\n\n\\(\\sum_i a_i = 0\\) と \\(\\sum_i b_i = 0\\) を満たしています。\n\\(\\sum_i \\frac{a_i b_i} {n_i} \\rightarrow (1 \\times 1) + (-1\\times 0) + (0\\times-1) = 1\\) ので，直交比較の条件を満たしていません。\n\nこのときの 2 回比較するので（帰無仮説は２つある），(\\(\\alpha_{fwer} = 0.05\\)) をするなら，ボンフェローニ補正で補正した\\(\\alpha_{twer} = \\alpha / 2 = 0.025\\) です。直交比較ではないので，帰無仮説は独立していないので，棄却した場合慎重に解釈したほうがいい。\n\nhttps://stats.stackexchange.com/questions/9751/do-we-need-a-global-test-before-post-hoc-tests/9753\nJust because the ANOVA can’t reject the global null (omnibus test) doesn’t mean that you cant run a multiple comparisons test. However, an ANOVA still needs to be calculated because the mean-square-errors from the ANOVA are used in the multiple comparisons test.",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#花びらの長さの事前比較直交比較",
    "href": "anova-slides.html#花びらの長さの事前比較直交比較",
    "title": "Comparing multiple groups",
    "section": "花びらの長さの事前比較（直交比較）",
    "text": "花びらの長さの事前比較（直交比較）\n\\(\\mu_\\text{setosa} = \\mu_\\text{versicolor} = \\mu_\\text{virginica}\\) の帰無仮説でなくて，\\(\\mu_\\text{setosa} = \\mu_\\text{versicolor}\\) と \\(\\frac{1}{2}(\\mu_\\text{setosa} +\\mu_\\text{versicolor})= \\mu_\\text{virginica}\\) が帰無仮説です。\n線型結合は次のとおりです。 \\[\n\\begin{aligned}\nA &= 1 \\mu_\\text{setosa} - 1 \\mu_\\text{versicolor} +    0 \\mu_\\text{virginica} \\\\\nB &= 1 \\mu_\\text{setosa} + 1 \\mu_\\text{versicolor} -    2 \\mu_\\text{virginica} \\\\\n\\end{aligned}\n\\]\n\n\\(\\sum_i a_i = 0\\) と \\(\\sum_i b_i = 0\\) を満たしています。\n\\(\\sum_i \\frac{a_i b_i} {n_i} \\rightarrow (1 \\times 1) + (-1\\times 1) + (0\\times-2) = 0\\) ので，直交比較の条件を満たしています。\n\nこのときも 2 回比較しているので，各帰無仮説のαは ボンフェローニ補正で補正したものを使用します (\\(\\alpha_{twer} = \\alpha / 2 = 0.025\\))。直交比較なので，帰無仮説は独立しています。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#r-における解析",
    "href": "anova-slides.html#r-における解析",
    "title": "Comparing multiple groups",
    "section": "R における解析",
    "text": "R における解析\nアヤメのデータをつかって事前比較を初回します。まず，データの準備です。\n\niris_new = iris %&gt;% as_tibble()\niris_new # 内容の確認\n\n\n  \n\n\n\n\nIn R contrasts() does not return the coefficient for the first level, because it is implicitly 1. But when assigning a contrast matrix, the matrix can be of size JxJ and the first level can be set.",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#デフォルト比較は-treatment-contrasts",
    "href": "anova-slides.html#デフォルト比較は-treatment-contrasts",
    "title": "Comparing multiple groups",
    "section": "デフォルト比較は treatment contrasts",
    "text": "デフォルト比較は treatment contrasts\nRでは，すでに設定されている比較があります。数種類の対比行列は関数で設定できます。\n\ntreatment contrasts 処理対比 (contr.treatment())\nHelmert contrasts ヘルマーと対比・直交対比 (contr.helmert())\nsum to zero contrasts 零和対比 (contr.sum())\npolynomial contrasts 多項式対比 (contr.poly())\nSAS contrasts SAS式対比 (contr.SAS())\n\n実際には，これらの関数はコーディング行列を返します。 Rでは，contr.treatment() がデフォルトの比較です。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#処理対比",
    "href": "anova-slides.html#処理対比",
    "title": "Comparing multiple groups",
    "section": "処理対比",
    "text": "処理対比\nRのデフォルト事前比較は処理対比です。線型結合の式は次の通りです。\n\\[\n\\begin{aligned}\n\\Lambda_1 &= 1 \\times \\text{setosa} - 1 \\times \\text{versicolor} + 0 \\times\\text{virginica} \\\\\n\\Lambda_2 &= 1 \\times \\text{setosa} + 0 \\times \\text{versicolor} - 1 \\times\\text{virginica}\n\\end{aligned}\n\\]\nこのときの帰無仮説は\n\n\\(\\mu_\\text{setosa} = \\mu_\\text{veriscolor}\\)\n\\(\\mu_\\text{setosa} = \\mu_\\text{virginica}\\)",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#r-処理対比",
    "href": "anova-slides.html#r-処理対比",
    "title": "Comparing multiple groups",
    "section": "R 処理対比",
    "text": "R 処理対比\ncontrasts() 関数をつかって，アヤメのコーディング行列を確認します。\n\ncontrasts(iris_new$Species)\n\n           versicolor virginica\nsetosa              0         0\nversicolor          1         0\nvirginica           0         1\n\n\n対比行列に変換するには，列に 1 を代入した列行列を先に足して，その行列の逆行列をもとめる必要があります。\nコーディング行列から対比行列への変換\n\n\n   1 2 3\n   1 0 0\n2 -1 1 0\n3 -1 0 1\n\n\n\n1 行目は setosa の平均値\n2 行目は setosa と versicolor の平均値の差\n3 行目は setosa と virginica の平均値の差",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#処理比較の結果",
    "href": "anova-slides.html#処理比較の結果",
    "title": "Comparing multiple groups",
    "section": "処理比較の結果",
    "text": "処理比較の結果\n\nm1 = lm(Petal.Length ~ Species, iris_new)\nm1 %&gt;% \n  summary.aov(split = list(Species =list(\"setosa:versicolor\" = 1,\n                                         \"setosa:virginica\" = 2)))\n\n                              Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies                        2  437.1   218.6  1180.2 &lt;2e-16 ***\n  Species: setosa:versicolor   1   18.9    18.9   102.1 &lt;2e-16 ***\n  Species: setosa:virginica    1  418.2   418.2  2258.3 &lt;2e-16 ***\nResiduals                    147   27.2     0.2                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n事前比較を指定したときに分散分析表です。setosa 対 versicolor の対比の有意性はとてもたかいです。\\(P \\leq 2\\times 10^{-16}\\) でした。 setosa 対 virginica の結果も同じです。平均値の差は次のように計算します。 このとき，(Interecept) は setosa の平均値です。\n\nsummary(m1)$coefficients\n\n                  Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept)          1.462 0.06085848 24.02294 9.303052e-53\nSpeciesversicolor    2.798 0.08606689 32.50960 5.254587e-69\nSpeciesvirginica     4.090 0.08606689 47.52118 4.106139e-91",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#ペア毎の比較",
    "href": "anova-slides.html#ペア毎の比較",
    "title": "Comparing multiple groups",
    "section": "ペア毎の比較",
    "text": "ペア毎の比較\n\\[\n\\begin{aligned}\n\\Lambda_1 &= 1 \\times \\text{setosa} - 1 \\times \\text{versicolor} + 0 \\times\\text{virginica} \\\\\n\\Lambda_2 &= 0 \\times \\text{setosa} + 1 \\times \\text{versicolor} - 1 \\times\\text{virginica}\n\\end{aligned}\n\\]\n次の帰無仮説は下記のとおりです。\n\n\\(\\mu_\\text{setosa} = \\mu_\\text{versicolor}\\)\n\\(\\mu_\\text{versicolor} = \\mu_\\text{virginica}\\)\n\n対比行列はつぎのとおりです。\n\nX = \n  rbind(c( 1,  0, 0), # 切片\n        c(-1,  1, 0), # 帰無仮説１\n        c( 0, -1, 1)) # 帰無仮説２\n\nX\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]   -1    1    0\n[3,]    0   -1    1",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#ペア毎の比較の結果",
    "href": "anova-slides.html#ペア毎の比較の結果",
    "title": "Comparing multiple groups",
    "section": "ペア毎の比較の結果",
    "text": "ペア毎の比較の結果\n\nZ = X %&gt;% solve()\nm2 = lm(Petal.Length ~ Species, iris_new, \n        contrasts=list(Species = Z[, -1]))\nm2 %&gt;% summary.aov(split = list(Species = list(\"setosa - versicolor\" = 1,\n                                                \"versicolor - virginica\" = 2)))\n\n                                   Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies                             2  437.1   218.6  1180.2 &lt;2e-16 ***\n  Species: setosa - versicolor      1  395.4   395.4  2135.0 &lt;2e-16 ***\n  Species: versicolor - virginica   1   41.7    41.7   225.3 &lt;2e-16 ***\nResiduals                         147   27.2     0.2                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept)    1.462 0.06085848 24.02294 9.303052e-53\nSpecies1       2.798 0.08606689 32.50960 5.254587e-69\nSpecies2       1.292 0.08606689 15.01158 1.810597e-31\n\n\n\nWe can also get the F-values with this code.\naov(m1) %&gt;% \n  summary(split = list(Species = list(versicolor = 1, virginica = 2)))",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#直交対比",
    "href": "anova-slides.html#直交対比",
    "title": "Comparing multiple groups",
    "section": "直交対比",
    "text": "直交対比\n\\[\n\\begin{aligned}\n\\Lambda_1 &= 1 \\times \\text{setosa} - 1 \\times \\text{versicolor} + 0 \\times\\text{virginica} \\\\\n\\Lambda_2 &= 1 \\times \\text{setosa} + 1 \\times \\text{versicolor} - 2 \\times\\text{virginica}\n\\end{aligned}\n\\]\n次の帰無仮説は下記のとおりです。\n\n\\(\\mu_\\text{setosa} = \\mu_\\text{versicolor}\\)\n\\(\\frac{1}{2}(\\mu_\\text{setosa} + \\mu_\\text{versicolor}) = \\mu_\\text{virginica}\\)\n\n対比行列はつぎのとおりです。\n\nX = \n  rbind(c( 1,  0,  0), # 切片\n        c(-1,  1,  0), # 帰無仮説１\n        c( 1,  1, -2)) # 帰無仮説２\n\nX\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]   -1    1    0\n[3,]    1    1   -2",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#直交対比の結果",
    "href": "anova-slides.html#直交対比の結果",
    "title": "Comparing multiple groups",
    "section": "直交対比の結果",
    "text": "直交対比の結果\n\nZ = X %&gt;% solve()\nm3 = lm(Petal.Length ~ Species, iris_new, \n        contrasts=list(Species = Z[, -1]))\nm3 %&gt;% summary.aov(split = list(Species = list(\"setosa - versicolor\" = 1,\n                                                \"mean - virginica\" = 2)))\n\n                                Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies                          2  437.1  218.55    1180 &lt;2e-16 ***\n  Species: setosa - versicolor   1  195.7  195.72    1057 &lt;2e-16 ***\n  Species: mean - virginica      1  241.4  241.38    1303 &lt;2e-16 ***\nResiduals                      147   27.2    0.19                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(m3)$coefficients\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)    1.462 0.06085848  24.02294 9.303052e-53\nSpecies1       2.798 0.08606689  32.50960 5.254587e-69\nSpecies2      -5.382 0.14907223 -36.10330 5.858996e-75\n\n\n\nThere is only one way to partition the sum-of-squares, so all of the sum-of-squares, regardless of the comparison are the same!\n\n\nlm(Petal.Length ~ Species, iris_new, \n        contrasts=list(Species = contr.helmert(3))) %&gt;% \n  summary.aov(split = list(Species = list(a = 1, b = 2)))\n\n\nlm(Petal.Length ~ Species, iris_new, \n        contrasts=list(Species = contr.diff(3))) %&gt;% \n  summary.aov(split = list(Species = list(a = 1, b = 2)))\n\n\nlm(Petal.Length ~ Species, iris_new, \n        contrasts=list(Species = contr.treatment(3))) %&gt;% \n  summary.aov(split = list(Species = list(a = 1, b = 2)))\n\n\nlm(Petal.Length ~ Species, iris_new, \n        contrasts=list(Species = contr.sum(3))) %&gt;% \n  summary.aov(split = list(Species = list(a = 1, b = 2)))",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#事後比較",
    "href": "anova-slides.html#事後比較",
    "title": "Comparing multiple groups",
    "section": "事後比較",
    "text": "事後比較\n事前に比較を検討しなかった場合，事後の比較も可能です。事後比較も数種類あります。\n\nBonferroni Procedure ボンフェロニ法\nHolm-Bonferroni Method ホルム=ボンフェロニ法\nTukey’s Honest Significant Difference Test テューキーのHSD検定\n\nTukey-Kramer method, Tukey’s test\n\nScheffe’s Method シェッフェの方法\nDunnett’s Test ダネットの検定\nFisher’s Least Significant Difference フィッシャーの最小有意差法\nDuncan’s new multiple range test ダンカンの新多重範囲検定\n\n他にもありますが，ダンカンとフィッシャーの検定は第 1 種の過誤を起こしやすいです。 最初の4つは，ペアごとの比較をしますが，ダネットの検定は基準水準との比較だけします。\n\nhttp://rtutorialseries.blogspot.com/2011/03/r-tutorial-series-anova-pairwise.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+RTutorialSeries+%28R+Tutorial+Series%29",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#ボンフェロニ法",
    "href": "anova-slides.html#ボンフェロニ法",
    "title": "Comparing multiple groups",
    "section": "ボンフェロニ法",
    "text": "ボンフェロニ法\n\npairwise.t.test(iris_new$Petal.Length, iris_new$Species, p.adjust.method = \"bonferroni\" )\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  iris_new$Petal.Length and iris_new$Species \n\n           setosa versicolor\nversicolor &lt;2e-16 -         \nvirginica  &lt;2e-16 &lt;2e-16    \n\nP value adjustment method: bonferroni \n\n\nボンフェロニ法でファミリワイズエラー率を調整するとき，\\(\\alpha_{\\text{fwer}} = \\alpha / m\\) です。 ここで表示されている P 値は t 検定で求めたP値と \\(m\\) の積です。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#ホルムボンフェロニ法",
    "href": "anova-slides.html#ホルムボンフェロニ法",
    "title": "Comparing multiple groups",
    "section": "ホルム=ボンフェロニ法",
    "text": "ホルム=ボンフェロニ法\n\npairwise.t.test(iris_new$Petal.Length, iris_new$Species, p.adjust.method = \"holm\" )\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  iris_new$Petal.Length and iris_new$Species \n\n           setosa versicolor\nversicolor &lt;2e-16 -         \nvirginica  &lt;2e-16 &lt;2e-16    \n\nP value adjustment method: holm \n\n\nボンフェロニ法ににていますが，有意水準を対比ごとに変えます。 まず，P値の低い値から高い値に並べます。 最初のP値の有意水準は \\(\\alpha / m\\) です。次のP値の有意水準は \\(\\alpha / (m-1)\\) です。 このように，有意水準を徐々に \\(\\alpha\\) に戻します。 ここで表示されているP値も t 検定で求めたP値と \\(m\\) の積です。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#テューキーのhsd法",
    "href": "anova-slides.html#テューキーのhsd法",
    "title": "Comparing multiple groups",
    "section": "テューキーのHSD法",
    "text": "テューキーのHSD法\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = .)\n\n$Species\n                      diff     lwr     upr p adj\nversicolor-setosa    2.798 2.59422 3.00178     0\nvirginica-setosa     4.090 3.88622 4.29378     0\nvirginica-versicolor 1.292 1.08822 1.49578     0\n\n\nテューキーのHSD法は各2水準間の平均値の差の検定を行っています。 t 検定とちがって，求めた統計量はステューデント化範囲の分布にしたます。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#シェッフェの方法",
    "href": "anova-slides.html#シェッフェの方法",
    "title": "Comparing multiple groups",
    "section": "シェッフェの方法",
    "text": "シェッフェの方法\n\nmout = lm(Petal.Length ~ Species, iris_new)\ndferror = df.residual(mout); mserror = deviance(mout)/dferror; fc = summary(mout)$fstatistic\nagricolae::scheffe.test(aov(mout), trt = \"Species\", # 因子\n                        DFerror = dferror, MSerror=mserror, Fc = fc, group = T, console = TRUE)\n\n\nStudy: aov(mout) ~ \"Species\"\n\nScheffe Test for Petal.Length \n\nMean Square Error  : 0.1851878 \n\nSpecies,  means\n\n           Petal.Length       std  r Min Max\nsetosa            1.462 0.1736640 50 1.0 1.9\nversicolor        4.260 0.4699110 50 3.0 5.1\nvirginica         5.552 0.5518947 50 4.5 6.9\n\nAlpha: 0.05 ; DF Error: 147 \nCritical Value of F: 3.057621 \n\nMinimum Significant Difference: 0.2128349 \n\nMeans with the same letter are not significantly different.\n\n           Petal.Length groups\nvirginica         5.552      a\nversicolor        4.260      b\nsetosa            1.462      c\n\n\nこの手法は分散分析のF値と残渣平均平方を用いて，ペア毎の比較を行います。 テューキーのHSD法の方が検出力高いので，一般的に使われていません。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#ダネットの検定",
    "href": "anova-slides.html#ダネットの検定",
    "title": "Comparing multiple groups",
    "section": "ダネットの検定",
    "text": "ダネットの検定\n\nDescTools::DunnettTest(Petal.Length ~ Species, data  = iris_new,\n                      control = \"versicolor\")\n\n\n  Dunnett's test for comparing several treatments with a control :  \n    95% family-wise confidence level\n\n$versicolor\n                       diff   lwr.ci   upr.ci    pval    \nsetosa-versicolor    -2.798 -2.99024 -2.60576 2.2e-16 ***\nvirginica-versicolor  1.292  1.09976  1.48424 &lt; 2e-16 ***\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n因子の水準にコントロールがあるとき，他の水準をコントロールと比較することがあります。 このとき，ダネットの検定を使います。対比を制限したテューキーのHSD法もつかえます。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#分散分析をしたあとの手順",
    "href": "anova-slides.html#分散分析をしたあとの手順",
    "title": "Comparing multiple groups",
    "section": "分散分析をしたあとの手順",
    "text": "分散分析をしたあとの手順\n\n実は，最初から多重比較をするつもりであれば，分散分析をする必要はないです。 分散分析をすることにより，\\(\\alpha_{\\text{fwer}}\\) をあえて上げています。ところが， 有意性のある分散分析を求めてから，多重仮説検定を行なうことが一般です。\nANOVA procedure で有意な結果がでて，さらに残渣・正規性に問題がなかったので，多重仮説検定をおこないます。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#多重比較検定誤った検定",
    "href": "anova-slides.html#多重比較検定誤った検定",
    "title": "Comparing multiple groups",
    "section": "多重比較検定：誤った検定",
    "text": "多重比較検定：誤った検定\n分散分析の結果は：\\(F_{(2, 27)} = 3.90, P = 0.0326\\) でした。\\(\\mu_A=\\mu_B=\\mu_C\\) を棄却しました。\n\npairwise.t.test(X$data, X$site, p.adjust.method = \"none\" )\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  X$data and X$site \n\n  A     B    \nB 0.017 -    \nC 0.031 0.795\n\nP value adjustment method: none \n\n\nファミリワイズエラー率を調整していないときのペアごとの t 検定によって， \\(\\mu_A =\\mu_B\\) と \\(\\mu_A =\\mu_C\\) を棄却できました。ところが，\\(\\alpha_\\text{fwer}\\) を調整していないので，第 1 種の過誤を起こす確率は \\(1 - (1-\\alpha)^m = 1 - (1-0.05)^3 = 0.1426\\) ですので，誤って帰無仮説を棄却している可能性は十分あります。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#多重比較検定ボンフェロニ法",
    "href": "anova-slides.html#多重比較検定ボンフェロニ法",
    "title": "Comparing multiple groups",
    "section": "多重比較検定：ボンフェロニ法",
    "text": "多重比較検定：ボンフェロニ法\n分散分析の結果は：\\(F_{(2, 27)} = 3.90, P = 0.0326\\) でした。\\(\\mu_A=\\mu_B=\\mu_C\\) を棄却しました。\n\npairwise.t.test(X$data, X$site, p.adjust.method = \"bonferroni\" )\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  X$data and X$site \n\n  A     B    \nB 0.052 -    \nC 0.093 1.000\n\nP value adjustment method: bonferroni \n\n\nファミリワイズエラー率をボンフェロニ法で調整しましたが，ペアごと平均値の差に有意性のある結果は得られなかったが，A–B のP値は 0.052 でした。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#多重比較検定ボンフェロニ法-1",
    "href": "anova-slides.html#多重比較検定ボンフェロニ法-1",
    "title": "Comparing multiple groups",
    "section": "多重比較検定：ボンフェロニ法",
    "text": "多重比較検定：ボンフェロニ法\n分散分析の結果は：\\(F_{(2, 27)} = 3.90, P = 0.0326\\) でした。\\(\\mu_A=\\mu_B=\\mu_C\\) を棄却しました。\n\npairwise.t.test(X$data, X$site, p.adjust.method = \"holm\" )\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  X$data and X$site \n\n  A     B    \nB 0.052 -    \nC 0.062 0.795\n\nP value adjustment method: holm \n\n\nホルム=ボンフェロニ法はボンフェロニ法と同様な結果でした。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "anova-slides.html#多重比較検定テューキーのhsd法",
    "href": "anova-slides.html#多重比較検定テューキーのhsd法",
    "title": "Comparing multiple groups",
    "section": "多重比較検定：テューキーのHSD法",
    "text": "多重比較検定：テューキーのHSD法\n分散分析の結果は：\\(F_{(2, 27)} = 3.90, P = 0.0326\\) でした。\\(\\mu_A=\\mu_B=\\mu_C\\) を棄却しました。\n\nmout = lm(data ~ site, X) %&gt;% aov()\nTukeyHSD(mout)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = .)\n\n$site\n          diff        lwr       upr     p adj\nB-A  5.1704004  0.1198744 10.220927 0.0440187\nC-A  4.6347851 -0.4157410  9.685311 0.0767033\nC-B -0.5356154 -5.5861414  4.514911 0.9626557\n\n\nテューキーのHSD法のとき，\\(\\mu_A = \\mu_B\\) を棄却できましたが，その他の ペアの帰無仮説を棄却できなかった。ところが，この解析手順に問題があります。このように，何度も多重仮説検定法を用いて，希望している結果がでるまでおこなうことは誤りです。この手法は P-fishing とよび，帰無仮説の有意性検定の概念に違反しています。 最初からテューキーのHSD法を使うべきでした。",
    "crumbs": [
      "基礎統計学用",
      "3群以上の分析（分散分析）"
    ]
  },
  {
    "objectID": "part04.html",
    "href": "part04.html",
    "title": "データの操作",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#理想的なデータ構造",
    "href": "part04.html#理想的なデータ構造",
    "title": "データの操作",
    "section": "理想的なデータ構造",
    "text": "理想的なデータ構造\n\n1 行 = 1 観測値\n1 列 = 1 変数",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#データ加工操作用関数",
    "href": "part04.html#データ加工操作用関数",
    "title": "データの操作",
    "section": "データ加工・操作用関数",
    "text": "データ加工・操作用関数\nデータの結合 (mutating join)\nx, y, by は関数の引数です。by で指定したキー（変数名）が一致するように行を合わせることができる。\n\nfull_join(x, y, by)：全ての x と y 行と列を結合する。\ninner_join(x, y, by)：x と y で共通する行と列を結合する。\nleft_join(x, y, by)：左側（）第 1 引数のtibble に y の変数を追加する。\nright_join(x, y, by)：右側（）第 2 引数のtibble に x の変数を追加する。\n\nデータの結合 (join)\n\nbind_cols()：渡したtibbleを横に結合する（行数が異なったらエラーが発生する）。\nbind_rows()：渡した tibble を立てに結合する（一致する変数名を合わせてくれます）。",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#mutating-join-のつかいかた",
    "href": "part04.html#mutating-join-のつかいかた",
    "title": "データの操作",
    "section": "mutating join のつかいかた",
    "text": "mutating join のつかいかた\n\nX = tibble(x = c(\"A\", \"B\", \"C\", \"G\"), y = c(NA, rnorm(3, mean = 5)))\nY = tibble(x = c(\"A\", \"C\", \"D\", \"E\"), z = c(rpois(3, lambda = 5), NA))\n\n\nX\n\n# A tibble: 4 × 2\n  x         y\n  &lt;chr&gt; &lt;dbl&gt;\n1 A     NA   \n2 B      4.28\n3 C      3.72\n4 G      3.37\n\n\n\nY\n\n# A tibble: 4 × 2\n  x         z\n  &lt;chr&gt; &lt;int&gt;\n1 A         4\n2 C         2\n3 D         5\n4 E        NA\n\n\n\nfull_join(X,Y, by = \"x\")\n\n# A tibble: 6 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n1 A     NA        4\n2 B      4.28    NA\n3 C      3.72     2\n4 G      3.37    NA\n5 D     NA        5\n6 E     NA       NA\n\n\n\ninner_join(X, Y, by = \"x\")\n\n# A tibble: 2 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n1 A     NA        4\n2 C      3.72     2\n\n\n\nleft_join(X, Y, by = \"x\")\n\n# A tibble: 4 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n1 A     NA        4\n2 B      4.28    NA\n3 C      3.72     2\n4 G      3.37    NA\n\n\n\nright_join(X, Y, by = \"x\")\n\n# A tibble: 4 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n1 A     NA        4\n2 C      3.72     2\n3 D     NA        5\n4 E     NA       NA\n\n\n\nbind_rows(X, Y)\n\n# A tibble: 8 × 3\n  x         y     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n1 A     NA       NA\n2 B      4.28    NA\n3 C      3.72    NA\n4 G      3.37    NA\n5 A     NA        4\n6 C     NA        2\n7 D     NA        5\n8 E     NA       NA\n\n\n\nbind_rows(\"X\" = X, \"Y\" = Y, .id = \"origin\")\n\n# A tibble: 8 × 4\n  origin x         y     z\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n1 X      A     NA       NA\n2 X      B      4.28    NA\n3 X      C      3.72    NA\n4 X      G      3.37    NA\n5 Y      A     NA        4\n6 Y      C     NA        2\n7 Y      D     NA        5\n8 Y      E     NA       NA\n\n\n\nbind_cols(X, Y)\n\nNew names:\n• `x` -&gt; `x...1`\n• `x` -&gt; `x...3`\n\n\n# A tibble: 4 × 4\n  x...1     y x...3     z\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;\n1 A     NA    A         4\n2 B      4.28 C         2\n3 C      3.72 D         5\n4 G      3.37 E        NA",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#行と列の加工操作用関数",
    "href": "part04.html#行と列の加工操作用関数",
    "title": "データの操作",
    "section": "行と列の加工・操作用関数",
    "text": "行と列の加工・操作用関数\n列における操作\n\nmutate()：既存の変数の書き換えや変数の追加する\nselect()：既存の変数を選らぶ\nrename()：既存の変数の名前を変える\npull()：既存の変数をリストとして抽出する\n`relocate()``：指定した列の位置を変える\n\n行における操作\n\nfilter()：条件を満たした行を返す\ndistinct()：指定した変数から重複している行を外す\nslice()：指定した行インデックスを返す\narrange()：指定した列の昇順で行を並べ替える",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#列の加工",
    "href": "part04.html#列の加工",
    "title": "データの操作",
    "section": "列の加工",
    "text": "列の加工\n\niris |&gt; as_tibble() |&gt; mutate(P2 = Petal.Length^2)\n\n# A tibble: 150 × 6\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species    P2\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;\n 1          5.1         3.5          1.4         0.2 setosa   1.96\n 2          4.9         3            1.4         0.2 setosa   1.96\n 3          4.7         3.2          1.3         0.2 setosa   1.69\n 4          4.6         3.1          1.5         0.2 setosa   2.25\n 5          5           3.6          1.4         0.2 setosa   1.96\n 6          5.4         3.9          1.7         0.4 setosa   2.89\n 7          4.6         3.4          1.4         0.3 setosa   1.96\n 8          5           3.4          1.5         0.2 setosa   2.25\n 9          4.4         2.9          1.4         0.2 setosa   1.96\n10          4.9         3.1          1.5         0.1 setosa   2.25\n# … with 140 more rows\n\n\n\niris |&gt; as_tibble() |&gt; select(Species, Petal.Length)\n\n# A tibble: 150 × 2\n   Species Petal.Length\n   &lt;fct&gt;          &lt;dbl&gt;\n 1 setosa           1.4\n 2 setosa           1.4\n 3 setosa           1.3\n 4 setosa           1.5\n 5 setosa           1.4\n 6 setosa           1.7\n 7 setosa           1.4\n 8 setosa           1.5\n 9 setosa           1.4\n10 setosa           1.5\n# … with 140 more rows\n\n\n\niris |&gt; as_tibble() |&gt; select(matches(\"Length\"))\n\n# A tibble: 150 × 2\n   Sepal.Length Petal.Length\n          &lt;dbl&gt;        &lt;dbl&gt;\n 1          5.1          1.4\n 2          4.9          1.4\n 3          4.7          1.3\n 4          4.6          1.5\n 5          5            1.4\n 6          5.4          1.7\n 7          4.6          1.4\n 8          5            1.5\n 9          4.4          1.4\n10          4.9          1.5\n# … with 140 more rows\n\n\n\niris |&gt; as_tibble() |&gt; rename(PL = Petal.Length)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width    PL Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5   1.4         0.2 setosa \n 2          4.9         3     1.4         0.2 setosa \n 3          4.7         3.2   1.3         0.2 setosa \n 4          4.6         3.1   1.5         0.2 setosa \n 5          5           3.6   1.4         0.2 setosa \n 6          5.4         3.9   1.7         0.4 setosa \n 7          4.6         3.4   1.4         0.3 setosa \n 8          5           3.4   1.5         0.2 setosa \n 9          4.4         2.9   1.4         0.2 setosa \n10          4.9         3.1   1.5         0.1 setosa \n# … with 140 more rows\n\n\n\niris |&gt; as_tibble() |&gt; \n  rename_with(~str_replace_all(.x, \"[(a-z.)]\", \"\"), .cols = matches(\"(Pet)|(Sep)\"))\n\n# A tibble: 150 × 5\n      SL    SW    PL    PW Species\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  \n 1   5.1   3.5   1.4   0.2 setosa \n 2   4.9   3     1.4   0.2 setosa \n 3   4.7   3.2   1.3   0.2 setosa \n 4   4.6   3.1   1.5   0.2 setosa \n 5   5     3.6   1.4   0.2 setosa \n 6   5.4   3.9   1.7   0.4 setosa \n 7   4.6   3.4   1.4   0.3 setosa \n 8   5     3.4   1.5   0.2 setosa \n 9   4.4   2.9   1.4   0.2 setosa \n10   4.9   3.1   1.5   0.1 setosa \n# … with 140 more rows\n\n\n\niris |&gt; as_tibble() |&gt; pull(Species)\n\n  [1] setosa     setosa     setosa     setosa     setosa     setosa    \n  [7] setosa     setosa     setosa     setosa     setosa     setosa    \n [13] setosa     setosa     setosa     setosa     setosa     setosa    \n [19] setosa     setosa     setosa     setosa     setosa     setosa    \n [25] setosa     setosa     setosa     setosa     setosa     setosa    \n [31] setosa     setosa     setosa     setosa     setosa     setosa    \n [37] setosa     setosa     setosa     setosa     setosa     setosa    \n [43] setosa     setosa     setosa     setosa     setosa     setosa    \n [49] setosa     setosa     versicolor versicolor versicolor versicolor\n [55] versicolor versicolor versicolor versicolor versicolor versicolor\n [61] versicolor versicolor versicolor versicolor versicolor versicolor\n [67] versicolor versicolor versicolor versicolor versicolor versicolor\n [73] versicolor versicolor versicolor versicolor versicolor versicolor\n [79] versicolor versicolor versicolor versicolor versicolor versicolor\n [85] versicolor versicolor versicolor versicolor versicolor versicolor\n [91] versicolor versicolor versicolor versicolor versicolor versicolor\n [97] versicolor versicolor versicolor versicolor virginica  virginica \n[103] virginica  virginica  virginica  virginica  virginica  virginica \n[109] virginica  virginica  virginica  virginica  virginica  virginica \n[115] virginica  virginica  virginica  virginica  virginica  virginica \n[121] virginica  virginica  virginica  virginica  virginica  virginica \n[127] virginica  virginica  virginica  virginica  virginica  virginica \n[133] virginica  virginica  virginica  virginica  virginica  virginica \n[139] virginica  virginica  virginica  virginica  virginica  virginica \n[145] virginica  virginica  virginica  virginica  virginica  virginica \nLevels: setosa versicolor virginica\n\n\n\niris |&gt; as_tibble() |&gt; relocate(Species, .before = \"Sepal.Length\")\n\n# A tibble: 150 × 5\n   Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n   &lt;fct&gt;          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1 setosa           5.1         3.5          1.4         0.2\n 2 setosa           4.9         3            1.4         0.2\n 3 setosa           4.7         3.2          1.3         0.2\n 4 setosa           4.6         3.1          1.5         0.2\n 5 setosa           5           3.6          1.4         0.2\n 6 setosa           5.4         3.9          1.7         0.4\n 7 setosa           4.6         3.4          1.4         0.3\n 8 setosa           5           3.4          1.5         0.2\n 9 setosa           4.4         2.9          1.4         0.2\n10 setosa           4.9         3.1          1.5         0.1\n# … with 140 more rows\n\n\n\niris |&gt; as_tibble() |&gt; relocate(Species, matches(\"Length\"), .before = \"Sepal.Length\")\n\n# A tibble: 150 × 5\n   Species Sepal.Length Petal.Length Sepal.Width Petal.Width\n   &lt;fct&gt;          &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 setosa           5.1          1.4         3.5         0.2\n 2 setosa           4.9          1.4         3           0.2\n 3 setosa           4.7          1.3         3.2         0.2\n 4 setosa           4.6          1.5         3.1         0.2\n 5 setosa           5            1.4         3.6         0.2\n 6 setosa           5.4          1.7         3.9         0.4\n 7 setosa           4.6          1.4         3.4         0.3\n 8 setosa           5            1.5         3.4         0.2\n 9 setosa           4.4          1.4         2.9         0.2\n10 setosa           4.9          1.5         3.1         0.1\n# … with 140 more rows",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#行の加工",
    "href": "part04.html#行の加工",
    "title": "データの操作",
    "section": "行の加工",
    "text": "行の加工\n\niris |&gt; as_tibble() |&gt; filter(str_detect(Species, \"versicolor\"))\n\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n\n\niris |&gt; as_tibble() |&gt; filter(Petal.Length &gt; 6 & Sepal.Length &gt; 7.5)\n\n# A tibble: 6 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;    \n1          7.6         3            6.6         2.1 virginica\n2          7.7         3.8          6.7         2.2 virginica\n3          7.7         2.6          6.9         2.3 virginica\n4          7.7         2.8          6.7         2   virginica\n5          7.9         3.8          6.4         2   virginica\n6          7.7         3            6.1         2.3 virginica\n\n\n\niris |&gt; as_tibble() |&gt; distinct(Species)\n\n# A tibble: 3 × 1\n  Species   \n  &lt;fct&gt;     \n1 setosa    \n2 versicolor\n3 virginica \n\n\n\niris |&gt; as_tibble() |&gt; distinct(Petal.Length, .keep_all = T)\n\n# A tibble: 43 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n 1          5.1         3.5          1.4         0.2 setosa    \n 2          4.7         3.2          1.3         0.2 setosa    \n 3          4.6         3.1          1.5         0.2 setosa    \n 4          5.4         3.9          1.7         0.4 setosa    \n 5          4.8         3.4          1.6         0.2 setosa    \n 6          4.3         3            1.1         0.1 setosa    \n 7          5.8         4            1.2         0.2 setosa    \n 8          4.6         3.6          1           0.2 setosa    \n 9          4.8         3.4          1.9         0.2 setosa    \n10          7           3.2          4.7         1.4 versicolor\n# … with 33 more rows\n\n\n\niris |&gt; as_tibble() |&gt; slice(1:5)\n\n# A tibble: 5 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n4          4.6         3.1          1.5         0.2 setosa \n5          5           3.6          1.4         0.2 setosa \n\niris |&gt; as_tibble() |&gt; slice_head(n = 2)\n\n# A tibble: 2 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n\niris |&gt; as_tibble() |&gt; slice_tail(n = 2)\n\n# A tibble: 2 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;    \n1          6.2         3.4          5.4         2.3 virginica\n2          5.9         3            5.1         1.8 virginica\n\n\n\niris |&gt; as_tibble() |&gt; slice_min(Petal.Length)\n\n# A tibble: 1 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1          4.6         3.6            1         0.2 setosa \n\niris |&gt; as_tibble() |&gt; slice_max(Petal.Length)\n\n# A tibble: 1 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;    \n1          7.7         2.6          6.9         2.3 virginica\n\niris |&gt; as_tibble() |&gt; slice_sample(n = 3)\n\n# A tibble: 3 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n1          5.7         4.4          1.5         0.4 setosa    \n2          6.1         2.8          4.7         1.2 versicolor\n3          6.3         3.3          4.7         1.6 versicolor\n\n\n\niris |&gt; as_tibble() |&gt; arrange(Sepal.Length)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          4.3         3            1.1         0.1 setosa \n 2          4.4         2.9          1.4         0.2 setosa \n 3          4.4         3            1.3         0.2 setosa \n 4          4.4         3.2          1.3         0.2 setosa \n 5          4.5         2.3          1.3         0.3 setosa \n 6          4.6         3.1          1.5         0.2 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          4.6         3.6          1           0.2 setosa \n 9          4.6         3.2          1.4         0.2 setosa \n10          4.7         3.2          1.3         0.2 setosa \n# … with 140 more rows\n\n\n\niris |&gt; as_tibble() |&gt; \n  arrange(desc(Sepal.Length), desc(Sepal.Width))\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;    \n 1          7.9         3.8          6.4         2   virginica\n 2          7.7         3.8          6.7         2.2 virginica\n 3          7.7         3            6.1         2.3 virginica\n 4          7.7         2.8          6.7         2   virginica\n 5          7.7         2.6          6.9         2.3 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          7.4         2.8          6.1         1.9 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          7.2         3.6          6.1         2.5 virginica\n10          7.2         3.2          6           1.8 virginica\n# … with 140 more rows",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#グループ化ネストに関する関数",
    "href": "part04.html#グループ化ネストに関する関数",
    "title": "データの操作",
    "section": "グループ化・ネストに関する関数",
    "text": "グループ化・ネストに関する関数\n\ngroup_by()：tibble をグループ化する\ngroup_nest()：グループ化した tibble をネスト（入れ子）する\nnest()：渡した列をネストする\nunnest()：ネストされている列を展開（アンネスト）する\ngroup_map()：グループ化した tibble に関数を適応して、リストを返す\ngroup_modify()：グループ化した tibble に関数を適応して、tibble を返す",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#tibble-のグループ化",
    "href": "part04.html#tibble-のグループ化",
    "title": "データの操作",
    "section": "tibble のグループ化",
    "text": "tibble のグループ化\n\niris |&gt; as_tibble() |&gt; select(1:3)\n\n# A tibble: 150 × 3\n   Sepal.Length Sepal.Width Petal.Length\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1          5.1         3.5          1.4\n 2          4.9         3            1.4\n 3          4.7         3.2          1.3\n 4          4.6         3.1          1.5\n 5          5           3.6          1.4\n 6          5.4         3.9          1.7\n 7          4.6         3.4          1.4\n 8          5           3.4          1.5\n 9          4.4         2.9          1.4\n10          4.9         3.1          1.5\n# … with 140 more rows\n\n\n\niris |&gt; as_tibble() |&gt; group_by(Species) |&gt; select(1:3)\n\nAdding missing grouping variables: `Species`\n\n\n# A tibble: 150 × 4\n# Groups:   Species [3]\n   Species Sepal.Length Sepal.Width Petal.Length\n   &lt;fct&gt;          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 setosa           5.1         3.5          1.4\n 2 setosa           4.9         3            1.4\n 3 setosa           4.7         3.2          1.3\n 4 setosa           4.6         3.1          1.5\n 5 setosa           5           3.6          1.4\n 6 setosa           5.4         3.9          1.7\n 7 setosa           4.6         3.4          1.4\n 8 setosa           5           3.4          1.5\n 9 setosa           4.4         2.9          1.4\n10 setosa           4.9         3.1          1.5\n# … with 140 more rows\n\n\n\niris |&gt; as_tibble() |&gt; group_nest(Species)\n\n# A tibble: 3 × 2\n  Species                  data\n  &lt;fct&gt;      &lt;list&lt;tibble[,4]&gt;&gt;\n1 setosa               [50 × 4]\n2 versicolor           [50 × 4]\n3 virginica            [50 × 4]\n\n\n\niris |&gt; as_tibble() |&gt; nest(data = matches(\"Length|Width\"))\n\n# A tibble: 3 × 2\n  Species    data             \n  &lt;fct&gt;      &lt;list&gt;           \n1 setosa     &lt;tibble [50 × 4]&gt;\n2 versicolor &lt;tibble [50 × 4]&gt;\n3 virginica  &lt;tibble [50 × 4]&gt;\n\n\n\niris |&gt; as_tibble() |&gt; group_nest(Species) |&gt; unnest(data)\n\n# A tibble: 150 × 5\n   Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n   &lt;fct&gt;          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1 setosa           5.1         3.5          1.4         0.2\n 2 setosa           4.9         3            1.4         0.2\n 3 setosa           4.7         3.2          1.3         0.2\n 4 setosa           4.6         3.1          1.5         0.2\n 5 setosa           5           3.6          1.4         0.2\n 6 setosa           5.4         3.9          1.7         0.4\n 7 setosa           4.6         3.4          1.4         0.3\n 8 setosa           5           3.4          1.5         0.2\n 9 setosa           4.4         2.9          1.4         0.2\n10 setosa           4.9         3.1          1.5         0.1\n# … with 140 more rows\n\n\n\niris |&gt; as_tibble() |&gt; group_by(Species) |&gt; group_map(~head(.x, n = 2))\n\n[[1]]\n# A tibble: 2 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1          5.1         3.5          1.4         0.2\n2          4.9         3            1.4         0.2\n\n[[2]]\n# A tibble: 2 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1          7           3.2          4.7         1.4\n2          6.4         3.2          4.5         1.5\n\n[[3]]\n# A tibble: 2 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1          6.3         3.3          6           2.5\n2          5.8         2.7          5.1         1.9\n\n\n\niris |&gt; as_tibble() |&gt; group_by(Species) |&gt; group_modify(~head(.x, n = 2))\n\n# A tibble: 6 × 5\n# Groups:   Species [3]\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa              5.1         3.5          1.4         0.2\n2 setosa              4.9         3            1.4         0.2\n3 versicolor          7           3.2          4.7         1.4\n4 versicolor          6.4         3.2          4.5         1.5\n5 virginica           6.3         3.3          6           2.5\n6 virginica           5.8         2.7          5.1         1.9",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#その他の関数",
    "href": "part04.html#その他の関数",
    "title": "データの操作",
    "section": "その他の関数",
    "text": "その他の関数\n\ndrop_na()：NA（欠損値）を含む行を削除\nreplace_na()：NAを他の値と書き換える\nfill()：NAを直前の値で埋める\nseparate()：文字列の変数を任意の区切りで複数変数に分裂する\nunite()：複数の変数を任意の区切りで 1 列にまとめる\n\n\nX\n\n# A tibble: 4 × 2\n  x         y\n  &lt;chr&gt; &lt;dbl&gt;\n1 A     NA   \n2 B      4.28\n3 C      3.72\n4 G      3.37\n\n\n\nX |&gt; drop_na()\n\n# A tibble: 3 × 2\n  x         y\n  &lt;chr&gt; &lt;dbl&gt;\n1 B      4.28\n2 C      3.72\n3 G      3.37\n\n\n\nX |&gt; replace_na(list(x = \"Z\", y = 0))\n\n# A tibble: 4 × 2\n  x         y\n  &lt;chr&gt; &lt;dbl&gt;\n1 A      0   \n2 B      4.28\n3 C      3.72\n4 G      3.37\n\n\n\nX |&gt; mutate(y = replace_na(y, 0))\n\n# A tibble: 4 × 2\n  x         y\n  &lt;chr&gt; &lt;dbl&gt;\n1 A      0   \n2 B      4.28\n3 C      3.72\n4 G      3.37\n\n\n\nY |&gt; fill(z)\n\n# A tibble: 4 × 2\n  x         z\n  &lt;chr&gt; &lt;int&gt;\n1 A         4\n2 C         2\n3 D         5\n4 E         5\n\n\n\ntibble(x = c(NA, \"Iris.setosa\", \"Iris.virginica\", \"Iris.versicolor\")) |&gt; \n  separate(x, into = c(\"Genus\", \"Species\"))\n\n# A tibble: 4 × 2\n  Genus Species   \n  &lt;chr&gt; &lt;chr&gt;     \n1 &lt;NA&gt;  &lt;NA&gt;      \n2 Iris  setosa    \n3 Iris  virginica \n4 Iris  versicolor\n\n\n\ntibble(x = rep(\"Iris\", 3), y = c(\"setosa\", \"virginica\", \"versicolor\")) |&gt; \n  unite(Species, x, y, sep = \"_\")\n\n# A tibble: 3 × 1\n  Species        \n  &lt;chr&gt;          \n1 Iris_setosa    \n2 Iris_virginica \n3 Iris_versicolor",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#ピボットtibbleを変形する関数",
    "href": "part04.html#ピボットtibbleを変形する関数",
    "title": "データの操作",
    "section": "ピボット・tibbleを変形する関数",
    "text": "ピボット・tibbleを変形する関数\n\npivot_longer()：tibble を wide format （横広）から long format （縦長）に変える\npivot_wider()：tibble をlong format から wide format に変える\n\n\n重要な引数\npivot_longer()\n\ncols：動かす変数\nnames_to：動かした変数の名前の移動先\nvalues_to：動かした変数の値の移動先\nnames_transform：移動先の変数のタイプを変換\n\npivot_wider()\n\nid_cols：行（値）を区別するための列名\nnames_from：移動先の列名になる変数\nvalues_from：移動したい値\nvalues_fill：存在しない要素の埋め込み方法\nvalues_fn：行の区別ができないときの処理（デフォルトはリスト）",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#pivot_longer-の使い方",
    "href": "part04.html#pivot_longer-の使い方",
    "title": "データの操作",
    "section": "pivot_longer() の使い方",
    "text": "pivot_longer() の使い方\n\nrelig_income |&gt; as_tibble()\n\n# A tibble: 18 × 11\n   religion      `&lt;$10k` $10-2…¹ $20-3…² $30-4…³ $40-5…⁴ $50-7…⁵ $75-1…⁶ $100-…⁷\n   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Agnostic           27      34      60      81      76     137     122     109\n 2 Atheist            12      27      37      52      35      70      73      59\n 3 Buddhist           27      21      30      34      33      58      62      39\n 4 Catholic          418     617     732     670     638    1116     949     792\n 5 Don’t know/r…      15      14      15      11      10      35      21      17\n 6 Evangelical …     575     869    1064     982     881    1486     949     723\n 7 Hindu               1       9       7       9      11      34      47      48\n 8 Historically…     228     244     236     238     197     223     131      81\n 9 Jehovah's Wi…      20      27      24      24      21      30      15      11\n10 Jewish             19      19      25      25      30      95      69      87\n11 Mainline Prot     289     495     619     655     651    1107     939     753\n12 Mormon             29      40      48      51      56     112      85      49\n13 Muslim              6       7       9      10       9      23      16       8\n14 Orthodox           13      17      23      32      32      47      38      42\n15 Other Christ…       9       7      11      13      13      14      18      14\n16 Other Faiths       20      33      40      46      49      63      46      40\n17 Other World …       5       2       3       4       2       7       3       4\n18 Unaffiliated      217     299     374     365     341     528     407     321\n# … with 2 more variables: `&gt;150k` &lt;dbl&gt;, `Don't know/refused` &lt;dbl&gt;, and\n#   abbreviated variable names ¹​`$10-20k`, ²​`$20-30k`, ³​`$30-40k`, ⁴​`$40-50k`,\n#   ⁵​`$50-75k`, ⁶​`$75-100k`, ⁷​`$100-150k`\n\n\n\nrelig_income |&gt; as_tibble() |&gt; \npivot_longer(!religion, names_to = \"income\", values_to = \"count\")\n\n# A tibble: 180 × 3\n   religion income             count\n   &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n 1 Agnostic &lt;$10k                 27\n 2 Agnostic $10-20k               34\n 3 Agnostic $20-30k               60\n 4 Agnostic $30-40k               81\n 5 Agnostic $40-50k               76\n 6 Agnostic $50-75k              137\n 7 Agnostic $75-100k             122\n 8 Agnostic $100-150k            109\n 9 Agnostic &gt;150k                 84\n10 Agnostic Don't know/refused    96\n# … with 170 more rows\n\n\n\nbillboard |&gt; as_tibble()\n\n# A tibble: 317 × 79\n   artist track date.ent…¹   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8   wk9\n   &lt;chr&gt;  &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac  Baby… 2000-02-26    87    82    72    77    87    94    99    NA    NA\n 2 2Ge+h… The … 2000-09-02    91    87    92    NA    NA    NA    NA    NA    NA\n 3 3 Doo… Kryp… 2000-04-08    81    70    68    67    66    57    54    53    51\n 4 3 Doo… Loser 2000-10-21    76    76    72    69    67    65    55    59    62\n 5 504 B… Wobb… 2000-04-15    57    34    25    17    17    31    36    49    53\n 6 98^0   Give… 2000-08-19    51    39    34    26    26    19     2     2     3\n 7 A*Tee… Danc… 2000-07-08    97    97    96    95   100    NA    NA    NA    NA\n 8 Aaliy… I Do… 2000-01-29    84    62    51    41    38    35    35    38    38\n 9 Aaliy… Try … 2000-03-18    59    53    38    28    21    18    16    14    12\n10 Adams… Open… 2000-08-26    76    76    74    69    68    67    61    58    57\n# … with 307 more rows, 67 more variables: wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;,\n#   wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, …\n\n\n\nbillboard |&gt; as_tibble() |&gt; \n  pivot_longer(col = starts_with(\"wk\"),\n               names_to = \"week\", names_prefix = \"wk\",\n               values_to = \"rank\", values_drop_na = TRUE)\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered week   rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   1        87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   2        82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   3        72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   4        77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   5        87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   6        94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   7        99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   1        91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   2        87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   3        92\n# … with 5,297 more rows\n\n\n\nwho |&gt; as_tibble()\n\n# A tibble: 7,240 × 60\n   country     iso2  iso3   year new_s…¹ new_s…² new_s…³ new_s…⁴ new_s…⁵ new_s…⁶\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Afghanistan AF    AFG    1980      NA      NA      NA      NA      NA      NA\n 2 Afghanistan AF    AFG    1981      NA      NA      NA      NA      NA      NA\n 3 Afghanistan AF    AFG    1982      NA      NA      NA      NA      NA      NA\n 4 Afghanistan AF    AFG    1983      NA      NA      NA      NA      NA      NA\n 5 Afghanistan AF    AFG    1984      NA      NA      NA      NA      NA      NA\n 6 Afghanistan AF    AFG    1985      NA      NA      NA      NA      NA      NA\n 7 Afghanistan AF    AFG    1986      NA      NA      NA      NA      NA      NA\n 8 Afghanistan AF    AFG    1987      NA      NA      NA      NA      NA      NA\n 9 Afghanistan AF    AFG    1988      NA      NA      NA      NA      NA      NA\n10 Afghanistan AF    AFG    1989      NA      NA      NA      NA      NA      NA\n# … with 7,230 more rows, 50 more variables: new_sp_m65 &lt;dbl&gt;,\n#   new_sp_f014 &lt;dbl&gt;, new_sp_f1524 &lt;dbl&gt;, new_sp_f2534 &lt;dbl&gt;,\n#   new_sp_f3544 &lt;dbl&gt;, new_sp_f4554 &lt;dbl&gt;, new_sp_f5564 &lt;dbl&gt;,\n#   new_sp_f65 &lt;dbl&gt;, new_sn_m014 &lt;dbl&gt;, new_sn_m1524 &lt;dbl&gt;,\n#   new_sn_m2534 &lt;dbl&gt;, new_sn_m3544 &lt;dbl&gt;, new_sn_m4554 &lt;dbl&gt;,\n#   new_sn_m5564 &lt;dbl&gt;, new_sn_m65 &lt;dbl&gt;, new_sn_f014 &lt;dbl&gt;,\n#   new_sn_f1524 &lt;dbl&gt;, new_sn_f2534 &lt;dbl&gt;, new_sn_f3544 &lt;dbl&gt;, …\n\n\n\nwho %&gt;% as_tibble() |&gt;\n  pivot_longer(cols = new_sp_m014:newrel_f65,\n               names_to = c(\"diagnosis\", \"gender\", \"age\"),\n               names_pattern = \"new_?(.*)_(.)(.*)\",\n               values_to = \"count\", values_drop_na = TRUE)\n\n# A tibble: 76,046 × 8\n   country     iso2  iso3   year diagnosis gender age   count\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n 1 Afghanistan AF    AFG    1997 sp        m      014       0\n 2 Afghanistan AF    AFG    1997 sp        m      1524     10\n 3 Afghanistan AF    AFG    1997 sp        m      2534      6\n 4 Afghanistan AF    AFG    1997 sp        m      3544      3\n 5 Afghanistan AF    AFG    1997 sp        m      4554      5\n 6 Afghanistan AF    AFG    1997 sp        m      5564      2\n 7 Afghanistan AF    AFG    1997 sp        m      65        0\n 8 Afghanistan AF    AFG    1997 sp        f      014       5\n 9 Afghanistan AF    AFG    1997 sp        f      1524     38\n10 Afghanistan AF    AFG    1997 sp        f      2534     36\n# … with 76,036 more rows\n\n\n\nanscombe |&gt; as_tibble()\n\n# A tibble: 11 × 8\n      x1    x2    x3    x4    y1    y2    y3    y4\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    10    10    10     8  8.04  9.14  7.46  6.58\n 2     8     8     8     8  6.95  8.14  6.77  5.76\n 3    13    13    13     8  7.58  8.74 12.7   7.71\n 4     9     9     9     8  8.81  8.77  7.11  8.84\n 5    11    11    11     8  8.33  9.26  7.81  8.47\n 6    14    14    14     8  9.96  8.1   8.84  7.04\n 7     6     6     6     8  7.24  6.13  6.08  5.25\n 8     4     4     4    19  4.26  3.1   5.39 12.5 \n 9    12    12    12     8 10.8   9.13  8.15  5.56\n10     7     7     7     8  4.82  7.26  6.42  7.91\n11     5     5     5     8  5.68  4.74  5.73  6.89\n\n\n\nanscombe %&gt;% as_tibble() |&gt; \n pivot_longer(everything(), names_to = c(\".value\", \"set\"), names_pattern = \"(.)(.)\"\n )\n\n# A tibble: 44 × 3\n   set       x     y\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1        10  8.04\n 2 2        10  9.14\n 3 3        10  7.46\n 4 4         8  6.58\n 5 1         8  6.95\n 6 2         8  8.14\n 7 3         8  6.77\n 8 4         8  5.76\n 9 1        13  7.58\n10 2        13  8.74\n# … with 34 more rows",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#pivot_wider-の使い方",
    "href": "part04.html#pivot_wider-の使い方",
    "title": "データの操作",
    "section": "pivot_wider() の使い方",
    "text": "pivot_wider() の使い方\n\nfish_encounters\n\n# A tibble: 114 × 3\n   fish  station  seen\n   &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1 4842  Release     1\n 2 4842  I80_1       1\n 3 4842  Lisbon      1\n 4 4842  Rstr        1\n 5 4842  Base_TD     1\n 6 4842  BCE         1\n 7 4842  BCW         1\n 8 4842  BCE2        1\n 9 4842  BCW2        1\n10 4842  MAE         1\n# … with 104 more rows\n\n\n\nfish_encounters |&gt; as_tibble() |&gt; \n  pivot_wider(names_from = station, values_from = seen)\n\n# A tibble: 19 × 12\n   fish  Release I80_1 Lisbon  Rstr Base_TD   BCE   BCW  BCE2  BCW2   MAE   MAW\n   &lt;fct&gt;   &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 4842        1     1      1     1       1     1     1     1     1     1     1\n 2 4843        1     1      1     1       1     1     1     1     1     1     1\n 3 4844        1     1      1     1       1     1     1     1     1     1     1\n 4 4845        1     1      1     1       1    NA    NA    NA    NA    NA    NA\n 5 4847        1     1      1    NA      NA    NA    NA    NA    NA    NA    NA\n 6 4848        1     1      1     1      NA    NA    NA    NA    NA    NA    NA\n 7 4849        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n 8 4850        1     1     NA     1       1     1     1    NA    NA    NA    NA\n 9 4851        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n10 4854        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n11 4855        1     1      1     1       1    NA    NA    NA    NA    NA    NA\n12 4857        1     1      1     1       1     1     1     1     1    NA    NA\n13 4858        1     1      1     1       1     1     1     1     1     1     1\n14 4859        1     1      1     1       1    NA    NA    NA    NA    NA    NA\n15 4861        1     1      1     1       1     1     1     1     1     1     1\n16 4862        1     1      1     1       1     1     1     1     1    NA    NA\n17 4863        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n18 4864        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n19 4865        1     1      1    NA      NA    NA    NA    NA    NA    NA    NA\n\n\n\nfish_encounters\n\n# A tibble: 114 × 3\n   fish  station  seen\n   &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1 4842  Release     1\n 2 4842  I80_1       1\n 3 4842  Lisbon      1\n 4 4842  Rstr        1\n 5 4842  Base_TD     1\n 6 4842  BCE         1\n 7 4842  BCW         1\n 8 4842  BCE2        1\n 9 4842  BCW2        1\n10 4842  MAE         1\n# … with 104 more rows\n\n\n\n# 存在しない組み合わせの要素を埋める\nfish_encounters |&gt;  as_tibble() |&gt; \n  pivot_wider(names_from = station, values_from = seen, values_fill = 0)\n\n# A tibble: 19 × 12\n   fish  Release I80_1 Lisbon  Rstr Base_TD   BCE   BCW  BCE2  BCW2   MAE   MAW\n   &lt;fct&gt;   &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 4842        1     1      1     1       1     1     1     1     1     1     1\n 2 4843        1     1      1     1       1     1     1     1     1     1     1\n 3 4844        1     1      1     1       1     1     1     1     1     1     1\n 4 4845        1     1      1     1       1     0     0     0     0     0     0\n 5 4847        1     1      1     0       0     0     0     0     0     0     0\n 6 4848        1     1      1     1       0     0     0     0     0     0     0\n 7 4849        1     1      0     0       0     0     0     0     0     0     0\n 8 4850        1     1      0     1       1     1     1     0     0     0     0\n 9 4851        1     1      0     0       0     0     0     0     0     0     0\n10 4854        1     1      0     0       0     0     0     0     0     0     0\n11 4855        1     1      1     1       1     0     0     0     0     0     0\n12 4857        1     1      1     1       1     1     1     1     1     0     0\n13 4858        1     1      1     1       1     1     1     1     1     1     1\n14 4859        1     1      1     1       1     0     0     0     0     0     0\n15 4861        1     1      1     1       1     1     1     1     1     1     1\n16 4862        1     1      1     1       1     1     1     1     1     0     0\n17 4863        1     1      0     0       0     0     0     0     0     0     0\n18 4864        1     1      0     0       0     0     0     0     0     0     0\n19 4865        1     1      1     0       0     0     0     0     0     0     0\n\n\n\nus_rent_income |&gt; as_tibble()\n\n# A tibble: 104 × 5\n   GEOID NAME       variable estimate   moe\n   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 01    Alabama    income      24476   136\n 2 01    Alabama    rent          747     3\n 3 02    Alaska     income      32940   508\n 4 02    Alaska     rent         1200    13\n 5 04    Arizona    income      27517   148\n 6 04    Arizona    rent          972     4\n 7 05    Arkansas   income      23789   165\n 8 05    Arkansas   rent          709     5\n 9 06    California income      29454   109\n10 06    California rent         1358     3\n# … with 94 more rows\n\n\n\nus_rent_income |&gt; as_tibble() |&gt; \n  pivot_wider(names_from = variable, values_from = c(estimate, moe))\n\n# A tibble: 52 × 6\n   GEOID NAME                 estimate_income estimate_rent moe_income moe_rent\n   &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 01    Alabama                        24476           747        136        3\n 2 02    Alaska                         32940          1200        508       13\n 3 04    Arizona                        27517           972        148        4\n 4 05    Arkansas                       23789           709        165        5\n 5 06    California                     29454          1358        109        3\n 6 08    Colorado                       32401          1125        109        5\n 7 09    Connecticut                    35326          1123        195        5\n 8 10    Delaware                       31560          1076        247       10\n 9 11    District of Columbia           43198          1424        681       17\n10 12    Florida                        25952          1077         70        3\n# … with 42 more rows\n\n# us_rent_income  |&gt; as_tibble() |&gt; \n#   pivot_wider(names_from = variable,\n#               names_sep = \".\",\n#               values_from = c(estimate, moe))\n\n# us_rent_income  |&gt; as_tibble() |&gt; \n#   pivot_wider(names_from = variable,\n#               names_glue = \"{variable}_{.value}\",\n#               values_from = c(estimate, moe))\n\n\nwarpbreaks |&gt; as_tibble()\n\n# A tibble: 54 × 3\n   breaks wool  tension\n    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  \n 1     26 A     L      \n 2     30 A     L      \n 3     54 A     L      \n 4     25 A     L      \n 5     70 A     L      \n 6     52 A     L      \n 7     51 A     L      \n 8     26 A     L      \n 9     67 A     L      \n10     18 A     M      \n# … with 44 more rows\n\n\n\nwarpbreaks |&gt; as_tibble() |&gt; \n  pivot_wider(names_from = wool,\n              values_from = breaks)\n\nWarning: Values from `breaks` are not uniquely identified; output will contain\nlist-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} %&gt;%\n  dplyr::group_by(tension, wool) %&gt;%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %&gt;%\n  dplyr::filter(n &gt; 1L)\n\n\n# A tibble: 3 × 3\n  tension A         B        \n  &lt;fct&gt;   &lt;list&gt;    &lt;list&gt;   \n1 L       &lt;dbl [9]&gt; &lt;dbl [9]&gt;\n2 M       &lt;dbl [9]&gt; &lt;dbl [9]&gt;\n3 H       &lt;dbl [9]&gt; &lt;dbl [9]&gt;\n\n\n\nwarpbreaks |&gt; as_tibble()\n\n# A tibble: 54 × 3\n   breaks wool  tension\n    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  \n 1     26 A     L      \n 2     30 A     L      \n 3     54 A     L      \n 4     25 A     L      \n 5     70 A     L      \n 6     52 A     L      \n 7     51 A     L      \n 8     26 A     L      \n 9     67 A     L      \n10     18 A     M      \n# … with 44 more rows\n\n\n\nwarpbreaks |&gt; as_tibble() |&gt; \n  pivot_wider(names_from = wool,\n              values_from = breaks,\n              values_fn = mean)\n\n# A tibble: 3 × 3\n  tension     A     B\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 L        44.6  28.2\n2 M        24    28.8\n3 H        24.6  18.8",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#不都合なデータ構造",
    "href": "part04.html#不都合なデータ構造",
    "title": "データの操作",
    "section": "不都合なデータ構造",
    "text": "不都合なデータ構造\n\nfname = \"photosynthesis1_low.csv\"\ndset1_low = read_csv(fname)\n\n\ndset1_low\n\n# A tibble: 35 × 12\n   sample   min   `0`   `5`  `10`  `15`  `20`  `25`  `30`  `35`  `40`  `45`\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      1     0  9.99 10.1  10.0  10.0  10.1   9.81 10.0  10.1  10.0  10.2 \n 2      2     0  9.97  9.99 10.1  10.0   9.99  9.94  9.91  9.95  9.79  9.97\n 3      3     0 10.0  10.1  10.1   9.97 10.1  10.0  10.0  10.0   9.88 10.0 \n 4      4     0 10.0   9.87 10.1   9.87 10.0   9.95 10.2  10.0  10.1  10.1 \n 5      5     0  9.92  9.97  9.88  9.86 10.0   9.91 10.0   9.94 10.0   9.95\n 6      1     5  9.67 10.0   9.93 10.5  10.4  10.7  10.8  10.6  10.7  10.6 \n 7      2     5  9.40  9.87 10.1  10.0  10.4  10.6  10.7  10.6  10.6  10.7 \n 8      3     5  9.37  9.84 10.2  10.3  10.5  10.6  10.5  10.6  10.7  10.7 \n 9      4     5  9.52  9.71  9.92 10.1  10.5  10.5  10.7  10.5  10.7  10.8 \n10      5     5  9.65  9.83 10.1  10.4  10.4  10.6  10.5  10.5  10.7  10.7 \n# … with 25 more rows\n\n\nsample と min の列はサンプル番号と時間 (minutes) の変数です。 それぞれに、サンプル番号と時間の値が入っています。 0 から 45 の列には溶存酸素濃度の値が入っています。 この時の変数名は光条件ですね。",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#ワイドからロングへ変換",
    "href": "part04.html#ワイドからロングへ変換",
    "title": "データの操作",
    "section": "ワイドからロングへ変換",
    "text": "ワイドからロングへ変換\n\ndset1_low |&gt; \n  pivot_longer(cols = matches(\"[0-9]+\"),　names_to = \"light\",\n               names_transform  = list(light = as.numeric))\n\n# A tibble: 350 × 4\n   sample   min light value\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      1     0     0  9.99\n 2      1     0     5 10.1 \n 3      1     0    10 10.0 \n 4      1     0    15 10.0 \n 5      1     0    20 10.1 \n 6      1     0    25  9.81\n 7      1     0    30 10.0 \n 8      1     0    35 10.1 \n 9      1     0    40 10.0 \n10      1     0    45 10.2 \n# … with 340 more rows",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#残りのデータの読み込み",
    "href": "part04.html#残りのデータの読み込み",
    "title": "データの操作",
    "section": "残りのデータの読み込み",
    "text": "残りのデータの読み込み\n\ndset1_high = read_csv(\"photosynthesis1_high.csv\")\ndset2_low  = read_csv(\"photosynthesis2_low.csv\")\ndset2_high = read_csv(\"photosynthesis2_high.csv\")",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#ピボットしてから結合",
    "href": "part04.html#ピボットしてから結合",
    "title": "データの操作",
    "section": "ピボットしてから結合",
    "text": "ピボットしてから結合\n\ndset1_low  = dset1_low  |&gt; pivot_longer(cols = matches(\"[0-9]+\"), names_to = \"light\", names_transform = list(light = as.numeric))\ndset1_high = dset1_high |&gt; pivot_longer(cols = matches(\"[0-9]+\"), names_to = \"light\", names_transform = list(light = as.numeric))\ndset2_low  = dset2_low  |&gt; pivot_longer(cols = matches(\"[0-9]+\"), names_to = \"light\", names_transform = list(light = as.numeric))\ndset2_high = dset2_high |&gt; pivot_longer(cols = matches(\"[0-9]+\"), names_to = \"light\", names_transform = list(light = as.numeric))\nalldata = bind_rows(dset1_low, dset2_low, dset1_high, dset2_high)\nalldata\n\n# A tibble: 910 × 4\n   sample   min light value\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      1     0     0  9.99\n 2      1     0     5 10.1 \n 3      1     0    10 10.0 \n 4      1     0    15 10.0 \n 5      1     0    20 10.1 \n 6      1     0    25  9.81\n 7      1     0    30 10.0 \n 8      1     0    35 10.1 \n 9      1     0    40 10.0 \n10      1     0    45 10.2 \n# … with 900 more rows",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part04.html#結合してからピボット",
    "href": "part04.html#結合してからピボット",
    "title": "データの操作",
    "section": "結合してからピボット",
    "text": "結合してからピボット\n\n\nRows: 35 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (12): sample, min, 0, 5, 10, 15, 20, 25, 30, 35, 40, 45\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 35 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): sample, min, 50, 75, 100\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 35 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (12): sample, min, 0, 5, 10, 15, 20, 25, 30, 35, 40, 45\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 35 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): sample, min, 50, 75, 100\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndset1 = full_join(dset1_low, dset1_high, by = c(\"sample\", \"min\"))\ndset2 = full_join(dset2_low, dset2_high, by = c(\"sample\", \"min\"))\nalldata = bind_rows(dset1, dset2)\nalldata = alldata |&gt; \n  pivot_longer(cols = matches(\"[0-9]+\"), names_to = \"light\", \n               names_transform = list(light = as.numeric))",
    "crumbs": [
      "Rの基礎",
      "データの操作"
    ]
  },
  {
    "objectID": "part02.html",
    "href": "part02.html",
    "title": "R 関数の基本",
    "section": "",
    "text": "関数をつくることにより、 での作業がとても楽になります。 コードを繰り返して使うなら関数をつくりましょう。\n\n\nR の関数に 2 つのパーツがあります。\n\nArguments: 引数\nCode block: 関数のコードは {} の間に納めます。\n\n\nhello = function(x) {\n  if(!is.character(x)) {\n    stop(\"Please provide a character string.\")\n  }\n  sprintf(\"Hello %s!\", x)\n}\n\nhello(214)\n\nError in hello(214): Please provide a character string.\n\nhello(\"Yukio\")\n\n[1] \"Hello Yukio!\"\n\n\n\n\n\n関数の中に作ったものは、関数の中にしか存在しない。\n\nsumofsquare = function(x) {\n  ss = (x - mean(x))^2 # 関数の外から見れない\n  ssq = sum(ss) # 関数の外から見れない\n  ssq # 関数の外に返す\n}\ndata = sample(1:10, 5, replace = TRUE)\ndata\n\n[1]  7  6 10  7  4\n\nvalue = sumofsquare(data)\nvalue\n\n[1] 18.8\n\n\n\n\n\nところが、関数は外の環境に存在するものは見れます。 このように関数を作ると、バグを起こしやすいので、注意。\n\nsumofsquare = function(x) {\n  ss = (s - mean(s))^2 # s は関数の外にあるが、関数の引数ではない\n  ssq = sum(ss) # 関数の外から見れない\n  ssq # 関数の外に返す\n}\ns = sample(100:1000, 5, replace = TRUE)\ns\n\n[1] 169 291 862 713 721\n\ndata = sample(1:10, 5, replace = TRUE)\ndata\n\n[1] 5 7 9 7 3\n\nvalue = sumofsquare(data)\nvalue　# これは s の平方和です。\n\n[1] 365388.8\n\n\n\n\n\n関数は次のようにもかけます。 \\(x){...} はラムダ式 (lambda expression) とも呼ばれています。\n\nadd_one = \\(x) { x + 1}\nadd_one(5)\n\n[1] 6\n\n\n無名関数をつくるときに便利な書き方です。\n\n# どちれも無名関数ですが、２つ目の関数がはラムダ式です。\nz = 1:5\nsapply(z, FUN = function(s){s^2})\n\n[1]  1  4  9 16 25\n\nsapply(z, FUN = \\(s){s^2})\n\n[1]  1  4  9 16 25",
    "crumbs": [
      "Rの基礎",
      "R 関数の基本"
    ]
  },
  {
    "objectID": "part02.html#関数の作り方",
    "href": "part02.html#関数の作り方",
    "title": "R 関数の基本",
    "section": "",
    "text": "R の関数に 2 つのパーツがあります。\n\nArguments: 引数\nCode block: 関数のコードは {} の間に納めます。\n\n\nhello = function(x) {\n  if(!is.character(x)) {\n    stop(\"Please provide a character string.\")\n  }\n  sprintf(\"Hello %s!\", x)\n}\n\nhello(214)\n\nError in hello(214): Please provide a character string.\n\nhello(\"Yukio\")\n\n[1] \"Hello Yukio!\"",
    "crumbs": [
      "Rの基礎",
      "R 関数の基本"
    ]
  },
  {
    "objectID": "part02.html#関数のスコープ-scope１",
    "href": "part02.html#関数のスコープ-scope１",
    "title": "R 関数の基本",
    "section": "",
    "text": "関数の中に作ったものは、関数の中にしか存在しない。\n\nsumofsquare = function(x) {\n  ss = (x - mean(x))^2 # 関数の外から見れない\n  ssq = sum(ss) # 関数の外から見れない\n  ssq # 関数の外に返す\n}\ndata = sample(1:10, 5, replace = TRUE)\ndata\n\n[1]  7  6 10  7  4\n\nvalue = sumofsquare(data)\nvalue\n\n[1] 18.8",
    "crumbs": [
      "Rの基礎",
      "R 関数の基本"
    ]
  },
  {
    "objectID": "part02.html#関数のスコープ-scope２",
    "href": "part02.html#関数のスコープ-scope２",
    "title": "R 関数の基本",
    "section": "",
    "text": "ところが、関数は外の環境に存在するものは見れます。 このように関数を作ると、バグを起こしやすいので、注意。\n\nsumofsquare = function(x) {\n  ss = (s - mean(s))^2 # s は関数の外にあるが、関数の引数ではない\n  ssq = sum(ss) # 関数の外から見れない\n  ssq # 関数の外に返す\n}\ns = sample(100:1000, 5, replace = TRUE)\ns\n\n[1] 169 291 862 713 721\n\ndata = sample(1:10, 5, replace = TRUE)\ndata\n\n[1] 5 7 9 7 3\n\nvalue = sumofsquare(data)\nvalue　# これは s の平方和です。\n\n[1] 365388.8",
    "crumbs": [
      "Rの基礎",
      "R 関数の基本"
    ]
  },
  {
    "objectID": "part02.html#諸略した関数の書き方と無名関数-anonymous-function",
    "href": "part02.html#諸略した関数の書き方と無名関数-anonymous-function",
    "title": "R 関数の基本",
    "section": "",
    "text": "関数は次のようにもかけます。 \\(x){...} はラムダ式 (lambda expression) とも呼ばれています。\n\nadd_one = \\(x) { x + 1}\nadd_one(5)\n\n[1] 6\n\n\n無名関数をつくるときに便利な書き方です。\n\n# どちれも無名関数ですが、２つ目の関数がはラムダ式です。\nz = 1:5\nsapply(z, FUN = function(s){s^2})\n\n[1]  1  4  9 16 25\n\nsapply(z, FUN = \\(s){s^2})\n\n[1]  1  4  9 16 25",
    "crumbs": [
      "Rの基礎",
      "R 関数の基本"
    ]
  },
  {
    "objectID": "part00.html",
    "href": "part00.html",
    "title": "R と RStudio について",
    "section": "",
    "text": "Rはプログラミング言語でありますが、実際はそれ以上のものです。 Rは統計計算とグラフィックスのための環境であす。\n\nビッグデータの効率的な処理\n統計解析・シミュレーション・作図\nインタプリタ型のプログラミング言\nオープンソース\nMacOS, Windows, Linux などのOSにインストールできる\n解析の履歴が残る",
    "crumbs": [
      "その前に",
      "R と RStudio について"
    ]
  },
  {
    "objectID": "part00.html#rとは",
    "href": "part00.html#rとは",
    "title": "R と RStudio について",
    "section": "",
    "text": "Rはプログラミング言語でありますが、実際はそれ以上のものです。 Rは統計計算とグラフィックスのための環境であす。\n\nビッグデータの効率的な処理\n統計解析・シミュレーション・作図\nインタプリタ型のプログラミング言\nオープンソース\nMacOS, Windows, Linux などのOSにインストールできる\n解析の履歴が残る",
    "crumbs": [
      "その前に",
      "R と RStudio について"
    ]
  },
  {
    "objectID": "part00.html#rstudio-とは",
    "href": "part00.html#rstudio-とは",
    "title": "R と RStudio について",
    "section": "RStudio とは？",
    "text": "RStudio とは？\nRStudio はオープンソースの統合開発環境 (Integrated Development Environment; IDE) です。 RStudio は、Rをより簡単に使用するための多くの機能を備えた環境を提供しています。",
    "crumbs": [
      "その前に",
      "R と RStudio について"
    ]
  },
  {
    "objectID": "part00.html#rstudio-を用いた基本操作",
    "href": "part00.html#rstudio-を用いた基本操作",
    "title": "R と RStudio について",
    "section": "RStudio を用いた基本操作",
    "text": "RStudio を用いた基本操作\nプロジェクト・ディレクトリをつくりましょう。\n\nRStudio を起動する\nFile メニューまたは右上にある Project のアイコンから New Project を選ぶ\n\n\n\n\n\n\n\n\n\n\n\nNew Project ウィンドウから New Directory を選んで、つぎに New Project を選ぶ。プロジェクト名を記入して、Create Project ボタンをクリックする。\n\n\n\n\n\n\n\n\n\n\n\nプロジェクトが完成した後、プロジェクトが自動的に RStudio で開かない場合は、File メニューから Open Project を選択し、First_Project.Rproj を選択します。\nRStudio が開くと、ウィンドウに 3 つのパネルが表示されます。\nFile メニューから New File を選択し、R Script を選択します。\nFile メニューから Save As... を選択し、First_Project.R と入力して Save を選択します。\n\n\n\n\n\n\n\nRStudio プロジェクト\n\n\n\n特定のプロジェクトの分析に関連するすべてを含むディレクトリです。 RStudio のプロジェクトは、コンテキスト固有の分析に取り組んでいて、それらを別々に管理したい場合に便利です。 RStudio でプロジェクトを作成する場合、任意の作業ディレクトリ（既存のもの、または新しいもの）に関連付けます。 .RProj ファイルは、そのディレクトリ内に作成され、コマンド履歴と環境変数を記録します。 この .RProj ファイルを使用して、現在の状態でプロジェクトを開くことができます。\nRStudio 内でプロジェクトを（再度）開くと、以下のアクションが実行されます：\n新しい R セッション（プロセス）が開始されます。 プロジェクトのメインディレクトリの .RData ファイルがロードされ、プロジェクトがクローズされたときに存在していたオブジェクトが環境に追加されます。 プロジェクトのメインディレクトリの .Rhistory ファイルが RStudio History ペインにロードされます（コンソールの上下矢印コマンド履歴に使用されます）。 現在の作業ディレクトリがプロジェクトディレクトリに設定されます。 以前に編集したソースドキュメントがエディタタブに復元されます。 その他の RStudio 設定（アクティブなタブ、スプリッタの位置など）は、プロジェクトを最後に閉じたときの状態に復元されます。",
    "crumbs": [
      "その前に",
      "R と RStudio について"
    ]
  },
  {
    "objectID": "part00.html#rstudio-インターフェイス",
    "href": "part00.html#rstudio-インターフェイス",
    "title": "R と RStudio について",
    "section": "RStudio インターフェイス",
    "text": "RStudio インターフェイス\nRStudio インターフェースには、4 つのメインパネルがあります：\n\nコンソール： コマンドを入力し、出力を見ることができます。コンソールは、RStudio を使用せずにコマンドラインで R を実行した場合に表示されます。\nスクリプトエディタ： コマンドを入力し、ファイルに保存できます。コンソールで実行するコマンドを送信することもできます。\n環境/履歴: 環境はすべてのアクティブなオブジェクトを表示し、履歴はコンソールで実行されたすべてのコマンドを記録します。\nファイル/プロット/パッケージ/ヘルプ",
    "crumbs": [
      "その前に",
      "R と RStudio について"
    ]
  },
  {
    "objectID": "part00.html#作業ディレクトリの整理と設定",
    "href": "part00.html#作業ディレクトリの整理と設定",
    "title": "R と RStudio について",
    "section": "作業ディレクトリの整理と設定",
    "text": "作業ディレクトリの整理と設定\n作業ディレクトリの表示\n作業ディレクトリを整理する前に、現在の作業ディレクトリがどこにあるか、コンソールに入力して確認してみよう：\n\ngetwd()\n\n作業ディレクトリは、プロジェクト作成時に作成した First_Project フォルダである必要があります。 作業ディレクトリは、特に指定がない限り、RStudio が自動的にファイルを探す場所であり、作成したファイルを自動的に保存する場所です。\nFiles/Plots/Packages/Help ウィンドウから Files タブを選択すると、作業ディレクトリを表示することができます。 もし別のディレクトリを作業ディレクトリにしたい場合は、Files タブで別のフォルダに移動し、 コグとして表示される More ドロップダウンメニューをクリックし、Set As Working Directoryを選択します。",
    "crumbs": [
      "その前に",
      "R と RStudio について"
    ]
  },
  {
    "objectID": "part00.html#作業ディレクトリの構造化",
    "href": "part00.html#作業ディレクトリの構造化",
    "title": "R と RStudio について",
    "section": "作業ディレクトリの構造化",
    "text": "作業ディレクトリの構造化\n特定の解析のために作業ディレクトリを整理するには、元のデータ（生データ）と中間データセットを分ける必要があります。 例えば、生データを格納する data/ ディレクトリを作業ディレクトリに作成し、 中間データセットを格納する results/ ディレクトリと、 生成するプロットを格納する figures/ ディレクトリを作成します。\nFiles タブ内の New Folder をクリックして、作業ディレクトリ内にこれら 3 つのディレクトリを作成しよう。\n完了すると、作業ディレクトリは以下のようになるはずです：",
    "crumbs": [
      "その前に",
      "R と RStudio について"
    ]
  },
  {
    "objectID": "part00.html#セットアップ",
    "href": "part00.html#セットアップ",
    "title": "R と RStudio について",
    "section": "セットアップ",
    "text": "セットアップ\nRStudio IDE の詳細なセットアップをします。 IDEの使い方に個人差があるが、参考にしてください。\nRStudio 画面の上部にある Tools をクリックし、プルダウンメニューから Global Options ... をクリックします。\nGeneral オプションの Basic タブの設定は次のスクリーンショットのようにします。\n\nWorkspace の自動再現と保存をしない\n最近使用したプロジェクトとファイルをスタートアップ時に開くにチェックを入れる\n\n\n\n\n\n\n\n\n\n\nつぎは、Code オプションの Editing タブの設定をします。\n\nUse native pipe operator にチェックをいれています。\nContinue comment when inserting new line にもチェック入れています。\n\n\n\n\n\n\n\n\n\n\n最後に、Appearance オプションを設定します。 ここでは、RStudio のテーマや使用するフォントの種類・サイズを変えられます。",
    "crumbs": [
      "その前に",
      "R と RStudio について"
    ]
  },
  {
    "objectID": "part00.html#rとの対話",
    "href": "part00.html#rとの対話",
    "title": "R と RStudio について",
    "section": "Rとの対話",
    "text": "Rとの対話\nIDE とディレクトリ構造を設定したので、Rを操作してみましょう。 RStudio で R と対話するには、コンソール を使用するか、スクリプトエディタ (コードを含むプレーンテキストファイル)を使用するかの2つの方法があります。\nコンソール(Console)\nコンソール（一般的には、左下のパネル）は、R がコマンドの実行を指示するのを待つ場所であり、コマンドの結果を表示する場所でもあります。 コンソールに直接コマンドを入力することもできますが、セッションを閉じるとコマンドは消えてしまいます。\n次のコードをコンソールに記述して実行してみてください。\n\n# 円の面積\npi * 5^2\n\n[1] 78.53982",
    "crumbs": [
      "その前に",
      "R と RStudio について"
    ]
  },
  {
    "objectID": "part00.html#スクリプトエディター",
    "href": "part00.html#スクリプトエディター",
    "title": "R と RStudio について",
    "section": "スクリプトエディター",
    "text": "スクリプトエディター\nスクリプトエディターでコマンドを入力し、スクリプトを保存するのが best practiceです。 実行中のコマンドを # を使って自由にコメントすることをお勧めします。 こうすることで、実行したことの完全な記録が残り、他の人に実行方法を簡単に示すことができ、必要であれば後でもう一度実行することができます。\nRstudio スクリプトエディターでは、スクリプトエディターの右上にある Run アイコンをクリックすることで、現在の行または現在ハイライトされているテキストを R コンソールに「送信」することができます。\nそれでは、スクリプトエディタ にコマンドを入力し、コメント文字 # を使って説明を追加し、テキストをハイライトして実行してみましょう。 あるいは、Ctrl キーと Return/Enter キーを同時に押すだけでもショートカットとして実行できる。 コンソールでコマンドが実行され、結果が出力されるはずだ。\n\n# Rの紹介\n# 2023年 9月 25日\n\n# 円の面積\npi * 5^2\n\n[1] 78.53982\n\n\n同じコマンドをコメント記号#なしで実行したらどうなるでしょうか。 前の # 記号を削除してからコマンドを再実行してください。\n\n円の面積\n\nError in eval(expr, envir, enclos): object '円の面積' not found\n\npi * 5^2\n\n[1] 78.53982\n\n\nError: object '円の面積' not found がでましたが、円の面積 というオブジェクトは存在しないと警告しています。",
    "crumbs": [
      "その前に",
      "R と RStudio について"
    ]
  },
  {
    "objectID": "part00.html#コンソールのコマンドプロンプト",
    "href": "part00.html#コンソールのコマンドプロンプト",
    "title": "R と RStudio について",
    "section": "コンソールのコマンドプロンプト",
    "text": "コンソールのコマンドプロンプト\nコマンドプロンプトを解釈することで、R　がいつコマンドを受け付ける準備ができているかを理解することができます。 以下に、コマンドプロンプトのさまざまな状態と、コマンドを終了する方法を示します。\n\nコンソールはコマンドを受け付ける準備ができています： &gt;\n\nコンソールに直接コマンドを入力したり、スクリプトエディタからコマンドを実行したりすると (Ctrl-Enter)、R はそのコマンドを実行しようとします。 実行後、コンソールは結果を表示し、新しいコマンドを待つために新しい &gt; プロンプトを表示します。\n\nコンソールはさらなるデータの入力を待っています： +\n\nRがまだ完了していないため、さらなるデータの入力を待っている場合、 というプロンプトがコンソールに表示されます。 これはプロンプトが表示されます。 多くの場合、これは括弧や引用符を「閉じて」いないことが原因です。\n最後に、RStudio でコマンドが実行されない理由がわからない場合、コンソールウィンドウをクリックして esc を押すと、コマンドをエスケープして新しいプロンプト &gt; を表示することができます。",
    "crumbs": [
      "その前に",
      "R と RStudio について"
    ]
  },
  {
    "objectID": "part00.html#rstudio-でのキーボードショートカット",
    "href": "part00.html#rstudio-でのキーボードショートカット",
    "title": "R と RStudio について",
    "section": "RStudio でのキーボードショートカット",
    "text": "RStudio でのキーボードショートカット\nこのレッスンで説明したショートカットに加えて、RStudio で作業する際に役立つショートカットをいくつか紹介します。\n\n\n\n\n\n\n\nショートカットキー\n動作\n\n\n\n\nCtrl+Enter\nスクリプトエディターからコマンドを実行する\n\n\nESC\nコマンドプロンプトに戻るには、現在のコマンドをエスケープする\n\n\nCtrl+1\nコンソールからスクリプトエディターにカーソルを移動\n\n\nCtrl+2\nスクリプトエディターからコンソールにカーソルを移動\n\n\nTab\nファイルやコマンドのオートコンプリート\n\n\nCtrl+Shift+C\nハイライトされたテキストブロックをコメントにする\n\n\nCtrl+Shift+M\nパイプ演算子を挿入する",
    "crumbs": [
      "その前に",
      "R と RStudio について"
    ]
  },
  {
    "objectID": "part00.html#the-r-syntax",
    "href": "part00.html#the-r-syntax",
    "title": "R と RStudio について",
    "section": "The R syntax",
    "text": "The R syntax\nスクリプト・エディターやコンソールを使って R とのやり取りがわかったところで、R を数字の足し算以上のことに使いたい。 そのためには、R の構文についてもっと知る必要がある。\nR の主な「品詞」（構文）には以下のものがある：\n\nコメント #\n代入演算子 &lt;- や =\nパイプ演算子 %&gt;% または |&gt;\n変数　や 関数\n\n代入演算子\nR で作業したいなら、代入演算子 &lt;- または = を使って 値を変数に代入する必要があります。 例えば、代入演算子を使って x に 3 という値を代入することができます：\nx = 3 \nx &lt;- 3\n*RStudio では、Alt + - (- キーと同時に Alt を押す。Mac では option + - と入力する。) を入力すると、&lt;- を一度に書き込むことができます。\nパイプ演算子\nパイプ演算子は左辺の出力を右辺の関数の第1引数に渡すために使います。 a |&gt; f() または a %&gt;% f() のよう記述しますが、これは f(a) と同じです。\n\na = c(5,1,2,3,1,2) \na |&gt; mean()\n\n[1] 2.333333\n\nmean(a)\n\n[1] 2.333333\n\n\nパイプ演算子の詳細は パイプ演算子の詳細 へ。\n変数\n変数は値を記録するためのものです。 値（情報）を繰り返して代入したり (assignment)、参照したり (reference) できます。\n変数 y に値 5 を代入します。\n\ny = 5\n\n変数名だけ実行すると、変数に代入した値を表示できます。\n\ny\n\n[1] 5\n\n\n変数を参照することによって、記録した値を繰り返し使えます。\n\ny + y\n\n[1] 10\n\n\n変数に値をを代入したら、 RStudio　の Environment パネルで変数の情報を見ることもできます。\n\n\n\n\n\n\n変数名や関数名の作成\n\n\n\n変数名には、x、temperature、subject_id など、自由に名前を決めれます。 しかし、次にルールに従いましょう。\n\n変数名は明確にしたほうがいいが、長すぎないように。\n数値や記号から始める変数名は使わない (2x　よりも　x2)\nR の関数名を避ける（例：if, else, for, mean, data など）\n変数名にはドットを例ないほうがいい。たとえば、my.dataset　よりも、 my_dataset または、myDataset\n変数名は名詞、関数名には動詞を使いましょう\nRは大文字と小文字を区別しています（例えば、genome_length は Genome_lengthとは異なる）\nコードのスタイル（空白を入れる位置、変数名の付け方など）に一貫性を持ちましょう。Rでは、Hadley Wickham のスタイルガイドとGoogleのスタイルガイドがよく使われる。",
    "crumbs": [
      "その前に",
      "R と RStudio について"
    ]
  },
  {
    "objectID": "pca.html#principal-component-analysis-pca",
    "href": "pca.html#principal-component-analysis-pca",
    "title": "Principal Component Analysis",
    "section": "Principal component analysis (PCA)",
    "text": "Principal component analysis (PCA)\nPCA1 is the eigenanalysis2 of the \\(p\\times p\\) variance – covariance matrix3 \\(\\Sigma\\)\n\\[\n\\mathbf{\\Sigma} = \\frac{\\mathbf{Y_c}^\\top \\mathbf{Y_c}}{n-1}\n\\]\n\n\n\\(\\mathbf{Y}_c\\) is the column-centered matrix of the orignal data \\(\\mathbf{Y}\\). \\(n\\) is the number of rows in \\(\\mathbf{Y}_c\\).\n主成分分析固有値解析分散共分散行列",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#definition-of-mathbfy_c",
    "href": "pca.html#definition-of-mathbfy_c",
    "title": "Principal Component Analysis",
    "section": "Definition of \\(\\mathbf{Y_c}\\)",
    "text": "Definition of \\(\\mathbf{Y_c}\\)\n\n\\[\n\\overbrace{\\begin{bmatrix}2.25 & 2.00 & 2.00\\\\-1.75 &  0.00 & -2.00\\\\ 0.25 & -1.00 & -2.00\\\\-0.75 & -1.00 &  2.00\\end{bmatrix}}^{\\mathbf{Y_c}} =\n\\overbrace{\\begin{bmatrix}5 & 4 & 5\\\\1 & 2 & 1\\\\3 & 1 & 1\\\\2 & 1 & 5\\end{bmatrix}}^{\\mathbf{Y}} -\n\\overbrace{\\begin{bmatrix}2.75 & 2.00 & 3.00\\\\2.75 & 2.00 & 3.00\\\\2.75 & 2.00 & 3.00\\\\2.75 & 2.00 & 3.00\\end{bmatrix}}^{\\overline{\\mathbf{Y}}}\n\\]\n\n\\(\\mathbf{Y}\\) is the original data, \\(\\overline{\\mathbf{Y}}\\) is the column-means of \\(\\mathbf{Y}\\).",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#decomposition-of-mathbfy_c",
    "href": "pca.html#decomposition-of-mathbfy_c",
    "title": "Principal Component Analysis",
    "section": "Decomposition of \\(\\mathbf{Y_c}\\)",
    "text": "Decomposition of \\(\\mathbf{Y_c}\\)\nThe eigenanalysis of \\(\\Sigma\\) is defined as the following:\n\\[\n\\frac{\\mathbf{Y_c}^\\top \\mathbf{Y_c}}{n-1}  = \\mathbf{\\Sigma} = \\mathbf{V}_{pca}\\mathbf{L}\\mathbf{V}_{pca}^\\top\n\\]\nwhere, \\(\\mathbf{V}_{pca} = \\begin{bmatrix}\\mathbf{v_1} & \\mathbf{v_2} & \\cdots & \\mathbf{v_p}\\end{bmatrix}\\) is a matrix of \\(p\\) eigenvectors1 and \\(\\mathbf{L}\\) is a diagonal matrix of the eigenvalues2. Each \\(\\mathbf{v_i}\\) is also known as the \\(i\\)th principal component3.\n固有ベクトル固有値主成分",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#eigenvalue",
    "href": "pca.html#eigenvalue",
    "title": "Principal Component Analysis",
    "section": "Eigenvalue",
    "text": "Eigenvalue\nThe eigenvalue matrix is a \\(p\\times p\\) diagonal matrix1.\n\\[\n\\mathbf{L} =\n\\begin{bmatrix}\n\\lambda_1 & 0 & 0 & \\cdots & 0 \\\\\n0 & \\lambda_2  & 0 & \\cdots & 0 \\\\\n\\vdots  &   & \\ddots  & \\   & \\vdots  \\\\\n0 & 0 & 0 & \\cdots & \\lambda_p \\\\\n\\end{bmatrix}\n\\]\n\n\nRecall that \\(p\\) is the number of variables (i.e., columns) in \\(\\mathbf{Y_c}\\)\n対角行列",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#singular-value-decomposition",
    "href": "pca.html#singular-value-decomposition",
    "title": "Principal Component Analysis",
    "section": "Singular value decomposition",
    "text": "Singular value decomposition\nInstead of solving \\(\\frac{\\mathbf{Y_c}^\\top \\mathbf{Y_c}}{n-1} = \\mathbf{\\Sigma} = \\mathbf{V}_{pca}\\mathbf{L}\\mathbf{V}_{pca}^\\top\\) (i.e., eigenanalysis1), it is easier to do a singular value decomposition (SVD)2.\n\\[\n\\mathbf{Y_c} = \\mathbf{U}\\mathbf{D}\\mathbf{V}_{svd}^\\top\n\\]\n固有値解析特異値分解",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#svd",
    "href": "pca.html#svd",
    "title": "Principal Component Analysis",
    "section": "SVD",
    "text": "SVD\n\\[\n\\mathbf{Y_c} = \\mathbf{U}\\mathbf{D}\\mathbf{V}_{svd}^\\top\n\\]\n\\(\\mathbf{U}\\) is the left singular vectors1 of \\(\\mathbf{Y_c}\\) and are the eigenvectors of \\(\\mathbf{Y_c}\\mathbf{Y_c}^\\top\\).\n左特異値ベクトル",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#svd-1",
    "href": "pca.html#svd-1",
    "title": "Principal Component Analysis",
    "section": "SVD",
    "text": "SVD\n\\[\n\\mathbf{Y_c} = \\mathbf{U}\\mathbf{D}\\mathbf{V}_{svd}^\\top\n\\]\n\\(\\mathbf{D}\\) is a diagonal matrix of the singular values1.\n特異値",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#svd-2",
    "href": "pca.html#svd-2",
    "title": "Principal Component Analysis",
    "section": "SVD",
    "text": "SVD\n\\[\n\\mathbf{Y_c} = \\mathbf{U}\\mathbf{D}\\mathbf{V}_{svd}^\\top\n\\]\n\\(\\mathbf{V}_{svd}\\) is the right singular vectors1 of \\(\\mathbf{Y_c}\\) and are the eigenvectors of \\(\\mathbf{Y_c}^\\top\\mathbf{Y_c}\\).\n\n\nNote that \\(\\mathbf{V}_{pca} \\approx \\mathbf{V}_{sda}\\).\n右特異値ベクトル",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#solving-for-mathbfl",
    "href": "pca.html#solving-for-mathbfl",
    "title": "Principal Component Analysis",
    "section": "Solving for \\(\\mathbf{L}\\)",
    "text": "Solving for \\(\\mathbf{L}\\)\nTo find the eigenvalue, we must solve for \\(\\mathbf{\\Sigma}\\).\n\\[\n\\mathbf{\\Sigma} = \\frac{\\mathbf{Y_c}^\\top \\mathbf{Y_c}}{n-1 }\n\\]\n\\[\n\\mathbf{Y_c} = \\mathbf{U}\\mathbf{D}\\mathbf{V}_{svd}^\\top\n\\]",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#substitute-mathbfy_c-in-to-mathbfsigma",
    "href": "pca.html#substitute-mathbfy_c-in-to-mathbfsigma",
    "title": "Principal Component Analysis",
    "section": "Substitute \\(\\mathbf{Y_c}\\) in to \\(\\mathbf{\\Sigma}\\)",
    "text": "Substitute \\(\\mathbf{Y_c}\\) in to \\(\\mathbf{\\Sigma}\\)\n\\[\n\\begin{aligned}\n\\mathbf{\\Sigma} &= \\frac{\\mathbf{Y_c}^\\top \\mathbf{Y_c}}{n-1 }\\\\\n& \\mathbf{Y_c} = \\mathbf{U}\\mathbf{D}\\mathbf{V}_{svd}^\\top \\\\\n\\mathbf{\\Sigma} &= \\frac{(\\mathbf{U}\\mathbf{D}\\mathbf{V}_{svd}^\\top )^\\top (\\mathbf{D}\\mathbf{S}\\mathbf{V}_{svd}^\\top )}{n-1 } \\\\\n\\mathbf{\\Sigma} &= \\frac{\\mathbf{V}_{svd}\\mathbf{D}^\\top\\mathbf{U}^\\top \\mathbf{U} \\mathbf{D} \\mathbf{V}_{svd}^\\top}{n-1 }\\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#substitute-mathbfy_c-in-to-mathbfsigma-1",
    "href": "pca.html#substitute-mathbfy_c-in-to-mathbfsigma-1",
    "title": "Principal Component Analysis",
    "section": "Substitute \\(\\mathbf{Y_c}\\) in to \\(\\mathbf{\\Sigma}\\)",
    "text": "Substitute \\(\\mathbf{Y_c}\\) in to \\(\\mathbf{\\Sigma}\\)\nSince \\(\\mathbf{U}^\\top\\mathbf{U}=1\\),\n\\[\n\\mathbf{\\Sigma} =\n\\frac{\\mathbf{V}_{svd}\\mathbf{D}^\\top \\mathbf{D} \\mathbf{V}_{svd}^\\top}{n-1 } =\n\\mathbf{V}_{pca}\\mathbf{L}\\mathbf{V}_{pca}^\\top\n\\]\nand \\(\\mathbf{V}^\\top\\mathbf{V}=1\\),\n\\[\n\\mathbf{\\Sigma} =\n\\frac{\\mathbf{D}^\\top\\mathbf{D}}{n-1} = \\mathbf{L}\n\\]\nor more simply, \\(\\lambda_i = \\frac{d_i^2}{n-1}\\).",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#visualize-what-we-want-to-do",
    "href": "pca.html#visualize-what-we-want-to-do",
    "title": "Principal Component Analysis",
    "section": "Visualize what we want to do",
    "text": "Visualize what we want to do\n\nThe rotation angle \\(\\theta\\) can also be determined from the rotation vector as follows: \\(\\mathbf{V}_\\text{svd} = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta\\\\ \\sin\\theta & \\cos\\theta\\\\ \\end{bmatrix}\\).",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#computation-mechanics",
    "href": "pca.html#computation-mechanics",
    "title": "Principal Component Analysis",
    "section": "Computation mechanics",
    "text": "Computation mechanics\n\nApply SVD to the previous example results in the following equation.\n\\[\n\\overbrace{\\begin{bmatrix}2.25 & 2.00 & 2.00\\\\-1.75 &  0.00 & -2.00\\\\ 0.25 & -1.00 & -2.00\\\\-0.75 & -1.00 &  2.00\\end{bmatrix}}^{\\mathbf{Y_c}} =\n\\overbrace{\\begin{bmatrix}-0.73 &  0.43 & -0.15\\\\ 0.526 &  0.078 & -0.683\\\\0.39 & 0.33 & 0.70\\\\-0.18 & -0.84 &  0.13\\end{bmatrix} }^{\\mathbf{U}}\\times\n\\overbrace{\\begin{bmatrix}4.66 & 0.00 & 0.00\\\\0.00 & 2.63 & 0.00\\\\0.00 & 0.00 & 1.47\\end{bmatrix} }^{\\mathbf{D}}\\times\n\\overbrace{\\begin{bmatrix}-0.50 & -0.36 & -0.79\\\\ 0.59 &  0.52 & -0.62\\\\ 0.633 & -0.772 & -0.051\\end{bmatrix} }^{\\mathbf{V}^\\top}\n\\]",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-1-determine-mathbfy_c",
    "href": "pca.html#example-1-determine-mathbfy_c",
    "title": "Principal Component Analysis",
    "section": "Example 1: determine \\(\\mathbf{Y_c}\\)",
    "text": "Example 1: determine \\(\\mathbf{Y_c}\\)\nCalculate the centered data matrix.\n\n\nY = iris[1:50, 1:2]\ncolnames(Y) = colnames(iris)[1:2] |&gt; \n  str_remove_all(\"[a-z|\\\\.]+\") \nnm = dim(Y)\nYbar = apply(Y, 2, mean)\nYbar = matrix(Ybar, ncol = nm[2], \n              nrow = nm[1],\n              byrow = TRUE)\nYc = Y - Ybar\nYc |&gt; head()\n\n\n      SL     SW\n1  0.094  0.072\n2 -0.106 -0.428\n3 -0.306 -0.228\n4 -0.406 -0.328\n5 -0.006  0.172\n6  0.394  0.472",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-1-svd",
    "href": "pca.html#example-1-svd",
    "title": "Principal Component Analysis",
    "section": "Example 1: SVD",
    "text": "Example 1: SVD\nCalculate the eigenvectors, eigenvalues, and singular values using svd().\n\nsvdout = svd(Yc)\numat = svdout$u  # Left singular vectors\nvmat = svdout$v  # Right singular vectors (i.e., eigenvectors)\nd = svdout$d     # Singular values\nn = nm[1]        # Data rows\np = nm[2]        # Data columns (i.e., variables)",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-1-eigenvalues",
    "href": "pca.html#example-1-eigenvalues",
    "title": "Principal Component Analysis",
    "section": "Example 1: eigenvalues",
    "text": "Example 1: eigenvalues\nRecall that,\n\\[\n\\lambda_i = \\frac{d_i^2} {n-1}\n\\]\n\n# lambda = ((diag(d) %*% t(diag(d))) / (n - 1)) |&gt; diag()\nlambda = d^2 / (n - 1)\n\nTherefore, the eigenvalues are 0.2337 and 0.0343.",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-1-eigenvectors",
    "href": "pca.html#example-1-eigenvectors",
    "title": "Principal Component Analysis",
    "section": "Example 1: eigenvectors",
    "text": "Example 1: eigenvectors\n\nrownames(vmat) = colnames(Yc)\ncolnames(vmat) = str_glue(\"PC{1:p}\")\n\nThe eigenvectors are:\n\n\nvmat\n\n\n          PC1        PC2\nSL -0.6717496 -0.7407783\nSW -0.7407783  0.6717496",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-1-mechanics-of-rotating-mathbfy_c",
    "href": "pca.html#example-1-mechanics-of-rotating-mathbfy_c",
    "title": "Principal Component Analysis",
    "section": "Example 1: mechanics of rotating \\(\\mathbf{Y_c}\\)",
    "text": "Example 1: mechanics of rotating \\(\\mathbf{Y_c}\\)\n\n\n\n\n\n\n\n\n\n\n\nThe eigenvectors indicate the direction of the vectors that maximizes the variance of each axis. In this case, maximizing the variance in one axis lead to a minimization in the other.",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-1-derived-information",
    "href": "pca.html#example-1-derived-information",
    "title": "Principal Component Analysis",
    "section": "Example 1: derived information",
    "text": "Example 1: derived information\nThe total variance before rotation is:\n\n\ndiag(var(Yc)) |&gt; sum()\n\n\n[1] 0.2679388\n\n\n\nand should be the same as the total inertia (i.e., variance) after rotation:\n\n\nsum(lambda)\n\n\n[1] 0.2679388\n\n\n\nThe variance of SL and SW is:\n\n\ndiag(var(Yc))\n\n\n       SL        SW \n0.1242490 0.1436898",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-1-more-derived-information",
    "href": "pca.html#example-1-more-derived-information",
    "title": "Principal Component Analysis",
    "section": "Example 1: more derived information",
    "text": "Example 1: more derived information\nThe relative importance of SL and SW is:\n\n\ndiag(var(Yc)) / sum(diag(var(Yc)))\n\n\n       SL        SW \n0.4637215 0.5362785 \n\n\n\nHowever, the variance of the principal components and their relative importance is different:\n\n\nlambda # The variance\n\n\n[1] 0.23366074 0.03427804\n\nimportance = lambda / sum(lambda)\nnames(importance) = colnames(vmat)\nimportance # The relative importance\n\n      PC1       PC2 \n0.8720677 0.1279323 \n\n\n\n\n\nThe relative importance indicates the proportion of variation explained by the principal component axis. Conceptually, \\(\\mathbf{Y_c}\\) is rotated until each component is maximized successively.",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-1-scores-and-loadings",
    "href": "pca.html#example-1-scores-and-loadings",
    "title": "Principal Component Analysis",
    "section": "Example 1: scores and loadings",
    "text": "Example 1: scores and loadings\nScores1 are the data points projected onto the principal component axes.\n\n# There are a total of n = 50 scores.\nscores = umat %*% diag(d)\ncolnames(scores) = str_c(\"PC\", 1:ncol(scores))\nscores |&gt; head()\n\n            PC1         PC2\n[1,] -0.1164805 -0.02126719\n[2,]  0.3882586 -0.20898631\n[3,]  0.3744528  0.07351926\n[4,]  0.5157056  0.08042214\n[5,] -0.1233834  0.11998560\n[6,] -0.6143167  0.02519914\n\n\n主成分得点: あるデータ点を主成分ベクトルで表現した場合の値",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-1-scores-and-loadings-1",
    "href": "pca.html#example-1-scores-and-loadings-1",
    "title": "Principal Component Analysis",
    "section": "Example 1: scores and loadings",
    "text": "Example 1: scores and loadings\nLoadings (rescaled loadings)1 indicates the relationship of the variable with the principal component axes.\n\n\nloadings = (vmat %*% diag(d)) / sqrt(n-1)\ncolnames(loadings) = str_glue(\"PC{1:p}\")\nloadings\n\n\n          PC1        PC2\nSL -0.3247134 -0.1371501\nSW -0.3580809  0.1243699\n\n\n\nTherefore, for variable SL, the loadings on PC1 and PC2 are -0.325 and -0.137, respectively. The cosine of the angle between loadings are their correlation.\n\n\nSL = loadings[1, ]\nSW = loadings[2, ]\nL1 = atan2(y = SL[2], x = SL[1])\nL2 = atan2(y = SW[2], x = SW[1])\ntheta = abs(L1 - L2)\ncos(theta)\n\n\n      PC2 \n0.7425467 \n\n\n\n主成分負荷量: ある主成分ベクトルのデータ点に対する重み",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-2-veganrda",
    "href": "pca.html#example-2-veganrda",
    "title": "Principal Component Analysis",
    "section": "Example 2: vegan::rda()",
    "text": "Example 2: vegan::rda()\nFirst run the analysis with both rda() and `svd().\n\n\nrdaout = rda(Yc)\nsvdout = svd(Yc)\n\n\n\n\nLeft singular matrix1\n\n\nsvdout$u |&gt; head(n = 2)     # SDV result\nrdaout$CA$u |&gt; head(n = 2)  # RDA result\n\n\n            [,1]        [,2]\n[1,] -0.03442408 -0.01640983\n[2,]  0.11474404 -0.16125450\n          PC1         PC2\n1 -0.03442408 -0.01640983\n2  0.11474404 -0.16125450\n\n\n\nRight singular matrix2\n\n\nsvdout$v |&gt; head(n = 2)    # SDV result\nrdaout$CA$v |&gt; head(n = 2) # RDA result\n\n\n           [,1]       [,2]\n[1,] -0.6717496 -0.7407783\n[2,] -0.7407783  0.6717496\n          PC1        PC2\nSL -0.6717496 -0.7407783\nSW -0.7407783  0.6717496\n\n\n\n左特異値ベクトル右特異値ベクトル",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-2-veganrda-1",
    "href": "pca.html#example-2-veganrda-1",
    "title": "Principal Component Analysis",
    "section": "Example 2: vegan::rda()",
    "text": "Example 2: vegan::rda()\nEigenvalues\n\n\n(svdout$d^2)/(n-1) # SDV result\nrdaout$CA$eig      # RDA result\n\n\n[1] 0.23366074 0.03427804\n       PC1        PC2 \n0.23366074 0.03427804",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-2-similar-output",
    "href": "pca.html#example-2-similar-output",
    "title": "Principal Component Analysis",
    "section": "Example 2: similar output",
    "text": "Example 2: similar output\n\nrdaout |&gt; summary(display = NA)\n\n\nCall:\nrda(X = Yc) \n\nPartitioning of variance:\n              Inertia Proportion\nTotal          0.2679          1\nUnconstrained  0.2679          1\n\nEigenvalues, and their contribution to the variance \n\nImportance of components:\n                         PC1     PC2\nEigenvalue            0.2337 0.03428\nProportion Explained  0.8721 0.12793\nCumulative Proportion 0.8721 1.00000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  \n\n\n\n\nNote the total inertia is the sum of the eigenvalues 0.268.\nThe proportion explained is the ratio of the eigenvalues to the sum of the eigenvalues \\((\\lambda_i / \\sum{\\lambda_i})\\).\n\n\n\n\nAt this stage numeric results from svd() and rda() are equivalent. However, the scores and loadings are scaled differently.",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-2-vegan-like-analysis",
    "href": "pca.html#example-2-vegan-like-analysis",
    "title": "Principal Component Analysis",
    "section": "Example 2: vegan-like analysis",
    "text": "Example 2: vegan-like analysis\nIn vegan::rda(), the scores are scaled according to the following:\n\nscaling = 1:1 \\(C\\sqrt{\\lambda_i / \\sum\\lambda_i}\\)\nscaling = 2:2 \\(C\\)\n\nThe constant term is \\(C = \\sqrt[4]{(n -1)\\sum{\\lambda_i}}\\).\nSee the decision-vegan.pdf vignette for details.\n\nconst = ((n - 1) * sum(lambda))^{1/4}\nscale1 = sqrt(lambda / sum(lambda) ) * const\nscale2 = const\numat1 = umat %*% diag(scale1)\numat2 = umat * scale2\nvmat1 = vmat %*% diag(scale1)\nvmat2 = vmat * scale2\n\nInterpret spatial patterns.Interpret vector directions and angles.",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-2-scaling-1-and-2",
    "href": "pca.html#example-2-scaling-1-and-2",
    "title": "Principal Component Analysis",
    "section": "Example 2: scaling = 1 and 2",
    "text": "Example 2: scaling = 1 and 2\nScaling 1\n\\[\n\\mathbf{U}_1 =\n\\mathbf{U} \\times\n\\begin{bmatrix}\n\\frac{\\sqrt{\\lambda_1}}{\\sum\\lambda_k} & 0 & 0 & \\cdots & 0\\\\\n0 & \\frac{\\sqrt{\\lambda_2}}{\\sum\\lambda_k} & 0 & \\cdots & 0\\\\\n0 & 0 & \\ddots & \\cdots & 0 \\\\\n0 & 0 & 0 & \\cdots  & \\frac{\\sqrt{\\lambda_k}}{\\sum\\lambda_k} \\\\\n\\end{bmatrix} \\times\n\\sqrt[4]{(n -1)\\sum{\\lambda_k}}\n\\]\nScaling 2\n\\[\n\\mathbf{U}_2 = \\mathbf{U} \\times\n\\sqrt[4]{(n -1)\\sum{\\lambda_k}}\n\\]\n\\(\\mathbf{U}_1\\) and \\(\\mathbf{U}_2\\) are the scaled scores.",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-2-no-scaling",
    "href": "pca.html#example-2-no-scaling",
    "title": "Principal Component Analysis",
    "section": "Example 2: no scaling",
    "text": "Example 2: no scaling\n\n\n\n\n\n\n\n\n\n\n\n\nThe unscaled principal components are in the matrix \\(\\mathbf{U}\\). The black circles indicate the vegan::rda() output and the colored dots indicate the results based on the SVD.\n\n\n\nlimits = c(-0.5, 0.5)\ntmp = scores(rdaout, scaling = \"none\", display = \"species\")\ntmp = tmp %*% diag(1/diag(sqrt(tmp %*% t(tmp)))) / 2\nscores(rdaout, scaling = \"none\", display = \"sites\") |&gt; \n  plot(cex = 1.5, xlim = limits, ylim = limits)\npoints(umat[,1], umat[,2], pch = 19, col = \"blue\", cex = 0.5)\nsegments(c(0,0), c(0,0), tmp[,1], tmp[,2])\ntext(tmp[,1], tmp[,2], labels = rownames(tmp))",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-2-scaling-1",
    "href": "pca.html#example-2-scaling-1",
    "title": "Principal Component Analysis",
    "section": "Example 2: scaling 1",
    "text": "Example 2: scaling 1\n\n\n\n\n\n\n\n\n\n\n\n\nThe scaled principal components are in the matrix \\(\\mathbf{U}_1\\). The black circles indicate the vegan::rda() output and the colored dots indicate the results based on the SVD. Distances between dots indicate approximate euclidean distances \\((d = \\sqrt{x^2+y^2})\\)1.\n\n\n\nlimits = c(-1, 1)\ntmp = scores(rdaout, scaling = 1, display = \"species\")\ntmp = tmp %*% diag(1/diag(sqrt(tmp %*% t(tmp))))\nscores(rdaout, scaling = 1, display = \"sites\") |&gt; \n  plot(cex = 1.5, xlim = limits, ylim = limits)\npoints(umat1[,1], umat1[,2], pch = 19, col = \"blue\", cex = 0.5)\nsegments(c(0,0), c(0,0), tmp[,1], tmp[,2])\ntext(tmp[,1], tmp[,2], labels = rownames(tmp))\n\nユークリッド距離",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-2-scaling-2",
    "href": "pca.html#example-2-scaling-2",
    "title": "Principal Component Analysis",
    "section": "Example 2: scaling 2",
    "text": "Example 2: scaling 2\n\n\n\n\n\n\n\n\n\n\n\n\nThe scaled principal components are in the matrix \\(\\mathbf{U}_2\\). The black circles indicate the vegan::rda() output and the colored dots indicate the results based on the SVD. Angles between vectors (radians) indicate correlation.\n\n\n\nlimits = c(-1.5, 1.5)\ntmp = scores(rdaout, choices = c(1,2), scaling = 2, display = \"species\")\ntmp = tmp %*% diag(1/diag(sqrt(tmp %*% t(tmp)))) \nscores(rdaout, scaling = 2, display = \"sites\") |&gt; \n  plot(cex = 1.5, xlim = limits, ylim = limits)\npoints(umat2[,1], umat2[,2], pch = 19, col = \"blue\", cex = 0.5)\nsegments(c(0,0), c(0,0), tmp[,1], tmp[,2])\ntext(tmp[,1], tmp[,2], labels = rownames(tmp))",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-3-unscaled-3d-plot",
    "href": "pca.html#example-3-unscaled-3d-plot",
    "title": "Principal Component Analysis",
    "section": "Example 3: unscaled 3D plot",
    "text": "Example 3: unscaled 3D plot",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-3-scaled-3d-plot",
    "href": "pca.html#example-3-scaled-3d-plot",
    "title": "Principal Component Analysis",
    "section": "Example 3: scaled 3D plot",
    "text": "Example 3: scaled 3D plot\n\n\n\n\n\n\nScaling = 1\n\n\n\n\n\n\n\nScaling = 2",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-3-summary-output",
    "href": "pca.html#example-3-summary-output",
    "title": "Principal Component Analysis",
    "section": "Example 3: summary output",
    "text": "Example 3: summary output\n\nrdaout |&gt; summary(display = NA)\n\n\nCall:\nrda(X = Yc) \n\nPartitioning of variance:\n              Inertia Proportion\nTotal           3.992          1\nUnconstrained   3.992          1\n\nEigenvalues, and their contribution to the variance \n\nImportance of components:\n                         PC1     PC2     PC3\nEigenvalue            3.6911 0.24138 0.05945\nProportion Explained  0.9246 0.06047 0.01489\nCumulative Proportion 0.9246 0.98511 1.00000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  \n\n\nPC1 explains 92.46 % of the inertia, therefore most of the variation occurs along the PC1 axis.",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-4-correlations-among-vectors",
    "href": "pca.html#example-4-correlations-among-vectors",
    "title": "Principal Component Analysis",
    "section": "Example 4: correlations among vectors",
    "text": "Example 4: correlations among vectors\nThe correlation between two vectors is determined as\n\\[\n\\begin{aligned}\n\\cos\\theta &= \\frac{v_1\\cdot v_2}{\\lVert v_1 \\rVert \\lVert v_2 \\rVert} \\\\\n\\sin\\theta &= \\frac{\\lVert v_1\\times v_2 \\rVert}{\\lVert v_1 \\rVert \\lVert v_2 \\rVert} \\\\\n\\tan\\theta &= \\frac{\\lVert v_1\\times v_2\\rVert}{v_1 \\cdot v_2 } \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-3-problem",
    "href": "pca.html#example-3-problem",
    "title": "Principal Component Analysis",
    "section": "Example 3: problem",
    "text": "Example 3: problem\n\n\nThe correlation between two vectors is the cosine of the angle between the vectors.\n\\[\n\\cos\\theta = \\frac{v_1\\cdot v_2}{\\lVert v_1 \\rVert \\lVert v_2 \\rVert}\n\\]\n\n\n\n\n\n\nDetermine the angle of the shaded region.",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-3-partial-solution",
    "href": "pca.html#example-3-partial-solution",
    "title": "Principal Component Analysis",
    "section": "Example 3: partial solution",
    "text": "Example 3: partial solution\n\\[\n\\cos\\theta = \\frac{v_1\\cdot v_2}{\\lVert v_1 \\rVert \\lVert v_2 \\rVert}\n\\]\n\\[\n\\begin{aligned}\nv_1 &= \\begin{bmatrix}  1.85 & -0.78 & -0.40 \\end{bmatrix} \\\\\nv_2 &= \\begin{bmatrix} -0.43 & -0.90 &  0.40 \\end{bmatrix} \\\\\n\\lVert v_1\\rVert &= \\sqrt{\\begin{bmatrix}  1.85 & -0.78 & -0.40 \\end{bmatrix} \\begin{bmatrix}  1.85 \\\\ -0.78 \\\\ -0.40 \\end{bmatrix}}  = 2.05 \\\\\n\\lVert v_2\\rVert &= \\sqrt{\\begin{bmatrix} -0.43 & -0.90 &  0.40 \\end{bmatrix} \\begin{bmatrix} -0.43 \\\\ -0.90 \\\\  0.40 \\end{bmatrix}}  = 1.08 \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-3-final-solution",
    "href": "pca.html#example-3-final-solution",
    "title": "Principal Component Analysis",
    "section": "Example 3: final solution",
    "text": "Example 3: final solution\n\\[\n\\cos\\theta = \\frac{\\begin{bmatrix}  1.85 & -0.78 & -0.40 \\end{bmatrix} \\begin{bmatrix} -0.43 \\\\ -0.90 \\\\  0.40 \\end{bmatrix}}{2.05 \\times 1.08}\n\\]\n\\[\n\\cos\\theta = \\frac{-0.26}{2.05 \\times1.08} = -0.12\n\\] The correlation between the two vectors is -0.12. The angle between the two vectors is \\(\\cos^{-1}(\\text{-0.1176}) =\\) 1.6886 radians, or 96.7519\\({}^\\circ\\).",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-3-correlations-in-r",
    "href": "pca.html#example-3-correlations-in-r",
    "title": "Principal Component Analysis",
    "section": "Example 3: correlations in R",
    "text": "Example 3: correlations in R\n\ncalc_costheta = function(v1, v2) {\n  v1 = matrix(as.numeric(v1), ncol = 3)\n  v2 = matrix(as.numeric(v2), ncol = 1)\n  bot = sqrt(v1 %*% t(v1)) * sqrt(t(v2)  %*% v2)\n  top = v1 %*% v2\n  costheta = (top/bot)\n  costheta\n}\n\nIsolate the vectors.\n\nsl = vmat[\"SL\", ]\nsw = vmat[\"SW\", ]\npl = vmat[\"PL\", ]\n\nCalculate \\(\\cos\\theta\\).\n\n\ncalc_costheta(sl, sw)\ncalc_costheta(sl, pl)\ncalc_costheta(pl, sw)\n\n\n           [,1]\n[1,] -0.1175698\n          [,1]\n[1,] 0.8717538\n           [,1]\n[1,] -0.4284401\n\n\n\nIt is easier to calculate the correlations on the original centered data.\n\n\ncor(Yc)\n\n\n           SL         SW         PL\nSL  1.0000000 -0.1175698  0.8717538\nSW -0.1175698  1.0000000 -0.4284401\nPL  0.8717538 -0.4284401  1.0000000",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#example-3-variable-pc-correlations",
    "href": "pca.html#example-3-variable-pc-correlations",
    "title": "Principal Component Analysis",
    "section": "Example 3: variable – PC correlations",
    "text": "Example 3: variable – PC correlations\nTo determine the correlations of each variable with respect to the principal component axes, use the function.\n\n\nI = diag(1, 3, 3)\ncc = \\(x) {calc_costheta(sl, I[x, ])}\npcsl = sapply(1:3, cc)\npcsw = sapply(1:3, cc)\npcpl = sapply(1:3, cc)\nmatrix(c(pcsl, pcsw, pcpl), ncol = 3,\n       dimnames = list(str_glue(\"PC{1:3}\"), \n                       c(\"SL\", \"SW\", \"PL\")))\n\n\n            SL         SW         PL\nPC1  0.9044678  0.9044678  0.9044678\nPC2 -0.3792589 -0.3792589 -0.3792589\nPC3 -0.1951939 -0.1951939 -0.1951939\n\n\n\nThe SL and PL vectors are positively correlated with PC1 (0.9045 and 0.9045, respectively). Whereas the SW vector is negatively correlated with PC2 (-0.3793).",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "pca.html#references",
    "href": "pca.html#references",
    "title": "Principal Component Analysis",
    "section": "References",
    "text": "References\nhttps://qiita.com/horiem/items/71380db4b659fb9307b4\nhttps://stats.stackexchange.com/questions/141085/positioning-the-arrows-on-a-pca-biplot\n\n\n\n\n\n\n\n\nHotelling, H. 1933. “Analysis of a Complex of Statistical Variables into Principal Components.” Journal of Educational Psychology 24 (6): 417–41. https://doi.org/10.1037/h0071325.\n\n\nJolliffe, Ian T., and Jorge Cadima. 2016. “Principal Component Analysis: A Review and Recent Developments.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 374 (2065): 20150202. https://doi.org/10.1098/rsta.2015.0202.\n\n\nPearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to Systems of Points in Space.” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11): 559–72. https://doi.org/10.1080/14786440109462720.",
    "crumbs": [
      "Multivariate",
      "主成分分析（PCA）"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "研究室と講義の R コード",
    "section": "",
    "text": "水圏植物生態学研究室 では藻場生態系の機能と保全について研究しています。\nこのサイトは研究室と学部生用の基礎統計学と教育関係共同利用拠点の 水産海洋データ解析演習 のために準備しました。\nマニュアルの更新日：2024年05月15日 09:42:50 JST\n【重要】R コードに自身あるが、日本語は怪しいです。It is easier to explain this stuff in English.\nコードは考えずにコピペしないでください。 コードに使用したデータは研究室のサーバにあるので、公開されていません。\n研究室の皆さん： 研究室用のデータは RStudio の ~/Lab_Data/ に入っています。\nはここ https://cran.r-project.org/ からダウンロードできます。Windows ユーザは Rtools43 もインストールしましょう。ページ内の Rtools43 installer のリンクを探してね。\nRStudio もインストールしましょう。Rのデフォルトの IDE はとても使いにくいです。\n重要: Windows にインストールするとき、OneDrive にインストールされないように注意してください。 詳細は 奥村先生のサイト を参考にしてください。\nSession Information",
    "crumbs": [
      "その前に",
      "研究室と講義の R コード"
    ]
  },
  {
    "objectID": "index.html#この資料について",
    "href": "index.html#この資料について",
    "title": "研究室と講義の R コード",
    "section": "この資料について",
    "text": "この資料について\n\nFira Code プログラミング用等幅フォントを使っています。 このフォントにより、演算子は見やすくなりますが、すこしなれる必要はあります。\n\nたとえば\n\n&lt;- は &lt; と - の合字 (リガチャー, ligature) です (ALT + -)\n|&gt; は | と &gt; の合字です (CTRL + SHIFT + M)\n&lt;= は &lt; と = の合字です\n!= は ! と = の合字です\n\nよく使うリンク\n\nCross Validated\nokumuralab.org\n私たちのR: ベストプラクティスの探究\npsychometrtics_syllabus\nRによる社会調査データ分析の手引き\nbiostatistics\ncucumber flesh (ブログ)",
    "crumbs": [
      "その前に",
      "研究室と講義の R コード"
    ]
  },
  {
    "objectID": "index.html#quick-reference",
    "href": "index.html#quick-reference",
    "title": "研究室と講義の R コード",
    "section": "Quick Reference",
    "text": "Quick Reference\nヘルプ\n\n?mean                          # mean() 関数のヘルプをみる\nhelp.search(\"mean\")            # 文字列で検索\nhelp(package = \"tidyverse\")    # パッケージのヘルプをみる\n\nライブラリー (library)\n\ninstall.packages(\"tidyverse\")   # CRANサーバからパッケージをインストールする\n\n一回インストールすればいいので、install.packages() は毎回する必要はない。\n\nlibrary(tidyverse)              # パッケージをライブラリーから読み込む\nnlstools::preview()             # パッケージ内の関数を直接使う\n\nプログラミング文の基礎\nfor loop\nfor (変数名 in シークエンス) {\n  繰り返し実行するコード\n}\n\nfor(i in 1:10) {\n  j = j + i\n  print(j)\n}\n\nwhile loop\nwhile(条件) {\n  繰り返し実行するコード\n}\n\ni = 0\nwhile(i &lt;= 10) {\n  i = i + 1\n  print(i)\n}\n\nif else\nif(条件) {\n  コード\n} else {\n　条件を満たさないときのコード\n}\n\nx1 = sample(1:6, 1) \nx2 = sample(1:6, 1)\nif(near(x1 + x2, 7)) {\n  print(paste(x1, x2))\n} else {\n  print(x1 + x2)}\n\n条件判定\n\nx == y : x と y は等しい\nx != y : x と y は等しくない\nx &gt; y : x が y より大きい\nx &lt; y : x が y より小さい\nx &gt;= y : x が y 以上\nx &lt;= y : x が y 以下\nis.na(x) : x が欠損値である\nis.null(x) : x が null 値である\nany(x %in% y) : x が y に含まれる\n\nfunction\n関数名 = function(変数1, 変数2, ...) {\n  コード\n}\n\nse = function(x, na.rm = FALSE) {\n  s = sd(x, na.rm = na.rm)\n  n = length(na.omit(x))\n  s / sqrt(n - 1)\n}\n\nファイルの入出力\n研究室では、ファイルの入出力に tidyverse　パッケージの関数を使っています。\n\nlibrary(tidyerse)\n\n\n# ファイルの読み込み\ndset = read_table(\"filename.txt\") # タブ・コンマ区切りのテキストファイル\ndset = read_csv(\"filename.csv\")   # コンマ区切りの csv ファイル\ndset = read_rds(\"filename.rds\")   # R オブジェクトファイル\n\n\n# ファイルの書き込み。文字のエンコーディングは UTF-8 です。\nwrite_tsv(dset, \"filename.txt\")         # タブ区切りの txt ファイル\nwrite_csv(dset, \"filename.csv\")         # コンマ区切りの csv ファイル\nwrite_excel_csv(dset, \"filename.csv\")   # Excel用 コンマ区切りの csv ファイル\nwrite_rds(dset, \"filename.rds\")         # R オブジェクトファイル\n\n資料は自由に使ってください (MIT License)。 サイトは Quarto で作成しました。",
    "crumbs": [
      "その前に",
      "研究室と講義の R コード"
    ]
  },
  {
    "objectID": "rda.html#what-is-rda",
    "href": "rda.html#what-is-rda",
    "title": "Redundancy Analysis",
    "section": "What is RDA",
    "text": "What is RDA\n\n\nRedundancy analysis (RDA)1 a canonical ordination2 method and is one type of asymmetric canonical analysis3 that is used to analyze two data tables simultaneously. In RDA we are interested in how the environmental data \\((\\mathbf{X})\\) influences ordination of the observation data \\((\\mathbf{Y})\\). Recall that PCA is an unconstrained ordination4.\n\n\n\n\n\n\n\n\n\n\n\n\n冗長性分析制約付き序列化非対称直接傾度分析制約なし序列化",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#asymmetric-canonical-analysis",
    "href": "rda.html#asymmetric-canonical-analysis",
    "title": "Redundancy Analysis",
    "section": "Asymmetric canonical analysis",
    "text": "Asymmetric canonical analysis\n\n\nRDA (Redundancy analysis)\n\nEach canonical axis corresponds to a direction that is most related to linear combinations1 of variables in \\(\\mathbf{X}\\).\nTwo ordinations\n\nLinear combinations of \\(\\mathbf{Y}\\).\nLinear combinations of \\(\\mathbf{\\hat{Y}}\\).\n\nPreserves the Euclidean distance2 among objects.\n\n\nCCA (Canonical correspondence analysis)3\n\nSimlar to RDA, except CCA preserves the \\(\\chi^2\\) distance between objects\n\nLDA (Linear discriminant analysis)4\n\n\\(\\mathbf{Y}\\) is divided in to \\(k\\) groups described by a factor.\nSearches for the linear combinations in \\(\\mathbf{X}\\) that can explain the groups by maximizing the dispersion of the centroids of groups.\n\n\n\n線型結合ユークリッド距離正準対応分析線形判別分析",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#approaches-to-analyze-community-data",
    "href": "rda.html#approaches-to-analyze-community-data",
    "title": "Redundancy Analysis",
    "section": "Approaches to analyze community data",
    "text": "Approaches to analyze community data\n\nUse DCA to choose RDA or CCA.\n\n\nIf the length of the first DCA axis &gt; 4 use CCA.\nIf the length of the first DCA axis &gt; 3 use RDA.\nIf the length is between 3 and 4, use either CCA or RDA.\n\n\nIf you transform the \\(\\mathbf{Y}\\),\n\n\nDo not apply DCA.\nChoose RDA for the analysis.\n\n\nUse distance-based RDA.",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#rda-preparation",
    "href": "rda.html#rda-preparation",
    "title": "Redundancy Analysis",
    "section": "RDA preparation",
    "text": "RDA preparation\n\nEach variable in \\(\\mathbf{Y}\\) and \\(\\mathbf{X}\\) should be centered (i.e., have means of zero).\nEach variable in \\(\\mathbf{Y}\\) and \\(\\mathbf{X}\\) should be standardized (i.e., divide by the standard deviation).\nCheck for multi-collinearity among \\(\\mathbf{X}\\).\n\n\n\nIn otherwords, determine the z-score \\((z = (x - \\overline{x}) \\big/ s)\\).",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#rda-algorithm",
    "href": "rda.html#rda-algorithm",
    "title": "Redundancy Analysis",
    "section": "RDA Algorithm",
    "text": "RDA Algorithm\n\nMultiple regression \\(\\mathbf{Y} \\sim \\mathbf{X}\\)\nPCA on the expected value of \\(\\mathbf{Y}\\) determined from the multiple regression (i.e., \\(\\mathbf{\\hat{Y}}\\))\nPCA on the residuals of the multiple regression \\(\\mathbf{Y_{res}} = \\mathbf{Y} - \\mathbf{\\hat{Y}}\\)",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#observation-data-mathbfy",
    "href": "rda.html#observation-data-mathbfy",
    "title": "Redundancy Analysis",
    "section": "Observation data \\((\\mathbf{Y})\\)",
    "text": "Observation data \\((\\mathbf{Y})\\)\nThe presence-absence community matrix1 is a series of zeros and ones to indicate absence and presence, respectively.\nThe dimensions of the matrix (rows and columns).\n\n\nseaweed |&gt; dim() \n\n\n[1] 147 108\n\n\n\nExample of some data points.\n\nseaweed[3:5, 3:5]\n\n# A tibble: 3 × 3\n  `Colpomenia-spp` `Dictyota-spiralis` `Gelidium-spp`\n             &lt;dbl&gt;               &lt;dbl&gt;          &lt;dbl&gt;\n1                0                   1              1\n2                0                   0              1\n3                0                   0              1\n\n\n在不在群集行列",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#hellinger-transform",
    "href": "rda.html#hellinger-transform",
    "title": "Redundancy Analysis",
    "section": "Hellinger transform",
    "text": "Hellinger transform\nIf we are interested in changes of relative species occurrence, then one good transformation is the Hellinger transform. The Hellinger transform is the square root of each element in a row divided by its row-sum.\n\\[\ny\\prime_{ij} = \\sqrt{\\frac{y_{ij}}{\\sum y_{i}}}\n\\]\n\nY = decostand(seaweed, \"hellinger\")\n\n\nY[3:5, 3:5]\n\n  Colpomenia-spp Dictyota-spiralis Gelidium-spp\n3              0         0.2294157    0.2294157\n4              0         0.0000000    0.2182179\n5              0         0.0000000    0.2182179\n\n\n\nsqrt(seaweed[3:5, 3:5] / rowSums(seaweed)[3:5])\n\n  Colpomenia-spp Dictyota-spiralis Gelidium-spp\n1              0         0.2294157    0.2294157\n2              0         0.0000000    0.2182179\n3              0         0.0000000    0.2182179",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#standardize-environmental-data",
    "href": "rda.html#standardize-environmental-data",
    "title": "Redundancy Analysis",
    "section": "Standardize environmental data",
    "text": "Standardize environmental data\nThe environmental variables should be standardized (i.e, find the z-score).\n\\[\nz = \\frac{x - \\bar{x}}{s}\n\\] where \\(x\\) is the value, \\(\\bar{x}\\) is the mean value of \\(x\\), and \\(s\\) is the standard deviation of \\(x\\). This data is already standardized.\n\nenvdata |&gt; print(n = 3)\n\n# A tibble: 147 × 10\n  year  season station month depth_mean pla_10…¹ sed_d…² herbi…³ compe…⁴ temp_…⁵\n  &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 2021  WS     St01    4         -0.904   -1.34   -0.904   1.84   -0.372    1.92\n2 2021  WS     St02    4         -0.254   -1.03   -0.783   0.122  -0.673    1.20\n3 2021  WS     St03    4          1.88    -0.151  -0.213   0.837   1.50    -1.32\n# … with 144 more rows, and abbreviated variable names ¹​pla_100g_day_mean,\n#   ²​sed_day_mean, ³​herbivorous_A_mean, ⁴​competing_A_mean, ⁵​temp_range_mean",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#check-length-of-first-axis",
    "href": "rda.html#check-length-of-first-axis",
    "title": "Redundancy Analysis",
    "section": "Check length of first axis",
    "text": "Check length of first axis\n\n\nIf the length of the first axis is &gt; 4.5, then use CCA not RDA (Leps & Smilauer 2003).\n\n\n&gt; 4 CCA: cca()\n3 ~ 4: rda() or cca()\n&lt; 3 RDA: rda()\n\n\n\n\n\ndecorana(Y)\n\n\n\nCall:\ndecorana(veg = Y) \n\nDetrended correspondence analysis with 26 segments.\nRescaling of axes with 4 iterations.\nTotal inertia (scaled Chi-square): 4.9589 \n\n                       DCA1   DCA2   DCA3   DCA4\nEigenvalues          0.3195 0.2796 0.1848 0.1298\nAdditive Eigenvalues 0.3195 0.2797 0.1825 0.1250\nDecorana values      0.3435 0.2744 0.1757 0.1325\nAxis lengths         3.5600 2.9877 2.8427 2.1297\n\n\n\n\n\n\n\n\n\nNote\n\n\nIf the Hellinger transform is applied, then it is simpler to use rda() and not worry about the dca().",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#null-model",
    "href": "rda.html#null-model",
    "title": "Redundancy Analysis",
    "section": "Null Model",
    "text": "Null Model\nDefine the null model and the full model of the multiple linear regression component.\n\nnullmodel = Y ~ 1\nfullmodel = Y ~ pla_100g_day_mean + \n  temp_range_mean +\n  season + \n  competing_A_mean +\n  herbivorous_A_mean +\n  sed_day_mean +\n  depth_mean",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#forward-selection",
    "href": "rda.html#forward-selection",
    "title": "Redundancy Analysis",
    "section": "Forward selection",
    "text": "Forward selection\nUse forward selection to eliminate unnecessary environmental variables. Removing variables will affect the scores and loadings.\nFit the null model.\n\nrdamodel0 = rda(formula(nullmodel), data = envdata)\n\nFit the full model.\n\nrdamodelf = rda(formula(fullmodel), data = envdata)\n\nRun the forward selection.\n\nrdamodel = ordiR2step(rdamodel0, # null\n                     scope = formula(fullmodel), # full\n                     R2permutations = 2^15,\n                     trace = FALSE) # change to TRUE to see the selection process!",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#results",
    "href": "rda.html#results",
    "title": "Redundancy Analysis",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n(A) RDA model selected by ordiR2step().\n\n\n\n(B) The variance and proportions explained by the constrained and unconstrained parts of the model.\n\n\n\n\n\n\n\n\n(C) The eigenvalues of the constrained components. These are the eigenvalues of the PCA run on the residuals.\n\n\n\n(D) The eigenvalues of the unconstrained components. These are the eigenvalues of the PCA run on the expected values.",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#variance-inflation-factor",
    "href": "rda.html#variance-inflation-factor",
    "title": "Redundancy Analysis",
    "section": "Variance inflation factor",
    "text": "Variance inflation factor\nUse the variance inflation factor (VIF)1 to check for multi-collinearity2. If it is larger than 10, then variable is can be removed from the model.\n\nvif.cca(rdamodel)\n\n pla_100g_day_mean           seasonWS       sed_day_mean herbivorous_A_mean \n          2.064950           1.024439           2.455251           3.070272 \n  competing_A_mean         depth_mean \n          3.936895           3.396944 \n\n\n分散拡大係数多重共線性",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#redundancy-analysis-model",
    "href": "rda.html#redundancy-analysis-model",
    "title": "Redundancy Analysis",
    "section": "Redundancy analysis model",
    "text": "Redundancy analysis model\nSolve for the eigenvalues \\((\\mathbf{\\lambda_k})\\) and eigenvectors \\((\\mathbf{u_k})\\) of the redundancy analysis model.\n\\[\n\\left(\\mathbf{S_{YX}}\\mathbf{S_{XX}^{-1}}\\mathbf{S_{YX}^\\prime} - \\mathbf{\\lambda_k}\\mathbf{I}\\right)\\mathbf{u_k}=0\n\\]\nwhere, \\(\\mathbf{S_{YX}}\\) is the covariance matrix of \\(\\mathbf{Y}\\) and \\(\\mathbf{Y}\\), \\(\\mathbf{S_{XX}^{-1}}\\) is the covariance matrix of $, and \\(\\mathbf{I}\\) is the identity matrix.",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#find-the-solution",
    "href": "rda.html#find-the-solution",
    "title": "Redundancy Analysis",
    "section": "Find the solution",
    "text": "Find the solution\nFirst standardize the Y and X matrices.\n\nY = iris[, 1:2]\nX = iris[, 3:4]\nX = apply(X, 2, scale)\nY = apply(Y, 2, scale)\n\nNext, fit the rda() version for comparison.\n\nrdaout = rda(Y~X)\n\nThen find \\(\\mathbf{\\hat{Y}}\\) multivariate linear regression \\(\\mathbf{Y} \\sim \\mathbf{X}\\).\n\nsxxinv = solve(t(X) %*% X)\nsyx = t(Y) %*% X\nsyxtr = t(syx)\nYhat = X %*% sxxinv %*% syxtr\nYres = Y - Yhat",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#confirm-the-eigenvalues",
    "href": "rda.html#confirm-the-eigenvalues",
    "title": "Redundancy Analysis",
    "section": "Confirm the eigenvalues",
    "text": "Confirm the eigenvalues\nCalculate the eigenvalues for the PCA part.\n\n# PCA part\nsvd(Yres)$d^2 / (nrow(Y) -1)\n\n[1] 0.8965380 0.1240973\n\nrdaout$CA$eig\n\n      PC1       PC2 \n0.8965380 0.1240973 \n\n\nCalculate the eigenvalues for the RDA part.\n\n# RDA part\nsvd(Yhat)$d^2 / (nrow(Y) - 1)\n\n[1] 0.96547259 0.01389207\n\nrdaout$CCA$eig\n\n      RDA1       RDA2 \n0.96547259 0.01389207",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#confirm-the-eigenvectors",
    "href": "rda.html#confirm-the-eigenvectors",
    "title": "Redundancy Analysis",
    "section": "Confirm the eigenvectors",
    "text": "Confirm the eigenvectors\n\nsvd(Yres)$v\n\n          [,1]       [,2]\n[1,] 0.3767512  0.9263145\n[2,] 0.9263145 -0.3767512\n\nrdaout$CA$v\n\n                   PC1        PC2\nSepal.Length 0.3767512  0.9263145\nSepal.Width  0.9263145 -0.3767512\n\n\n\nsvd(Yhat)$v\n\n           [,1]      [,2]\n[1,]  0.8891863 0.4575454\n[2,] -0.4575454 0.8891863\n\nrdaout$CCA$v\n\n                   RDA1      RDA2\nSepal.Length  0.8891863 0.4575454\nSepal.Width  -0.4575454 0.8891863",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#calcualte-inertias",
    "href": "rda.html#calcualte-inertias",
    "title": "Redundancy Analysis",
    "section": "Calcualte inertias",
    "text": "Calcualte inertias\n\nn = nrow(Y)\nconstrained = sum(svd(Yhat)$d^2 / (n - 1))\nunconstrained = sum(svd(Yres)$d^2 / (n - 1))\ntotal = constrained + unconstrained\n\ntibble(type = c(\"Total\", \"Constrained\", \"Unconstrained\"),\n       Inertia = c(total, constrained, unconstrained)) |&gt; \n  mutate(Proportion = Inertia / total)\n\n# A tibble: 3 × 3\n  type          Inertia Proportion\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n1 Total           2.00       1    \n2 Constrained     0.979      0.490\n3 Unconstrained   1.02       0.510\n\n\n\nrdaout\n\nCall: rda(formula = Y ~ X)\n\n              Inertia Proportion Rank\nTotal          2.0000     1.0000     \nConstrained    0.9794     0.4897    2\nUnconstrained  1.0206     0.5103    2\nInertia is variance \n\nEigenvalues for constrained axes:\n  RDA1   RDA2 \n0.9655 0.0139 \n\nEigenvalues for unconstrained axes:\n   PC1    PC2 \n0.8965 0.1241",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#confirm-the-site-scores",
    "href": "rda.html#confirm-the-site-scores",
    "title": "Redundancy Analysis",
    "section": "Confirm the site scores",
    "text": "Confirm the site scores\n\nrdascores = scores(rdaout, scaling = 0, display = \"sites\") |&gt; as_tibble()\neigscores = Y %*% svd(Yhat)$v %*% diag(1/svd(Yhat)$d)  |&gt; as_tibble()\nggplot() + \n  geom_point(aes(x = V1, y = V2, color = \"EIG\"), data = eigscores, size = 3) +\n  geom_point(aes(x = RDA1, y = RDA2, color = \"RDA\"), data = rdascores, size = 1) +\n  scale_color_viridis_d(end = 0.8)",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#confirm-the-environmental-variables",
    "href": "rda.html#confirm-the-environmental-variables",
    "title": "Redundancy Analysis",
    "section": "Confirm the environmental variables",
    "text": "Confirm the environmental variables\n\nvariables = str_remove_all(colnames(Y), \"[a-z|\\\\.]\")\nrdavariables = scores(rdaout, scaling = 0, display = \"bp\") |&gt; as_tibble()\ntemp = t(X) %*% svd(Yhat)$u\nsvdvariables = apply(temp, 1, \\(x) x / sqrt(sum(x^2))) |&gt; t() |&gt; as_tibble()\nrdavariables = rdavariables |&gt; mutate(variables = variables, .before = 1)\nsvdvariables = svdvariables |&gt; mutate(variables = variables, .before = 1)\nfull_join(rdavariables, svdvariables, by = \"variables\")\n\n# A tibble: 2 × 5\n  variables  RDA1  RDA2    V1    V2\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 SL        0.988 0.152 0.988 0.152\n2 SW        0.911 0.413 0.911 0.413",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "rda.html#example-analysis",
    "href": "rda.html#example-analysis",
    "title": "Redundancy Analysis",
    "section": "Example analysis",
    "text": "Example analysis\nExample data set.\nDune meadow vegetation data.\n\nA1 soil thickness\nMoisture ordered factor of levels 1 &lt; 2 &lt; 4 &lt; 5\nManagement factor of 4 levels\nUse ordered factor Hayfield &lt; Haypastu &lt; Pasture\nManure Ordered factor 0 &lt; 1 &lt; 2 &lt; 3 &lt; 4\n\n\n\n   Achimill Agrostol Airaprae Alopgeni Anthodor Bellpere Bromhord Chenalbu\n1         1        0        0        0        0        0        0        0\n2         1        0        0        1        0        1        1        0\n3         0        1        0        1        0        1        0        0\n4         0        1        0        1        0        1        1        0\n5         1        0        0        0        1        1        1        0\n6         1        0        0        0        1        0        0        0\n7         1        0        0        0        1        0        1        0\n8         0        1        0        1        0        0        0        0\n9         0        1        0        1        0        0        0        0\n10        1        0        0        0        1        1        1        0\n11        0        0        0        0        0        0        0        0\n12        0        1        0        1        0        0        0        0\n13        0        1        0        1        0        0        0        1\n14        0        1        0        0        0        0        0        0\n15        0        1        0        0        0        0        0        0\n16        0        1        0        1        0        0        0        0\n17        1        0        1        0        1        0        0        0\n18        0        0        0        0        0        1        0        0\n19        0        0        1        0        1        0        0        0\n20        0        1        0        0        0        0        0        0\n   Cirsarve Comapalu Eleopalu Elymrepe Empenigr Hyporadi Juncarti Juncbufo\n1         0        0        0        1        0        0        0        0\n2         0        0        0        1        0        0        0        0\n3         0        0        0        1        0        0        0        0\n4         1        0        0        1        0        0        0        0\n5         0        0        0        1        0        0        0        0\n6         0        0        0        0        0        0        0        0\n7         0        0        0        0        0        0        0        1\n8         0        0        1        0        0        0        1        0\n9         0        0        0        1        0        0        1        1\n10        0        0        0        0        0        0        0        0\n11        0        0        0        0        0        1        0        0\n12        0        0        0        0        0        0        0        1\n13        0        0        0        0        0        0        0        1\n14        0        1        1        0        0        0        0        0\n15        0        1        1        0        0        0        1        0\n16        0        0        1        0        0        0        1        0\n17        0        0        0        0        0        1        0        0\n18        0        0        0        0        0        0        0        0\n19        0        0        0        0        1        1        0        0\n20        0        0        1        0        0        0        1        0\n   Lolipere Planlanc Poaprat Poatriv Ranuflam Rumeacet Sagiproc Salirepe\n1         1        0       1       1        0        0        0        0\n2         1        0       1       1        0        0        0        0\n3         1        0       1       1        0        0        0        0\n4         1        0       1       1        0        0        1        0\n5         1        1       1       1        0        1        0        0\n6         1        1       1       1        0        1        0        0\n7         1        1       1       1        0        1        0        0\n8         1        0       1       1        1        0        1        0\n9         1        0       1       1        0        1        1        0\n10        1        1       1       1        0        0        0        0\n11        1        1       1       0        0        0        1        0\n12        0        0       0       1        0        1        1        0\n13        0        0       1       1        1        0        1        0\n14        0        0       0       0        1        0        0        0\n15        0        0       0       0        1        0        0        0\n16        0        0       0       1        1        0        0        0\n17        0        1       1       0        0        0        0        0\n18        1        1       1       0        0        0        0        1\n19        0        0       0       0        0        0        1        1\n20        0        0       0       0        1        0        0        1\n   Scorautu Trifprat Trifrepe Vicilath Bracruta Callcusp\n1         0        0        0        0        0        0\n2         1        0        1        0        0        0\n3         1        0        1        0        1        0\n4         1        0        1        0        1        0\n5         1        1        1        0        1        0\n6         1        1        1        0        1        0\n7         1        1        1        0        1        0\n8         1        0        1        0        1        0\n9         1        0        1        0        1        0\n10        1        0        1        1        1        0\n11        1        0        1        1        1        0\n12        1        0        1        0        1        0\n13        1        0        1        0        0        0\n14        1        0        1        0        0        1\n15        1        0        1        0        1        0\n16        0        0        0        0        1        1\n17        1        0        0        0        0        0\n18        1        0        1        1        1        0\n19        1        0        1        0        1        0\n20        1        0        0        0        1        1\n\n\n\n\n     A1 Moisture Management      Use Manure\n1   2.8        1         SF Haypastu      4\n2   3.5        1         BF Haypastu      2\n3   4.3        2         SF Haypastu      4\n4   4.2        2         SF Haypastu      4\n5   6.3        1         HF Hayfield      2\n6   4.3        1         HF Haypastu      2\n7   2.8        1         HF  Pasture      3\n8   4.2        5         HF  Pasture      3\n9   3.7        4         HF Hayfield      1\n10  3.3        2         BF Hayfield      1\n11  3.5        1         BF  Pasture      1\n12  5.8        4         SF Haypastu      2\n13  6.0        5         SF Haypastu      3\n14  9.3        5         NM  Pasture      0\n15 11.5        5         NM Haypastu      0\n16  5.7        5         SF  Pasture      3\n17  4.0        2         NM Hayfield      0\n18  4.6        1         NM Hayfield      0\n19  3.7        5         NM Hayfield      0\n20  3.5        5         NM Hayfield      0\n\n\n\n\n# A tibble: 20 × 5\n      A1 Moisture Management Use      Manure\n   &lt;dbl&gt; &lt;ord&gt;    &lt;fct&gt;      &lt;ord&gt;    &lt;ord&gt; \n 1   2.8 1        SF         Haypastu 4     \n 2   3.5 1        BF         Haypastu 2     \n 3   4.3 2        SF         Haypastu 4     \n 4   4.2 2        SF         Haypastu 4     \n 5   6.3 1        HF         Hayfield 2     \n 6   4.3 1        HF         Haypastu 2     \n 7   2.8 1        HF         Pasture  3     \n 8   4.2 5        HF         Pasture  3     \n 9   3.7 4        HF         Hayfield 1     \n10   3.3 2        BF         Hayfield 1     \n11   3.5 1        BF         Pasture  1     \n12   5.8 4        SF         Haypastu 2     \n13   6   5        SF         Haypastu 3     \n14   9.3 5        NM         Pasture  0     \n15  11.5 5        NM         Haypastu 0     \n16   5.7 5        SF         Pasture  3     \n17   4   2        NM         Hayfield 0     \n18   4.6 1        NM         Hayfield 0     \n19   3.7 5        NM         Hayfield 0     \n20   3.5 5        NM         Hayfield 0     \n\n\n\n\n\nCall:\nrda(formula = dune ~ A1 + Moisture + Use + Manure, data = dune.env) \n\nPartitioning of variance:\n              Inertia Proportion\nTotal           5.261     1.0000\nConstrained     3.384     0.6432\nUnconstrained   1.877     0.3568\n\nEigenvalues, and their contribution to the variance \n\nImportance of components:\n                        RDA1   RDA2    RDA3    RDA4    RDA5    RDA6    RDA7\nEigenvalue            1.4129 0.7859 0.28559 0.24176 0.17851 0.15076 0.12636\nProportion Explained  0.2686 0.1494 0.05429 0.04596 0.03393 0.02866 0.02402\nCumulative Proportion 0.2686 0.4180 0.47226 0.51822 0.55215 0.58081 0.60483\n                        RDA8    RDA9    RDA10     PC1     PC2     PC3     PC4\nEigenvalue            0.1021 0.06278 0.037263 0.43922 0.43799 0.29683 0.22855\nProportion Explained  0.0194 0.01193 0.007083 0.08349 0.08326 0.05643 0.04345\nCumulative Proportion 0.6242 0.63616 0.643248 0.72674 0.81000 0.86643 0.90988\n                          PC5     PC6     PC7     PC8      PC9\nEigenvalue            0.16015 0.12726 0.09615 0.06368 0.026875\nProportion Explained  0.03044 0.02419 0.01828 0.01210 0.005109\nCumulative Proportion 0.94032 0.96451 0.98279 0.99489 1.000000\n\nAccumulated constrained eigenvalues\nImportance of components:\n                        RDA1   RDA2   RDA3    RDA4    RDA5    RDA6    RDA7\nEigenvalue            1.4129 0.7859 0.2856 0.24176 0.17851 0.15076 0.12636\nProportion Explained  0.4175 0.2322 0.0844 0.07144 0.05275 0.04455 0.03734\nCumulative Proportion 0.4175 0.6498 0.7342 0.80563 0.85838 0.90293 0.94027\n                         RDA8    RDA9   RDA10\nEigenvalue            0.10206 0.06278 0.03726\nProportion Explained  0.03016 0.01855 0.01101\nCumulative Proportion 0.97043 0.98899 1.00000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  \n\n\nPermutation test for rda under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 999\n\nModel: rda(formula = dune ~ A1 + Moisture + Use + Manure, data = dune.env)\n         Df Variance      F Pr(&gt;F)   \nA1        1  0.54865 2.6311  0.006 **\nMoisture  3  1.44897 2.3162  0.002 **\nUse       2  0.50253 1.2050  0.257   \nManure    4  0.88368 1.0595  0.406   \nResidual  9  1.87670                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPermutation test for rda under reduced model\nForward tests for axes\nPermutation: free\nNumber of permutations: 999\n\nModel: rda(formula = dune ~ A1 + Moisture + Use + Manure, data = dune.env)\n         Df Variance      F Pr(&gt;F)   \nRDA1      1  1.41288 6.7757  0.002 **\nRDA2      1  0.78586 3.7687  0.117   \nRDA3      1  0.28559 1.3696  0.996   \nRDA4      1  0.24176 1.1594  1.000   \nRDA5      1  0.17851 0.8561  0.999   \nRDA6      1  0.15076 0.7230  1.000   \nRDA7      1  0.12636 0.6060  1.000   \nRDA8      1  0.10206 0.4894  1.000   \nRDA9      1  0.06278 0.3011  1.000   \nRDA10     1  0.03726 0.1787  0.999   \nResidual  9  1.87670                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Multivariate",
      "冗長性分析（RDA）"
    ]
  },
  {
    "objectID": "part01.html",
    "href": "part01.html",
    "title": "R の基本操作",
    "section": "",
    "text": "代入は = か &lt;- (ALT+-) です。伝統的に使われる代入は &lt;- ですが、私は = を使っています。\n左辺は変数名、右辺は値です。\n\n\n# 二種類の代入と c() 関数\na = 4.2\nb &lt;- 5.0\nc(a, b)\n\n[1] 4.2 5.0\n\n\n\nc() は渡された引数を結合します。\n# の後から続く文字列はコードとして実行されません。実行されない文書はコメントと呼びます。\n\n\n(a + b) * c(a, b)\n\n[1] 38.64 46.00\n\n\n\nR はベクトル処理という実行機構が特徴的です。\n上のコードは (a + b) \\times a と (a + b) \\times b を求めています。\n\nRStudio の場合 &lt;- は ALT + - のショートカットを定義しています。",
    "crumbs": [
      "Rの基礎",
      "R の基本操作"
    ]
  },
  {
    "objectID": "part01.html#代入演算子-assignment-operator-とベクトル-vector-とは",
    "href": "part01.html#代入演算子-assignment-operator-とベクトル-vector-とは",
    "title": "R の基本操作",
    "section": "",
    "text": "代入は = か &lt;- (ALT+-) です。伝統的に使われる代入は &lt;- ですが、私は = を使っています。\n左辺は変数名、右辺は値です。\n\n\n# 二種類の代入と c() 関数\na = 4.2\nb &lt;- 5.0\nc(a, b)\n\n[1] 4.2 5.0\n\n\n\nc() は渡された引数を結合します。\n# の後から続く文字列はコードとして実行されません。実行されない文書はコメントと呼びます。\n\n\n(a + b) * c(a, b)\n\n[1] 38.64 46.00\n\n\n\nR はベクトル処理という実行機構が特徴的です。\n上のコードは (a + b) \\times a と (a + b) \\times b を求めています。\n\nRStudio の場合 &lt;- は ALT + - のショートカットを定義しています。",
    "crumbs": [
      "Rの基礎",
      "R の基本操作"
    ]
  },
  {
    "objectID": "part01.html#r-の主なデータタイプとデータ構造",
    "href": "part01.html#r-の主なデータタイプとデータ構造",
    "title": "R の基本操作",
    "section": "R の主なデータタイプとデータ構造",
    "text": "R の主なデータタイプとデータ構造\n\n整数 integer\n実数 double, numeric\n複素数 complex number\n時系列 time-series (POSIX)\n文字列 character\n論理値 logical\n因子 factor\nベクトル　vector\n配列 array, matrix\nリスト list\nテーブル（データフレーム） dataframe",
    "crumbs": [
      "Rの基礎",
      "R の基本操作"
    ]
  },
  {
    "objectID": "part01.html#データの作り方",
    "href": "part01.html#データの作り方",
    "title": "R の基本操作",
    "section": "データの作り方",
    "text": "データの作り方\nベクトル\n\na = c(10.3, 20.2, 30.1)\nb = c(\"rabbit\", \"cat\", \"mouse\", \"dog\")\nd = c(TRUE, FALSE, T)\ne = factor(c(\"nagasaki\", \"kagoshima\", \"fukuoka\"))\n\nリスト\nベクトルの長さは異なってもいい。 ここでは、リストの要素名を指定しました。\n\nz1 = list(\"A\" = a, \"B\"= b, \"D\" = d, \"E\" = e)\n\nデータフレーム\nベクトルの長さを揃える必要がある。 ここではb[1:3]をbに渡すことで、変数名を指定しました。\n\nz2 = data.frame(a, b = b[1:3], d, e)",
    "crumbs": [
      "Rの基礎",
      "R の基本操作"
    ]
  },
  {
    "objectID": "part01.html#リストの構造を確認しよう",
    "href": "part01.html#リストの構造を確認しよう",
    "title": "R の基本操作",
    "section": "リストの構造を確認しよう",
    "text": "リストの構造を確認しよう\nRオブジェクトの構造 (structure) は str() で確認します。\n\nstr(z1)\n\nList of 4\n $ A: num [1:3] 10.3 20.2 30.1\n $ B: chr [1:4] \"rabbit\" \"cat\" \"mouse\" \"dog\"\n $ D: logi [1:3] TRUE FALSE TRUE\n $ E: Factor w/ 3 levels \"fukuoka\",\"kagoshima\",..: 3 2 1",
    "crumbs": [
      "Rの基礎",
      "R の基本操作"
    ]
  },
  {
    "objectID": "part01.html#リストからデータを抽出する",
    "href": "part01.html#リストからデータを抽出する",
    "title": "R の基本操作",
    "section": "リストからデータを抽出する",
    "text": "リストからデータを抽出する\nリストの要素は次のように抽出できます。\n\nz1$A\n\n[1] 10.3 20.2 30.1\n\n\nリストからの抽出方法は $ 以外に, [ や [[ でもできます。\n\nz1[c(\"A\", \"D\")]\nz1[c(1,4)]\nz1[[2]]\nz1[[c(2,3)]]\nz1[[2]][c(1,2)]",
    "crumbs": [
      "Rの基礎",
      "R の基本操作"
    ]
  },
  {
    "objectID": "part01.html#データフレームの構造を確認しよう",
    "href": "part01.html#データフレームの構造を確認しよう",
    "title": "R の基本操作",
    "section": "データフレームの構造を確認しよう",
    "text": "データフレームの構造を確認しよう\n\nstr(z2)\n\n'data.frame':   3 obs. of  4 variables:\n $ a: num  10.3 20.2 30.1\n $ b: chr  \"rabbit\" \"cat\" \"mouse\"\n $ d: logi  TRUE FALSE TRUE\n $ e: Factor w/ 3 levels \"fukuoka\",\"kagoshima\",..: 3 2 1\n\n\nリストと似ていますが、そもそもデータフレームはリストです。 つまり、リストと同じように操作できます。\n\nz2$a\n\n[1] 10.3 20.2 30.1\n\n\n\nz2[c(\"a\", \"d\")]\nz2[c(1, 4)]\nz2[[2]]\nz2[[c(2,3)]]\nz2[[2]][c(1,2)]",
    "crumbs": [
      "Rの基礎",
      "R の基本操作"
    ]
  },
  {
    "objectID": "part01.html#比較演算",
    "href": "part01.html#比較演算",
    "title": "R の基本操作",
    "section": "比較演算",
    "text": "比較演算\n\n比較に使う論理演算子：&（論理積 AND）, | （論理和 OR）, !（否定 NOT）\n\n\nA = c(5, 3, 2)\nB = c(5, 2, 1)\n\n# 論理積\n(A[1] &gt; B[1]) & (A[1] == B[1])\n\n[1] FALSE\n\n# 論理和\n(A[1] &gt; B[1]) | (A[1] == B[1])\n\n[1] TRUE\n\n\n\n# 否定と論理積\n!(A[1] &gt; B[1]) & (A[1] == B[1])\n\n[1] TRUE\n\n# 否定と論理和\n(A[1] &lt; B[2]) | !(A[1] == B[1])\n\n[1] FALSE",
    "crumbs": [
      "Rの基礎",
      "R の基本操作"
    ]
  },
  {
    "objectID": "part01.html#比較演算を使ったデータの抽出",
    "href": "part01.html#比較演算を使ったデータの抽出",
    "title": "R の基本操作",
    "section": "比較演算を使ったデータの抽出",
    "text": "比較演算を使ったデータの抽出\n\na = c(10.3, 20.2, 30.1)\nb = c(\"rabbit\", \"cat\", \"mouse\")\nd = c(TRUE, FALSE, T)\ne = factor(c(\"nagasaki\", \n             \"kagoshima\", \n             \"fukuoka\"))\nZ = data.frame(a, b, d, e)\nZ[Z$a &gt; 20, ]\n\n     a     b     d         e\n2 20.2   cat FALSE kagoshima\n3 30.1 mouse  TRUE   fukuoka\n\n\n\nZ[Z$a &gt; 10 & Z$a &lt; 20.2, ]\n\n     a      b    d        e\n1 10.3 rabbit TRUE nagasaki\n\nZ[Z$a &gt; 10 & Z$a &lt;= 20.2, ]\n\n     a      b     d         e\n1 10.3 rabbit  TRUE  nagasaki\n2 20.2    cat FALSE kagoshima\n\nZ[identical(Z$a, 20) | !Z$d, ]\n\n     a   b     d         e\n2 20.2 cat FALSE kagoshima\n\n\n重要! 数値の比較について\nパソコンは 2 進数で計算しているので、数値は正確ではない！\nたとえば：\n\n0.2 * 0.2 / 0.2 == 0.2 # = と =\n\n[1] FALSE\n\n\n数値の比較をする場合は all.equal() を使いましょう。\n\n# 上のコードと同じ\nall.equal(0.2 * 0.2 / 0.2, 0.2, tolerance = 0)\n\n[1] \"Mean relative difference: 1.387779e-16\"\n\n# 機械誤差を考慮した比較\nall.equal(0.2 * 0.2 / 0.2, 0.2, tolerance = .Machine$double.eps)\n\n[1] TRUE\n\n\n\nちなみに比較用記号は &lt;, &gt;, &gt;= (&gt; と =), &lt;= (&lt; と =), != (! と =), == (= と =)　です。",
    "crumbs": [
      "Rの基礎",
      "R の基本操作"
    ]
  },
  {
    "objectID": "part03.html",
    "href": "part03.html",
    "title": "Tidyverse",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part03.html#tidyverse-1",
    "href": "part03.html#tidyverse-1",
    "title": "Tidyverse",
    "section": "tidyverse",
    "text": "tidyverse\ntidyverse はメタパッケージなので、library(tidyverse) を実行すると次の 8 つのパッケージが読み込まれます。\n\ndplyr：データの変形・加工\nforcats：factor() 因子が使いやすくなります\nggplot2：データの可視化・作図\npurrr ：関数型プログラミング\nreadr ：CSV、TSVデータの読み込み\nstringr ：文字列の操作が楽になる\ntibble ：データフレームの操作が楽になる\ntidyr ：データをタイディ (tidy) にして操作しやすくなる\n\ntidyverse の概念をもっと知りたい方は tidyverse のマニフェスト を読みましょう。",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part03.html#tibble-には-list-列を入れられる",
    "href": "part03.html#tibble-には-list-列を入れられる",
    "title": "Tidyverse",
    "section": "tibble には list 列を入れられる",
    "text": "tibble には list 列を入れられる\nちょっと高度の方法ですが, list を変数の要素として記録できます。\n\na1 = list(1,5,1,3,5,1)\na2 = list(2,3,5,2)\na3 = list(\"A\",\"b\",\"E\")\ntibble(a = 1:3, values = list(a1, a2, a3))\n\n# A tibble: 3 × 2\n      a values    \n  &lt;int&gt; &lt;list&gt;    \n1     1 &lt;list [6]&gt;\n2     2 &lt;list [4]&gt;\n3     3 &lt;list [3]&gt;\n\n\nvalues 列は list の list ですね。",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part03.html#列名変数名について",
    "href": "part03.html#列名変数名について",
    "title": "Tidyverse",
    "section": "列名・変数名について",
    "text": "列名・変数名について\ndata.frame() は無効な変数名を自動的に変更します。\n\ndata.frame(`1 name` = 1) |&gt; names()\n\n[1] \"X1.name\"\n\n\ntibble() はそのままにしてくれます。\n\ntibble(`1 name` = 1) |&gt; names()\n\n[1] \"1 name\"\n\n\ndata.frame()の場合 * 有効な変数名：文字または、ドット(.)と文字から始まる文字列。変数名に使用できるものは文字、数字、ドットとアンダースコア (_) だけです。 * 無効な変数名の例：2021FY, 2021 FY, 2020-FY, FY-2021 は自動的に X2021FY, X2021.FY, X2020.FY, FY.2021 に変更されます。\ntibble()の場合 * 変数名はそのまま使えますが、使うときは ` ` （バクチック）に囲んでください。\nところが! どうしても data.frame() に無効な変数名を使いたいのであれば、check.names = F を渡してください。\n\ndata.frame(`1 name` = 1, check.names = FALSE) |&gt; names()\n\n[1] \"1 name\"",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part03.html#引数を連続的に使える",
    "href": "part03.html#引数を連続的に使える",
    "title": "Tidyverse",
    "section": "引数を連続的に使える",
    "text": "引数を連続的に使える\ntibble()はこのように, 計算処理をしながらデータフレームを構築できます。\n\ntibble(x = 1:4, `x^2` = x^2, `sqrt(x)` = sqrt(x))\n\n# A tibble: 4 × 3\n      x `x^2` `sqrt(x)`\n  &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1     1     1      1   \n2     2     4      1.41\n3     3     9      1.73\n4     4    16      2",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part03.html#ベクトルをリサイクルしない",
    "href": "part03.html#ベクトルをリサイクルしない",
    "title": "Tidyverse",
    "section": "ベクトルをリサイクルしない",
    "text": "ベクトルをリサイクルしない\n二つのベクトルの長さが異なるときに, データフレームを作ると, 小さいほうのベクトルは先頭から繰り返して使われます。ただし長いベクトルの要素数は短いベクトルの要素数で除算できる必要があります。\n\nx = 1:4\ny = 1:8\ndata.frame(x, y)\n\n  x y\n1 1 1\n2 2 2\n3 3 3\n4 4 4\n5 1 5\n6 2 6\n7 3 7\n8 4 8\n\n\nところが, この機能はデータ解析時にバグの原因になります。tibble()はベクトルのリサイクルはできません。\n\nx = 1:4\ny = 1:8\ntibble(x, y)\n\nError:\n! Tibble columns must have compatible sizes.\n• Size 4: Existing data.\n• Size 8: Column at position 2.\nℹ Only values of size one are recycled.",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part03.html#io-関係の関数",
    "href": "part03.html#io-関係の関数",
    "title": "Tidyverse",
    "section": "I/O 関係の関数",
    "text": "I/O 関係の関数\n読み込み関数\n\nread_delim()：一般性の高い関数, 区切りの指定が必要\nread_csv()：コンマ区切りフィアルの読み込み（csv ファイル）\nread_table()：ホワイトスペース区切りファイルの読み込み（タブ・スペース区切りファイル）\nread_rds()：R オブジェクトの読み込み\n\n書き出し関数\n\nwrite_delim()：一般性の高い関数, 区切りの指定が必要\nwrite_csv()：コンマ区切りフィアルの書き出し\nwrite_excel_csv()：Excel 用にコンマ区切りフィアルを書き出す\nwrite_table()：ホワイトスペース区切りファイルの書き出し（タブ・スペース区切りファイル）\nwrite_rds()：R オブジェクトの書き出し\nggsave(): ggplot2 でつくった図を書き出し",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part03.html#read_csv-の重要な引数",
    "href": "part03.html#read_csv-の重要な引数",
    "title": "Tidyverse",
    "section": "read_csv() の重要な引数",
    "text": "read_csv() の重要な引数\n\nfile：パスとファイル名\ncol_names = TRUE：TRUEのとき, 1行目は列名として使う, FALSE のときは列名を自動的に作成する, 文字列ベクトルを渡せば読み込み中に列名を付けられます\ncol_types = NULL：列のデータ型を指定できるが NULL のときは関数に任せる\ncomment = \"\"：コメント記号を指定し, コメント記号後の文字を無視する\nskip = 0 先頭から無視する行数\nlocale：ロケール（地域の設定）\nn_max = Inf：読み込む行数、デフォルトは全ての行数",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part03.html#read_csvの使い方",
    "href": "part03.html#read_csvの使い方",
    "title": "Tidyverse",
    "section": "read_csv()の使い方",
    "text": "read_csv()の使い方\nread_csv()\n\nrabbits = read_csv(\"Assignment_06_Dataset01.csv\")\nrabbits\n\n\n\nRows: 37 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): host\ndbl (1): scutum.width\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 37 × 2\n   host    scutum.width\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 Rabbit1          380\n 2 Rabbit1          376\n 3 Rabbit1          360\n 4 Rabbit1          368\n 5 Rabbit1          372\n 6 Rabbit1          366\n 7 Rabbit1          374\n 8 Rabbit1          382\n 9 Rabbit2          350\n10 Rabbit2          356\n# … with 27 more rows",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part03.html#readxl-パッケージ",
    "href": "part03.html#readxl-パッケージ",
    "title": "Tidyverse",
    "section": "readxl パッケージ",
    "text": "readxl パッケージ\nreadxl は Microsoft Excelファイルの読み込みに使えるパッケージです。\n\nlibrary(readxl)\n\nファイルの読み込みには read_excel() を使いますが、研究室では read_xlsx() もよく使います。 read_excel() は read_xlsx() のラッパーです。 使い方は全くおなじです。\n重要： エクセルでデータの管理をした場合エクセルのオートコレクト機能によってデータがかってに変換されるので気をつけましょう。遺伝子の名前のオートコレクトによく問題が発生すると報告されています。とくに Excel と Google Sheets のオートコレクトはアグレッシブです。Abeysooriya et al. 2021. PLOS Computational Biology。",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part03.html#read_excel-の主な引数",
    "href": "part03.html#read_excel-の主な引数",
    "title": "Tidyverse",
    "section": "read_excel() の主な引数",
    "text": "read_excel() の主な引数\n\npath：パスとファイル名\nsheet = NULL：読み込むシート名またはシートインデックス\nrange = NULL：読み込む範囲, 例えば “B3:D8” または, “Data!B3:D8”\ncol_names = TRUE：1行目を列名として使う論理値\ncol_types = NULL：読み込む列のデータ型を指定できます (デフォルトは guess)\nna = \"\"：欠損値の定義, 空セルは欠損値とされます\nskip = 0：無視する行数\nn_max = Inf：読み込む最大行数",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part03.html#read_excel-の使用例１",
    "href": "part03.html#read_excel-の使用例１",
    "title": "Tidyverse",
    "section": "read_excel() の使用例（１）",
    "text": "read_excel() の使用例（１）\n最初のシート (sheet = 1) の先頭から1行無視して (skip = 1) データを読み込む。\n\nfilename = \"Table 2.xlsx\"\nexceldata = read_excel(filename, sheet = 1, skip = 1)\nexceldata\n\n\n\nNew names:\n• `WT (˚C)` -&gt; `WT (˚C)...2`\n• `S.D.**` -&gt; `S.D.**...3`\n• `` -&gt; `...4`\n• `WT (˚C)` -&gt; `WT (˚C)...5`\n• `S.D.**` -&gt; `S.D.**...6`\n\n\n# A tibble: 12 × 6\n   Month `WT (˚C)...2` `S.D.**...3` ...4  `WT (˚C)...5` `S.D.**...6`\n   &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;lgl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n 1 Jan.           21.1        0.446 NA             16.5        0.428\n 2 Feb.           21.3        0.441 NA             16.3        0.483\n 3 Mar.           21.5        0.470 NA             16.5        0.579\n 4 Apr.           21.8        0.554 NA             18.3        1.27 \n 5 May            23.4        0.726 NA             21.1        1.08 \n 6 Jun.           25.5        1.20  NA             22.9        1.02 \n 7 Jul.           28.6        0.491 NA             26.6        1.15 \n 8 Aug.           28.8        0.546 NA             28.5        0.470\n 9 Sep.           28.5        0.375 NA             27.7        0.794\n10 Oct.           26.6        0.893 NA             24.4        1.05 \n11 Nov.           24.7        0.516 NA             21.6        0.928\n12 Dec.           22.8        0.720 NA             19.1        0.893",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part03.html#read_excel-の使用例２",
    "href": "part03.html#read_excel-の使用例２",
    "title": "Tidyverse",
    "section": "read_excel() の使用例（２）",
    "text": "read_excel() の使用例（２）\n先程のように読み込むと、不都合な変数名に変換されました。次は、変数名も指定して読み込みます。\n\nfilename = \"Table 2.xlsx\"\ncol_names = c(\"month\", \"temperature1\", \"sd1\", \"empty\",\"temperature2\", \"sd2\")\nexceldata = read_excel(filename, sheet = 1, skip = 2, col_name = col_names)\nexceldata |&gt; print(n = 4)\n\n\n\n# A tibble: 12 × 6\n  month temperature1   sd1 empty temperature2   sd2\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 Jan.          21.1 0.446 NA            16.5 0.428\n2 Feb.          21.3 0.441 NA            16.3 0.483\n3 Mar.          21.5 0.470 NA            16.5 0.579\n4 Apr.          21.8 0.554 NA            18.3 1.27 \n# … with 8 more rows\n\n\nシートの２行目には変数名が記録されているので、skip = 2 を渡しました。",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part03.html#データの出力",
    "href": "part03.html#データの出力",
    "title": "Tidyverse",
    "section": "データの出力",
    "text": "データの出力\nCSVファイルの出力\n\nfname = \"table2_output.csv\"\nexceldata |&gt; write_csv(file = fname) # 文字コードは UTF-8 です。\n\nエクセルにCSVファイルを読み込んで文字化けした場合、write_excel_csv()を試してみてください。\n\nexceldata |&gt; write_excel_csv(file = fname)\n\nRDSファイルの出力\nRのオブジェクトをバイナリファイルとして保存したい場合は write_rds() を使います。\n\nfname = \"table2_output.rds\"\nexceldata |&gt; write_rds(file = fname)",
    "crumbs": [
      "Rの基礎",
      "Tidyverse"
    ]
  },
  {
    "objectID": "part05.html",
    "href": "part05.html",
    "title": "ggplot の図",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(readxl)\nlibrary(ggpubr)\nlibrary(showtext)\n\nLoading required package: sysfonts\nLoading required package: showtextdb\n\nlibrary(patchwork)",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#図の詳細設定",
    "href": "part05.html#図の詳細設定",
    "title": "ggplot の図",
    "section": "図の詳細設定",
    "text": "図の詳細設定\nshowtext パッケージを使って、システムフォントを使えるようにします。 使用可能なフォントは次のように調べます。\n\n#! eval: false\nfont_files() |&gt; as_tibble()\n\n# A tibble: 720 × 6\n   path                                       file  family face  version ps_name\n   &lt;chr&gt;                                      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  \n 1 /home/gnishihara/.local/share/fonts/Unkno… Font… Fonti… Bold… \"1.000\" Fontin…\n 2 /home/gnishihara/.local/share/fonts/Unkno… Font… Fonti… Ital… \"1.000\" Fontin…\n 3 /home/gnishihara/.local/share/fonts/Unkno… Font… Fonti… Regu… \"1.000\" Fontin…\n 4 /home/gnishihara/.local/share/fonts/Unkno… Font… Fonti… Smal… \"1.000\" Fontin…\n 5 /home/gnishihara/.local/share/fonts/Unkno… EB_G… EB Ga… Ital… \"Versi… EBGara…\n 6 /home/gnishihara/.local/share/fonts/Unkno… EB_G… EB Ga… Regu… \"Versi… EBGara…\n 7 /home/gnishihara/.local/share/fonts/Unkno… Font… Fonti… Smal… \"Versi… Fontin…\n 8 /home/gnishihara/.local/share/fonts/Unkno… Font… Fontin Bold  \"Versi… Fontin…\n 9 /home/gnishihara/.local/share/fonts/Unkno… Font… Fontin Ital… \"Versi… Fontin…\n10 /home/gnishihara/.local/share/fonts/Unkno… Font… Fontin Regu… \"Versi… Fontin…\n# … with 710 more rows\n\n\nGoogle の Noto シリーズのフォントを使いたいので、filter() にかけます。\n\nfont_files() |&gt; as_tibble() |&gt; \n  filter(str_detect(ps_name, \"NotoSansCJK|NotoSansSymbol\")) |&gt; \n  select(file, face, ps_name) \n\n# A tibble: 17 × 3\n   file                           face    ps_name                   \n   &lt;chr&gt;                          &lt;chr&gt;   &lt;chr&gt;                     \n 1 NotoSansCJKjp-Black.otf        Regular NotoSansCJKjp-Black       \n 2 NotoSansCJKjp-Bold.otf         Regular NotoSansCJKjp-Bold        \n 3 NotoSansCJKjp-DemiLight.otf    Regular NotoSansCJKjp-DemiLight   \n 4 NotoSansCJKjp-Light.otf        Regular NotoSansCJKjp-Light       \n 5 NotoSansCJKjp-Medium.otf       Regular NotoSansCJKjp-Medium      \n 6 NotoSansCJKjp-Regular.otf      Regular NotoSansCJKjp-Regular     \n 7 NotoSansCJKjp-Thin.otf         Regular NotoSansCJKjp-Thin        \n 8 NotoSansSymbols-Black.ttf      Regular NotoSansSymbols-Black     \n 9 NotoSansSymbols-Bold.ttf       Bold    NotoSansSymbols-Bold      \n10 NotoSansSymbols-ExtraBold.ttf  Regular NotoSansSymbols-ExtraBold \n11 NotoSansSymbols-ExtraLight.ttf Regular NotoSansSymbols-ExtraLight\n12 NotoSansSymbols-Light.ttf      Regular NotoSansSymbols-Light     \n13 NotoSansSymbols-Medium.ttf     Regular NotoSansSymbols-Medium    \n14 NotoSansSymbols-Regular.ttf    Regular NotoSansSymbols-Regular   \n15 NotoSansSymbols-SemiBold.ttf   Regular NotoSansSymbols-SemiBold  \n16 NotoSansSymbols-Thin.ttf       Regular NotoSansSymbols-Thin      \n17 NotoSansSymbols2-Regular.ttf   Regular NotoSansSymbols2-Regular  \n\n\nフォントファイルのファイル名は file 変数にあります。 その変数を使って、font_add() 関数で用意します。\n\nfont_add(family = \"notosans\", \n         regular = \"NotoSansCJKjp-Regular.otf\",\n         bold = \"NotoSansCJKjp-Black.otf\",\n         symbol = \"NotoSansSymbols-Regular.ttf\")\n\n図のデフォルトテーマをここで設定します。 base_size はフォントの大きさ。 base_family は font_add() で定義した family です。\n\ntheme_gray(base_size = 10, base_family = \"notosans\") |&gt; theme_set()\nshowtext_auto()\n\n論文用のテーマは ggpubr パッケージの theme_pubr() をおすすめします。\n\ntheme_pubr(base_size = 10, base_family = \"notosans\") |&gt; theme_set()\nshowtext_auto()",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#ggplot2-について",
    "href": "part05.html#ggplot2-について",
    "title": "ggplot の図",
    "section": "ggplot2 について",
    "text": "ggplot2 について\n\nggplot2 の関数は + でつなげる\nggplot() はベースレイヤー\ngeom_*() はプロットレイヤー\nscales_*() でエステティク (aesthetics) を調整\ntheme() や theme_() で書式を調整\nfacet_wrap() や facet_grid() は多変量データのプロットのパネル分け",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#aesthetics-エステティクとは",
    "href": "part05.html#aesthetics-エステティクとは",
    "title": "ggplot の図",
    "section": "Aesthetics （エステティク）とは",
    "text": "Aesthetics （エステティク）とは\n\n色・透明度\n\ncolor：点と線の色\nfill：面の色\nalpha：透明度（0 – 1 の値）\n\n\n\n大きさ・形状\n\nsize：点と文字の大きさ、線の太さ\nshape：点の形\nlinetype：線の種類\n\n\n\nグループ化\n\ngroup：点や線のグループ化\n\n\n\n座標、始点・終点\n\nx, y\nxmin, ymin\nxend, yend",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#geom-の種類",
    "href": "part05.html#geom-の種類",
    "title": "ggplot の図",
    "section": "geom の種類",
    "text": "geom の種類\n散布図\n\ngeom_point()\ngeom_jitter()\n\n折れ線グラフ\n\ngeom_path()\ngeom_line()\ngeom_step()\n\n面グラフ * geom_ribbon() * geom_area() * geom_polygon()\nヒートマップ・コンター図 * geom_tile() * geom_raster() * geom_rect() * geom_contour()\nエラーバー * geom_error() * geom_linerange() * geom_pointrange() * geom_crossbar()",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#geom-の種類-1",
    "href": "part05.html#geom-の種類-1",
    "title": "ggplot の図",
    "section": "geom の種類",
    "text": "geom の種類\n曲線など\n\ngeom_smooth()\ngeom_curve()\ngeom_segment()\ngeom_abline()\ngeom_hline()\ngeom_vline()\n\n文字列\n\ngeom_text()\ngeom_label()\n\nヒストグラム・密度曲線 * geom_histogram() * geom_freqpoly() * geom_density() * geom_bin2d() * geom_hex() * geom_dotplot()\n棒グラフ・箱ひげ図 * geom_bar() * geom_col() * geom_boxplot() * geom_violin()",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#ggplot2-の付属パッケージ",
    "href": "part05.html#ggplot2-の付属パッケージ",
    "title": "ggplot の図",
    "section": "ggplot2 の付属パッケージ",
    "text": "ggplot2 の付属パッケージ\n研究室が使っているパッケージ\n\nggpubr: theme_pubr(), ggarrange()\nggrepel: geom_text_repel()\nlemon: facet_rep_grid(), facet_rep_wrap()\nshowtext: システムフォントの埋め込み\n\nggplot2 extensions",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#データを読み込んだら可視化しよう",
    "href": "part05.html#データを読み込んだら可視化しよう",
    "title": "ggplot の図",
    "section": "データを読み込んだら、可視化しよう",
    "text": "データを読み込んだら、可視化しよう\n\nfilename = \"Table 2.xlsx\"\ncol_names = c(\"month\", \"temperature1\", \"sd1\", \"empty\",\"temperature2\", \"sd2\")\nexceldata = read_excel(filename, sheet = 1, skip = 2, col_name = col_names)\n\n\nggplot(exceldata) + geom_point(aes(x = month, y = temperature1))\n\n\n\n\n\n\n\n\n横軸の順序がおかしいですね。軸タイトルも変えたほうがいいですね。",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#軸タイトルの関数",
    "href": "part05.html#軸タイトルの関数",
    "title": "ggplot の図",
    "section": "軸タイトルの関数",
    "text": "軸タイトルの関数\n軸タイトルや図のタイトルは labs() 関数でします。\n\nxlabel = \"Month\"\nylabel = \"'Temperature ('*degree*'C)'\" # plotmath expression see ?plotmath\nggplot(exceldata) + \n  geom_point(aes(x = month, y = temperature1)) + \n  labs(x = xlabel, \n       y = parse(text = ylabel),\n       title = \"Monthly mean water temperature\")",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#論文用に変える",
    "href": "part05.html#論文用に変える",
    "title": "ggplot の図",
    "section": "論文用に変える",
    "text": "論文用に変える\n学術論文に記載する図の場合、図から余計なかざりを外します。 研究室では ggpubr の theme_pubr() 関数を使っています。\n\nxlabel = \"Month\"\nylabel = \"'Temperature ('*degree*'C)'\" # plotmath expression see ?plotmath\nggplot(exceldata) + \n  geom_point(aes(x = month, y = temperature1)) + \n  labs(x = parse(text = xlabel), \n       y = parse(text = ylabel))  +\n  theme_pubr(base_size = 10)",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#月の順序をなおす",
    "href": "part05.html#月の順序をなおす",
    "title": "ggplot の図",
    "section": "月の順序をなおす",
    "text": "月の順序をなおす\nもう気づいたと思いますが、横軸の月の順序が間違っています。 factor() で、month 変数を整えます。\n\n# element_text() size is in points (pt)\n# 1 pt = 0.35 mm\nxlabel = \"Month\"\nylabel = \"'Temperature ('*degree*'C)'\" # plotmath expression see ?plotmath\n\nlevels = month.abb\nlevels = str_c(levels, ifelse(levels == \"May\", \"\", \".\"))\n\nexceldata |&gt; \n  mutate(month = factor(month, levels = levels)) |&gt; \n  ggplot() + \n  geom_point(aes(x = month, y = temperature1)) + \n  labs(x = parse(text = xlabel), \n       y = parse(text = ylabel))  +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))\n\n\n\n\n\n\n\n\n\n\nLinking to ImageMagick 6.9.11.60\nEnabled features: fontconfig, freetype, fftw, heic, lcms, pango, webp, x11\nDisabled features: cairo, ghostscript, raw, rsvg\n\n\nUsing 32 threads",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#図を保存する",
    "href": "part05.html#図を保存する",
    "title": "ggplot の図",
    "section": "図を保存する",
    "text": "図を保存する\n図は PDF と PNG 形式で保存しましょう。\nPDFファイル ggsave() は最後の表示した図を書き出しします。 width と height を指定したら必ず単位も指定しましょう (units = \"mm\")。 PDFファイルにシステムフォントを埋め込むなら、device = cairo_pdfも渡しましょう。\n\nwh = list(width = 80, height = 80) # 図の縦横幅\npdffile = \"temperature_plot.pdf\"\nggsave(pdffile, width = wh$width, height = wh$height, units = \"mm\", device = cairo_pdf)\n\nPNGファイル 直接PNGファイルに保存する場合は、画像の解像度 (dpi = 300) も必要です。\n\npngfile = \"temperature_plot.png\"\nggsave(pngfile, width = wh$width, height = wh$height, units = \"mm\", dpi = 300)",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#保存の結果",
    "href": "part05.html#保存の結果",
    "title": "ggplot の図",
    "section": "保存の結果",
    "text": "保存の結果\n\n\n\n\n\n\n\n\n\n\nwh = list(width = 80, height = 80) は同じだが、図は似ていません。\nモニターでみたとき、PDF の解像度は 96 です。つまり、dpi = 300 のPNGファイルはPDFの約 3 倍の大きさです。",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#図のフォントを拡大してpngファイルを修正する",
    "href": "part05.html#図のフォントを拡大してpngファイルを修正する",
    "title": "ggplot の図",
    "section": "図のフォントを拡大して、PNGファイルを修正する",
    "text": "図のフォントを拡大して、PNGファイルを修正する\n\nDPI = 300\nscale = DPI / 96\nexceldata |&gt; \n  mutate(month = factor(month, levels = levels)) |&gt; \n  ggplot() + \n  geom_point(aes(x = month, y = temperature1)) + \n  labs(x = parse(text = xlabel), \n       y = parse(text = ylabel))  +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10 * scale))\n\npngfile = \"temperature_plot.png\"\nwh = list(width = 80, height = 80)\nggsave(pngfile, width = wh$width, height = wh$height, units = \"mm\", dpi = DPI)",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#研究室のワークフロー",
    "href": "part05.html#研究室のワークフロー",
    "title": "ggplot の図",
    "section": "研究室のワークフロー",
    "text": "研究室のワークフロー\nPNGファイルのDPIをいじるのが面倒なので、PDFをPNGに変換するのが楽です。 月の頭文字をチックラベルにします。さらに、lemon パッケージの geom_pointline()を使ってみました。\n\nlibrary(lemon)\n\n\nAttaching package: 'lemon'\n\n\nThe following object is masked from 'package:purrr':\n\n    %||%\n\nxlabel = \"Month\"\nylabel = \"'Temperature'~(degree*C)\" # plotmath expression see ?plotmath\nlevels = month.abb\nlevels = str_c(levels, ifelse(levels == \"May\", \"\", \".\"))\nlabels = str_sub(month.abb, 1, 1)\n# 図の結果は plot1 にいれます。\nplot1 =   exceldata |&gt; mutate(month = factor(month, levels = levels)) |&gt; \n  ggplot() + \n  geom_point(aes(x = month, y = temperature1)) +\n  scale_x_discrete(name = xlabel, labels = labels) +\n  scale_y_continuous(name = parse(text = ylabel), breaks = seq(21, 29, by = 1)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))\n\nまず、PDFファイルを保存します。システムフォントをPDFファイルに入れるためには device = cairo_pdf を渡します。\n\nwh = list(width = 80, height = 80) # 図の縦横幅\npdffile = \"temperature_plot.pdf\"\nggsave(pdffile, width = wh$width, height = wh$height, units = \"mm\", device = cairo_pdf)\n\nImageMagick のAPIを使って、PDFをPNGに変換します。 この方法だと、DPIのややこしい変換は不要です。\nつぎに magick パッケージを読み込みます。\n\nlibrary(magick) # imagemagick パッケージ\n\nつづいて、PDF ファイルを 600 DPI で読み込む。\n\nimg = image_read_pdf(pdffile, density = 600)\n\nPDFファイルをPNGファイルに書き出す。\n\nimg |&gt; image_write(pngfile)",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#保存の結果-1",
    "href": "part05.html#保存の結果-1",
    "title": "ggplot の図",
    "section": "保存の結果",
    "text": "保存の結果\n\n\n\n\n\n\n\n\n\nこのとき、フォントサイズは 10 pt にしました：theme(text = element_text(size = 10))。",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#データを追加してプロット",
    "href": "part05.html#データを追加してプロット",
    "title": "ggplot の図",
    "section": "データを追加してプロット",
    "text": "データを追加してプロット\n\nxlabel = \"Month\"\nylabel = \"'Temperature'~(degree*C)\" # plotmath expression see ?plotmath\nlevels = month.abb\nlevels = str_c(levels, ifelse(levels == \"May\", \"\", \".\"))\nlabels = str_sub(month.abb, 1, 1)\nexceldata |&gt; mutate(month = factor(month, levels = levels)) |&gt; \n  ggplot() + \n  geom_pointline(aes(x = month, y = temperature1, color = \"Group 1\", shape = \"Group 1\", group = 1)) +\n  geom_pointline(aes(x = month, y = temperature2, color = \"Group 2\", shape = \"Group 2\", group = 1)) +\n  scale_x_discrete(name = xlabel, labels = labels) +\n  scale_y_continuous(name = parse(text = ylabel), breaks = seq(15, 30, by = 5), limits = c(15, 30)) +\n  scale_color_viridis_d(\"\", option = \"turbo\", begin = 0, end = 0.5) +\n  scale_shape_discrete(\"\") +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(1, 0),\n        legend.justification = c(1, 0),\n        legend.background = element_blank(),\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n]",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#複数パネルのプロット",
    "href": "part05.html#複数パネルのプロット",
    "title": "ggplot の図",
    "section": "複数パネルのプロット",
    "text": "複数パネルのプロット\n\nxlabel = \"Petal width (cm)\"\nylabel = \"Sepal width (cm)\"\niris |&gt; group_nest(Species) |&gt; \n  mutate(L = c(\"A\", \"B\", \"C\")) |&gt; \n  mutate(Species = sprintf(\"italic('I.')~italic('%s')~'(%s)'\",  Species, L)) |&gt; \n  unnest(data) |&gt; \n  ggplot() + \n  geom_point(aes(x = Petal.Width, y = Sepal.Width, color = Species)) +\n  geom_text(aes(x = 3, y = 5, label = Species), parse = TRUE, vjust = 1, hjust = 1,\n            family = \"notosans\", size =3,  check_overlap = TRUE) +\n  scale_x_continuous(name = xlabel, breaks = seq(0, 3), limits = c(0, 3)) +\n  scale_y_continuous(name = ylabel, breaks = seq(0, 5), limits = c(0, 5)) +\n  scale_color_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  guides(color = \"none\") +\n  facet_rep_grid(cols = vars(Species)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        strip.background = element_blank(),\n        strip.text = element_blank())",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#複数プロットの結合",
    "href": "part05.html#複数プロットの結合",
    "title": "ggplot の図",
    "section": "複数プロットの結合",
    "text": "複数プロットの結合\n\nxlabel1 = \"Petal width (cm)\"\nylabel1 = \"Sepal width (cm)\"\nxlabel2 = \"Petal length (cm)\"\nylabel2 = \"Sepal length (cm)\"\n\niris2 = iris |&gt; \n    mutate(Species = sprintf(\"italic('I.')~italic('%s')\",  Species))\n\n\nplot1 = \n  ggplot(iris2) + \n  geom_point(aes(x = Petal.Width, y = Sepal.Width, color = Species)) +\n  scale_x_continuous(name = xlabel1, breaks = seq(0, 8), limits = c(0, 8)) +\n  scale_y_continuous(name = ylabel1, breaks = seq(0, 8), limits = c(0, 8)) +\n  scale_color_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))\n\n\nplot2 = \n  ggplot(iris2) + \n  geom_point(aes(x = Petal.Length, y = Sepal.Length, color = Species)) +\n  scale_x_continuous(name = xlabel2, breaks = seq(0, 8), limits = c(0, 8)) +\n  scale_y_continuous(name = ylabel2, breaks = seq(0, 8), limits = c(0, 8)) +\n  scale_color_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#複数プロットの結合の結果",
    "href": "part05.html#複数プロットの結合の結果",
    "title": "ggplot の図",
    "section": "複数プロットの結合の結果",
    "text": "複数プロットの結合の結果\n\nplot1 + plot2 + plot_layout(ncol = 2, \n                            nrow = 1, \n                            guides = \"collect\")",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#線と点説明変数は離散型変数の場合",
    "href": "part05.html#線と点説明変数は離散型変数の場合",
    "title": "ggplot の図",
    "section": "線と点（説明変数は離散型変数の場合）",
    "text": "線と点（説明変数は離散型変数の場合）\n\nylabel = \"Petal length (cm)\"\niris2 |&gt; group_by(Species) |&gt; \n  summarise(PL = mean(Petal.Length),\n            sd = sd(Petal.Length)) |&gt; \n  mutate(lower = PL - sd,\n         upper = PL + sd) |&gt; \nggplot() + \n  geom_line(aes(x = Species, y = PL, group = 1)) +\n  geom_point(aes(x = Species, y = PL), size = 2, color = \"white\") +\n  geom_point(aes(x = Species, y = PL), size = 1) +\n  geom_errorbar(aes(x = Species, ymin = lower, ymax = upper),\n                width = 0.0) +  \n  scale_x_discrete(name = \"Species\", labels = scales::parse_format()) +\n  scale_y_continuous(name = ylabel, breaks = seq(0, 8), limits = c(0, 8)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#ボーグラフ",
    "href": "part05.html#ボーグラフ",
    "title": "ggplot の図",
    "section": "ボーグラフ",
    "text": "ボーグラフ\n\nylabel = \"Petal length (cm)\"\niris2 |&gt; group_by(Species) |&gt; \n  summarise(PL = mean(Petal.Length),\n            sd = sd(Petal.Length)) |&gt; \n  mutate(lower = PL - sd,\n         upper = PL + sd) |&gt; \nggplot() + \n  geom_col(aes(x = Species, y = PL, fill = Species)) +\n  geom_errorbar(aes(x = Species, ymin = lower, ymax = upper),\n                width = 0.01) +  \n  scale_x_discrete(name = \"Species\", labels = scales::parse_format()) +\n  scale_y_continuous(name = ylabel, breaks = seq(0, 8), limits = c(0, 8)) +\n  scale_fill_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  guides(fill = \"none\") +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#ボーグラフ横向き並び替える",
    "href": "part05.html#ボーグラフ横向き並び替える",
    "title": "ggplot の図",
    "section": "ボーグラフ（横向き・並び替える）",
    "text": "ボーグラフ（横向き・並び替える）\n\nylabel = \"Petal length (cm)\"\niris2 |&gt; group_by(Species) |&gt; \n  summarise(PL = mean(Petal.Length),\n            sd = sd(Petal.Length)) |&gt; \n  mutate(lower = PL - sd,\n         upper = PL + sd) |&gt; \n  ggplot(aes(x = fct_reorder(Species, PL, .desc = T))) + \n  geom_col(aes(y = PL, fill = Species)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper),\n                width = 0.1) +  \n  scale_x_discrete(name = \"Species\", labels = scales::parse_format()) +\n  scale_y_continuous(name = ylabel, breaks = seq(0, 8), limits = c(0, 8)) +\n  scale_fill_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  guides(fill = \"none\") +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))+ \n  coord_flip()",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#ヒストグラム",
    "href": "part05.html#ヒストグラム",
    "title": "ggplot の図",
    "section": "ヒストグラム",
    "text": "ヒストグラム\n\nxlabel = \"Petal length (cm)\"\nylabel = \"Frequency\"\niris2 |&gt; \n  ggplot() + \n  geom_histogram(aes(x = Petal.Length, fill = Species)) +\n  scale_x_continuous(name = xlabel) +\n  scale_y_continuous(name = ylabel) +\n  scale_fill_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(1,1),\n        legend.justification = c(1,1))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#ヒストグラムパネル",
    "href": "part05.html#ヒストグラムパネル",
    "title": "ggplot の図",
    "section": "ヒストグラム・パネル",
    "text": "ヒストグラム・パネル\n\nxlabel = \"Petal length (cm)\"\nylabel = \"Frequency\"\niris2 |&gt; \n  ggplot() + \n  geom_histogram(aes(x = Petal.Length, fill = Species),\n                 binwidth = 0.1, center = 0) +\n  scale_x_continuous(name = xlabel, limits = c(0, 10)) +\n  scale_y_continuous(name = ylabel) +\n  scale_fill_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  facet_rep_wrap(facets = vars(Species)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(1,1),\n        legend.justification = c(1,1),\n        strip.background = element_blank(),\n        strip.text = element_blank())\n\nWarning: Removed 6 rows containing missing values (`geom_bar()`).",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#時系列",
    "href": "part05.html#時系列",
    "title": "ggplot の図",
    "section": "時系列",
    "text": "時系列\nデータは (https://covid.ourworldindata.org/data/owid-covid-data.csv)。\n\ncovid = read_csv(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\")\n\nRows: 255834 Columns: 67\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (4): iso_code, continent, location, tests_units\ndbl  (62): total_cases, new_cases, new_cases_smoothed, total_deaths, new_dea...\ndate  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid2 = covid |&gt; \n  group_by(continent, date) |&gt; \n  summarise(tc = sum(total_cases_per_million, na.rm=T),\n            td = sum(total_deaths_per_million, na.rm= T)) |&gt; \n  drop_na()\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n\nxlabel = \"Date\"\nylabel = \"COVID cases per million\"\nggplot(covid2) +\n  geom_path(aes(x=date, y = tc, color = continent))+\n  scale_x_date(name = xlabel) +\n  scale_y_continuous(name = ylabel) +\n  scale_color_viridis_d(\"\", option = \"turbo\", begin = 0, end = 0.8) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(0,1),\n        legend.justification = c(0,1),\n        strip.background = element_blank(),\n        strip.text = element_blank())",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#時系列-1",
    "href": "part05.html#時系列-1",
    "title": "ggplot の図",
    "section": "時系列",
    "text": "時系列\n\nxlabel = \"Date\"\nylabel = \"COVID cases per million\"\nggplot(covid2) +\n  geom_path(aes(x=date, y = tc, color = continent))+\n  scale_x_date(name = xlabel, date_labels = \"%Y-%m-%d\") +\n  scale_y_continuous(name = ylabel, \n                     breaks = 10^seq(-2,7), limits = c(0.01, 10^7),\n                     trans = \"log10\", labels = scales::label_math(format = log10)) +\n  scale_color_viridis_d(\"\", option = \"turbo\", begin = 0, end = 0.8) +\n  guides(color = guide_legend(ncol = 2)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(0.5,0),\n        legend.justification = c(0,0),\n        legend.background = element_blank(),\n        strip.background = element_blank(),\n        strip.text = element_blank())\n\nWarning: Transformation introduced infinite values in continuous y-axis",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#時系列軸のカスタムラベル",
    "href": "part05.html#時系列軸のカスタムラベル",
    "title": "ggplot の図",
    "section": "時系列軸のカスタムラベル",
    "text": "時系列軸のカスタムラベル\n\ngnn_date = function() {\n  function(x) {\n    m = format(x, \"%b\")\n    m = str_sub(m, start = 1, end = 1)\n    y = format(x, \"%Y\")\n    ifelse(duplicated(y), m, sprintf(\"%s\\n%s\", m,y))\n  }\n}",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#時系列-2",
    "href": "part05.html#時系列-2",
    "title": "ggplot の図",
    "section": "時系列",
    "text": "時系列\n\nxlabel = \"Date\"\nylabel = \"COVID cases per million\"\nggplot(covid2) +\n  geom_path(aes(x=date, y = tc, color = continent))+\n  scale_x_date(name = xlabel, date_breaks = \"months\", labels = gnn_date()) +\n  scale_y_continuous(name = ylabel, \n                     breaks = 10^seq(-2,7, by = 2), limits = c(0.01, 10^7),\n                     trans = \"log10\", labels = scales::label_math(format = log10)) +\n  scale_color_viridis_d(\"\", option = \"turbo\", begin = 0, end = 0.8) +\n  guides(color = guide_legend(ncol = 2)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(0.5,0),\n        legend.justification = c(0,0),\n        legend.background = element_blank(),\n        strip.background = element_blank(),\n        strip.text = element_blank())\n\nWarning: Transformation introduced infinite values in continuous y-axis",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "part05.html#箱ひげ図",
    "href": "part05.html#箱ひげ図",
    "title": "ggplot の図",
    "section": "箱ひげ図",
    "text": "箱ひげ図\n\ncovid2 = covid |&gt; \n  filter(date &gt;= lubridate::ymd(\"2021-01-01\")) |&gt; \n  filter(str_detect(location, \"Indonesia|Japan|South Korea|Taiwan|China\"))\n\n\nxlabel = \"Country\"\nylabel = \"Daily cases per million\"\nggplot(covid2) +\n  geom_boxplot(aes(x = fct_reorder(location, new_cases_per_million, mean, na.rm=T, .desc=T), \n                   y = new_cases_per_million, fill = location)) + \n  scale_x_discrete(name = xlabel) +\n  scale_y_continuous(name = ylabel, \n                     breaks = 10^seq(-3,3, by = 1), limits = 10^c(-3, 3),\n                     trans = \"log10\", labels = scales::label_math(format = log10)) +\n  scale_color_viridis_d(\"\", option = \"turbo\",  begin = 0, end = 0.8) +\n  guides(fill = guide_legend(ncol = 2)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(0,0),\n        legend.justification = c(0,0),\n        legend.background = element_blank(),\n        legend.title = element_blank(),\n        strip.background = element_blank(),\n        strip.text = element_blank())\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 506 rows containing non-finite values (`stat_boxplot()`).",
    "crumbs": [
      "Rの基礎",
      "ggplot の図"
    ]
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "一元配置分散分析",
    "section": "",
    "text": "Note\n\n\n\n解析の紹介に使った疑似データは次のようにつくりました。\n\nset.seed(2021) # 疑似乱数のシードを設定する\nnA = 6         # サンプル数を決める\nnB = 6\nnC = 6\nmeanA = 20     # 真の平均値\nmeanB = 22\nmeanC = 18\nsigmaA = 1     # 真の標準偏差\nsigmaB = 1\nsigmaC = 1\nsiteA = rnorm(nA, meanA, sigmaA) |&gt; round(1) # データを発生する\nsiteB = rnorm(nB, meanB, sigmaB) |&gt; round(1)\nsiteC = rnorm(nC, meanC, sigmaC) |&gt; round(1)\n\n# tibble を組み立てる\ndset = tibble(g = c(\"A\", \"B\", \"C\"), data = list(siteA, siteB, siteC)) |&gt; \n  unnest(data) |&gt; \n  mutate(g = factor(g))",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#ノコギリモク-sargassum-macrocarpum-の疑似データ",
    "href": "anova.html#ノコギリモク-sargassum-macrocarpum-の疑似データ",
    "title": "一元配置分散分析",
    "section": "ノコギリモク (Sargassum macrocarpum) の疑似データ",
    "text": "ノコギリモク (Sargassum macrocarpum) の疑似データ\n\n\n\n\n\n\n\n\n\nTable 1: Size (mm) of juvenile Sargassum macrocarpum（ノコギリモク）.\n\n\n\n\n\n\n\nSample\nSite A\nSite B\nSite C\n\n\n\n\n1\n19.9\n22.3\n18.2\n\n\n2\n20.6\n22.9\n19.5\n\n\n3\n20.3\n22.0\n19.6\n\n\n4\n20.4\n23.7\n16.2\n\n\n5\n20.9\n20.9\n19.6\n\n\n6\n18.1\n21.7\n18.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nノコギモクの大きさは Table 1 に示しています。 サンプルは 3 箇所（3群）から採取しました。 各サンプルに個体数番号もふっています。\nデータは 上のノートに紹介したコードで発生しました。",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#データの可視化",
    "href": "anova.html#データの可視化",
    "title": "一元配置分散分析",
    "section": "データの可視化",
    "text": "データの可視化\n\nggplot(dset) + \n  geom_point(aes(x = g, y = data, color = g),\n             size = 2,\n             position = position_jitter(0.1)) +\n  scale_color_manual(\"\", values = viridis::viridis(4)) +\n  labs(y = \"Width (mm)\", x = \"Site\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n各サイトの平均値 (\\overline{x}), 標準偏差 (s), と標準誤差 (s.e.) は、\n\n\\overline{x}_A= 20; s_A= 1; s.e. = 0.41\n\\overline{x}_B= 22.2; s_B= 0.97; s.e. = 0.4\n\\overline{x}_C= 18.5; s_C= 1.34; s.e. = 0.55",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#hypothesis",
    "href": "anova.html#hypothesis",
    "title": "一元配置分散分析",
    "section": "仮説を決める",
    "text": "仮説を決める\n\n\n\n\n\n\nImportant\n\n\n\n解析する前に作業仮説、帰無仮説、対立仮説を設定する必要があります。\n\n\n\n\n\n\n\n\nWorking hypothesis\n\n\n\n作業仮設: ノコギリモクの大きさは採取した場所によって異なる。\n\n\n\n記述統計量によって、平均値以外の統計量（標準偏差と標準誤差）は似ています。\n\\overline{x}_A= 20; s= 1; s.e. = 0.41\n\\overline{x}_B= 22.2; s= 0.97; s.e. = 0.4\n\\overline{x}_C= 18.5; s= 1.34; s.e. = 0.55\n\n\n帰無仮説と対立仮説\n統計学的に解析するための帰無仮説と対立仮説を決めます。\n\nH_0 (null hypothesis 帰無仮説・ヌル仮説): ノコギリモクの大きさは場所によって異ならない\nH_A (alternative hypothesis 対立仮設): ノコギリモクの大きさは場所によって異なる",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#ナイーブな-ペア毎の-t-検定",
    "href": "anova.html#ナイーブな-ペア毎の-t-検定",
    "title": "一元配置分散分析",
    "section": "ナイーブな ペア毎の t 検定",
    "text": "ナイーブな ペア毎の t 検定\nとりあえず、場所のペア毎の t 検定を実施します。 このとき、3 つの帰無仮説が必要なので、(hypothesis?) と違います。\n\nH0,A-B: Site A と Site B の大きさは同じ\nH0,A-C: Site A と Site C の大きさは同じ\nH0,B-C: Site B と Site C の大きさは同じ\n\nでは、それぞれの t 検定を実施します。\n\nSite A and B の t 検定\n\nresultAB = dset |&gt; filter(!str_detect(g, \"C\"))\nresultAB = t.test(data ~ g, data = resultAB)\nresultAB\n\n\n    Welch Two Sample t-test\n\ndata:  data by g\nt = -3.8886, df = 9.9893, p-value = 0.003022\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -3.486976 -0.946357\nsample estimates:\nmean in group A mean in group B \n       20.03333        22.25000 \n\n\nt値は -3.889、P値は 0.003 です。 0.003 \\le 0.05 なので、帰無仮説は棄却できます。\n\n\nSite A and C の t 検定\n\nresultAC = dset |&gt; filter(!str_detect(g, \"B\"))\nresultAC = t.test(data ~ g, data = resultAC)\nresultAC\n\n\n    Welch Two Sample t-test\n\ndata:  data by g\nt = 2.1968, df = 9.2717, p-value = 0.05477\nalternative hypothesis: true difference in means between group A and group C is not equal to 0\n95 percent confidence interval:\n -0.03773888  3.03773888\nsample estimates:\nmean in group A mean in group C \n       20.03333        18.53333 \n\n\nt値は 2.197、P値は 0.0548 です。 0.0548 \\ge 0.05 なので、帰無仮説は棄却できません。\n\n\nSite B and C の t 検定\n\nresultBC = dset |&gt; filter(!str_detect(g, \"A\"))\nresultBC = t.test(data ~ g, data = resultBC)\nresultBC\n\n\n    Welch Two Sample t-test\n\ndata:  data by g\nt = 5.5063, df = 9.1228, p-value = 0.0003595\nalternative hypothesis: true difference in means between group B and group C is not equal to 0\n95 percent confidence interval:\n 2.192862 5.240472\nsample estimates:\nmean in group B mean in group C \n       22.25000        18.53333 \n\n\nt値は 5.506、P値は 0.0004 です。 0.0004 \\le 0.05 なので、帰無仮説は棄却できます。\nt検定の結果をまとめました。\n\n\n\n\nTable 2: Summary of three t-tests.\n\n\n\n\n\n\n\nPair\nDifference\nt-value\nP-value\nd.f.\n95% CI\nIs P ≤ 0.05?\n\n\n\n\nB-A\n-2.216667\n-3.888623\n0.0030224\n9.989348\n-3.487 to -0.95\nYes\n\n\nC-A\n1.500000\n2.196821\n0.0547748\n9.271711\n-0.038 to 3.04\nNo\n\n\nC-B\n3.716667\n5.506257\n0.0003595\n9.122821\n2.193 to 5.24\nYes\n\n\n\n\n\n\n\n\n\n\n\nd.f. は Welch-Satterthwaite 式で求めた自由度1、95% CI は 95% 信頼区間です。\n1 degrees-of-freedom",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#第１種の誤り-accepting-a-false-h0",
    "href": "anova.html#第１種の誤り-accepting-a-false-h0",
    "title": "一元配置分散分析",
    "section": "第１種の誤り (accepting a false H0)",
    "text": "第１種の誤り (accepting a false H0)\n\n\n\n\n\n\n第１種の誤り\n\n\n\nH0 が FALSE のときに帰無仮説を棄却できなかった誤りです。\n\n\nt 検定を 1 回実施したときの誤りは\n\n\\text{Type I error rate} = \\alpha = 0.05\n\nt 検定を 2 回実施したときの誤りは\n\n1 - (1 - \\alpha) \\times (1 - \\alpha) = 1 - (1-\\alpha)^2=0.0975\n\nt 検定を 3 回実施したときの誤りは\n\n1 - (1 - \\alpha) \\times (1 - \\alpha) \\times (1 - \\alpha)= 1 - (1-\\alpha)^3=0.142625\n\nt 検定を h 回実施したとき、第１種の誤りは 1 - (1-\\alpha)^h です。\n\n群が増えると大変なことなる\nn 群のサンプルの全ペア毎の比較がしたい場合、 h のペア (k = 2) が存在します。\n\nh = \\binom{n}{k}=\\frac{n!}{k!(n-k!)}\n\nペア毎の h の数を求める式は次のようになります。\n\nh = \\binom{n}{2}=\\frac{n!}{2!(n-2!)} = \\frac{n(n-1)}{2}\n\n例えば、5 site の場合、10 のペアが存在します。 ペア毎の t 検定をしたら、第１種の誤りは\n\n1 - (1-0.05)^{10}=0.4012631\n\n\n\n\n\n\n\nR での求め方\n\n\n\n\nalpha = 0.05       # 有意水準\nk = 2              # ペアだから 2\nn = 5              # 比較する群・場所・グループの数\nh = choose(n, k)   # ペアの数\n1 - (1 - alpha)^h  # 第１種の誤り\n\n[1] 0.4012631\n\n\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n比較する群が増えると、t 検定を繰り返して実施すると、第１種の誤りを起こしやすい。",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#one-way-anova-一元配置分散分析",
    "href": "anova.html#one-way-anova-一元配置分散分析",
    "title": "一元配置分散分析",
    "section": "One-Way ANOVA (一元配置分散分析)",
    "text": "One-Way ANOVA (一元配置分散分析)\n複数群（因子の水準）の解析は 一元配置分散分析)2 用います。\n2 One-Way ANOVA (One-Way Analysis of Variance\n因子・要因3：説明変数、一般的には離散型な変数\n水準4：説明変数における値、レベル、要素\n\n3 factor4 level, factor level分散分析の帰無仮説は、\n\n\\mu_1 = \\mu_2 = \\cdots = \\mu_i\n\nつまり、一つの検定で複数群の平均値を同時に解析するから、第１種の誤りは 0.05 に抑えられる。\n分散分析のモデル式は次のように表せます。\n\nx_{ij} = \\mu_i + \\epsilon_{ij}\n\n水準 i とサンプル j の値は x_{ij}です。 水準 i の平均値は \\mu_i です。 モデルの残渣[residual]または誤差項[error term]は \\epsilon_{ij} です。",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#一元配置分散分析表",
    "href": "anova.html#一元配置分散分析表",
    "title": "一元配置分散分析",
    "section": "一元配置分散分析表",
    "text": "一元配置分散分析表\n\n\n\n\n\n\nFactor\nDegrees-of-freedom (df)\nSum-of-Squares (SS)\nMean-square (MS)\nF-value\nP-value\n\n\n\n\nA\n$df_A = I-1$\n$SS_A$\n$MS_A = SS_A / df_A$\n$MS_A / MS_R$\n$qf(1-α, df_A, df_R)$\n\n\ne\n$df_R = I(J-1)$\n$SS_R$\n$MS_R = SS_R / df_R$\n\n\n\n\n\n$df_T =IJ-1$\n$SS_T$\n\n\n\n\n\n\n\n\n\n\n\n\n因子は A5\n残渣は e6\n水準数は I7\nサンプル数は　J8\n水準間平方和は SS_A9\n残渣平方和は SS_R10\n総平方和は SS_T11\n水準間平均平方は MS_A12\n残渣平均平方は MS_R13\nF値14は MS の比です。\n\n5 factor6 residual7 number of levels8 number of samples9 among levels sum-of-squares (SS)10 residual SS11 total SS12 among levels mean square (MS)13 residual mean square (MS)14 F-value",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#平方和の方程式",
    "href": "anova.html#平方和の方程式",
    "title": "一元配置分散分析",
    "section": "平方和の方程式",
    "text": "平方和の方程式\n\n\\underbrace{\\sum_{i=1}^I\\sum_{j=1}^J(x_{ij} - \\overline{\\overline{x}})^2 }_{\\text{総平方和}\\;(SS_T)} =\n\\overbrace{J\\sum_{i=1}^I(\\overline{x}_{i}-\\overline{\\overline{x}})^2}^{\\text{水準間平方和}\\;SS_A} +\n\\underbrace{\\sum_{i=1}^I\\sum_{j=1}^J(x_{ij} - \\overline{x}_i)^2}_{\\text{残渣平方和}\\;SS_R}\n\n標本平均は \\bar{x}_i、総平均は \\bar{\\bar{x}} です。",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#decomposing-the-sum-of-squares",
    "href": "anova.html#decomposing-the-sum-of-squares",
    "title": "一元配置分散分析",
    "section": "Decomposing the sum-of-squares",
    "text": "Decomposing the sum-of-squares\n\n\n\n\n\n\n\n\n\n残渣平方和：各サンプルの値は点、グループ毎の平均値は黒線で示しています。黒線から点の縦線は残渣を表しています。\n\n\n\n\n\n\n\n水準間平方和：各グループの平均値は点、総平均は黒線で示しています。黒線から点の縦線はグループ毎の平均値と総平均の違いを表しています。",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#分散分析の統計量",
    "href": "anova.html#分散分析の統計量",
    "title": "一元配置分散分析",
    "section": "分散分析の統計量",
    "text": "分散分析の統計量\n\nF = \\left . \\frac{SS_A}{I-1} \\right / \\frac{SS_R}{I(J-1)}  = \\frac{SS_A}{SS_R} \\frac{I(J-1)}{I-1} = \\frac{MS_A}{MS_R}\n\nF値は 自由度 \\text{df}_1 = I-1, \\text{df}_2 = I(J-1) のF分布に従います。 水準の数は I、水準ごとのサンプルの数は J です。\n\n\n\n\n\n\nNote\n\n\n\n\nF値の分子15 が大きとき、または分母16が小さいとき、F値は大きいです。\nF値とP値は反比例します。\n\n\n\n16 denominator15 numerator",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#f値の確率密度関数",
    "href": "anova.html#f値の確率密度関数",
    "title": "一元配置分散分析",
    "section": "F値の確率密度関数",
    "text": "F値の確率密度関数\n\nP(x|\\text{df}_1, \\text{df}_2) = \\frac{1}{\\mathrm{B}\\left(\\frac{\\text{df}_1}{2}, \\frac{\\text{df}_2}{2}\\right)}\\left(\\frac{\\text{df}_1}{\\text{df}_2}\\right)^{\\left(\\frac{\\text{df}_1}{2}\\right)}x^{\\left(\\frac{\\text{df}_1}{2}-1\\right)}\\left(1+\\frac{\\text{df}_1}{\\text{df}_2}x\\right)^{\\left(-\\frac{\\text{df}_1+\\text{df}_2}{2}\\right)}\n \\mathrm{B}(\\text{df}_1, \\text{df}_2)=\\int_0^1t^{x-1}(1-t)^{y-1}dt は ベータ関数17 といいます。 \\text{df}_1 と \\text{df}_2 は自由度、x は確率変数です。\n17 Beta function\n\n\n\n\n自由度が変わるとF分布の形が変わります。",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#一元配置分散分析表の仮定",
    "href": "anova.html#一元配置分散分析表の仮定",
    "title": "一元配置分散分析",
    "section": "一元配置分散分析表の仮定",
    "text": "一元配置分散分析表の仮定\n分散分析するときに注意する仮定\n\n水準毎の母分散は等しい\n残渣は正規分布に従う\n観測値はお互いに独立であり、同一分布に従う\n観測変数は連続変数18\n説明変数は離散変数19\n\n18 continuous19 discrete",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#rにおける解析",
    "href": "anova.html#rにおける解析",
    "title": "一元配置分散分析",
    "section": "Rにおける解析",
    "text": "Rにおける解析\n解析例に使うデータは thedata.csv に保存したので、まずは読み込みます。\n\n# Read data from a csv file\ndset = read_csv(\"thedata.csv\")\n\nデータをグループ化したあと、最初の 2 行を表示する。\n\ndset |&gt; group_by(site) |&gt; slice(1:2)\n\n# A tibble: 6 × 2\n# Groups:   site [3]\n  site    obs\n  &lt;fct&gt; &lt;dbl&gt;\n1 A      19.9\n2 A      20.6\n3 B      22.3\n4 B      22.9\n5 C      18.2\n6 C      19.5\n\n\n帰無仮説を当てはめる。\n\nnullmodel = lm(obs ~ 1, data = dset) # 帰無モデル、ヌルモデル\n\nフルモデル（対立仮説）を当てはめる。\n\nfullmodel = lm(obs ~ site, data = dset) # 対立モデル、フルモデル",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#分散分析の結果",
    "href": "anova.html#分散分析の結果",
    "title": "一元配置分散分析",
    "section": "分散分析の結果",
    "text": "分散分析の結果\n帰無仮説と対立仮説のモデル結果を用いた方法。\n\nanova(nullmodel, fullmodel, test = \"F\")\n\nAnalysis of Variance Table\n\nModel 1: obs ~ 1\nModel 2: obs ~ site\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     17 60.656                                  \n2     15 18.702  2    41.954 16.825 0.0001471 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nF値は 16.825、自由度は df1 = 2 と df2 = 15 です。 よって、P値は 0.000147 です。 有意水準が \\alpha = 0.05 、自由度が　(2, 15) のときのF値は 3.682.\nフルモデルの結果でけ用いた解析\n\nanova(fullmodel)\n\nAnalysis of Variance Table\n\nResponse: obs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nsite       2 41.954 20.9772  16.825 0.0001471 ***\nResiduals 15 18.702  1.2468                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\naov() 関数を用いた方法\nこのとき、lm() は不要です。\n\naovout = aov(obs ~ site, data = dset)\nsummary(aovout)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nsite         2  41.95  20.977   16.82 0.000147 ***\nResiduals   15  18.70   1.247                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nImportant\n\n\n\n分散分析の帰無仮説は \\mu_0 = \\mu_1 = \\cdots \\mu_i なので、 ペア間の検定ではないです。",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#多重比較-1",
    "href": "anova.html#多重比較-1",
    "title": "一元配置分散分析",
    "section": "多重比較",
    "text": "多重比較\n分散分析の帰無仮説を棄却したら、ペア毎の比較がしたくなります。 第１種の誤りを抑える多重比較の検定は豊富に存在します。\n\nBonferroni Procedure (ボンフェロニ法)\nHolm-Bonferroni Method (ホルム = ボンフェロニ法)\nTukey’s Honest Signiﬁcant Difference Test (テューキーの HSD 検定)\nTukey-Kramer method, Tukey’s test\nScheffe’s Method (シェッフェの方法)\nDunnett’s Test (ダネットの検定)\nFisher’s Least Signiﬁcant Difference (フィッシャーの最小有意差法)\nDuncan’s new multiple range test (ダンカンの新多重範囲検定)\n\n1 から 4 はペア毎の比較です。 ダネットの検定は水準に対する比較です。 フィッシャーとダンカンの検定の第１種の誤りは高いので、使用しないでください。",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#多重比較用-r-パッケージ",
    "href": "anova.html#多重比較用-r-パッケージ",
    "title": "一元配置分散分析",
    "section": "多重比較用 R パッケージ",
    "text": "多重比較用 R パッケージ\n多重比較用の関数は次のパッケージにあります。\n\nmultcomp\nemmeans\n\nここでは、emmeans を紹介します。\n\nlibrary(emmeans) # 多重比較用パッケージ\nlibrary(nlme)    # gls() 関数はこのパッケージにある\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#繰り返しウェルチの-t-検定",
    "href": "anova.html#繰り返しウェルチの-t-検定",
    "title": "一元配置分散分析",
    "section": "繰り返しウェルチの t 検定",
    "text": "繰り返しウェルチの t 検定\n説明のために紹介しています。実際の解析には使わないでください。\n\nglsmodel = gls(obs ~ site, data = dset, \n               weights = varIdent(form = ~ 1|site))\nemout = emmeans(glsmodel, specs = pairwise ~ site, adjust = \"none\")\nemout$contrasts |&gt; summary(infer =T)\n\n contrast estimate    SE   df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.570 9.99  -3.4870   -0.946  -3.889  0.0030\n A - C        1.50 0.683 9.26  -0.0381    3.038   2.197  0.0548\n B - C        3.72 0.675 9.11   2.1925    5.241   5.506  0.0004\n\nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n\n第１種の誤りを調整していません。",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#繰り返し-t-検定",
    "href": "anova.html#繰り返し-t-検定",
    "title": "一元配置分散分析",
    "section": "繰り返し t 検定",
    "text": "繰り返し t 検定\n説明のために紹介しています。実際の解析には使わないでください。\n\nemout = emmeans(fullmodel, specs = pairwise ~ site, data = dset, adjust = \"none\")\nemout$contrasts |&gt; summary(infer =T)\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.645 15   -3.591   -0.843  -3.438  0.0037\n A - C        1.50 0.645 15    0.126    2.874   2.327  0.0344\n B - C        3.72 0.645 15    2.343    5.091   5.765  &lt;.0001\n\nConfidence level used: 0.95 \n\n\n第１種の誤りを調整していません。",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#ボンフェロニ法",
    "href": "anova.html#ボンフェロニ法",
    "title": "一元配置分散分析",
    "section": "ボンフェロニ法",
    "text": "ボンフェロニ法\n\nemout = emmeans(fullmodel, specs = pairwise ~ site, data=dset, adjust = \"bonferroni\")\nemout$contrasts |&gt; summary(infer =T)\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.645 15   -3.953    -0.48  -3.438  0.0110\n A - C        1.50 0.645 15   -0.237     3.24   2.327  0.1032\n B - C        3.72 0.645 15    1.980     5.45   5.765  0.0001\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 3 estimates \nP value adjustment: bonferroni method for 3 tests \n\n\nP値は p_{adj} =m\\times p によって求められました.",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#ホルムボンフェロニ法",
    "href": "anova.html#ホルムボンフェロニ法",
    "title": "一元配置分散分析",
    "section": "ホルム=ボンフェロニ法",
    "text": "ホルム=ボンフェロニ法\n\nemout = emmeans(fullmodel, specs = pairwise ~ site, data=dset, adjust = \"holm\")\nemout$contrasts |&gt; summary(infer =T)\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.645 15   -3.953    -0.48  -3.438  0.0073\n A - C        1.50 0.645 15   -0.237     3.24   2.327  0.0344\n B - C        3.72 0.645 15    1.980     5.45   5.765  0.0001\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 3 estimates \nP value adjustment: holm method for 3 tests \n\n\nP値は低い値から高い値へ並べ替えてから、p_{adj} = (m+1-k)\\times p によって求めます。 m は比較の数、 k は比較の指数です。\n\n\n\n\n\n\nNote\n\n\n\nボンフェロニ法とホルム=ボンフェロニ法の P値は次のように求められます。\n\nemout = emmeans(fullmodel, specs = pairwise ~ site, adjust = \"none\")\nx = emout$contrasts  |&gt;  as_tibble()\nx  |&gt;  arrange(p.value)  |&gt; \n  mutate(k = 1:3)  |&gt; mutate(m = n())  |&gt; \n  mutate(p.bonferroni = p.value * m,\n         p.holm = p.value * (m + 1 - k))  |&gt; \n  select(contrast, m, k, starts_with(\"p\"))\n\n# A tibble: 3 × 6\n  contrast     m     k   p.value p.bonferroni   p.holm\n  &lt;chr&gt;    &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 B - C        3     1 0.0000373     0.000112 0.000112\n2 A - B        3     2 0.00366       0.0110   0.00731 \n3 A - C        3     3 0.0344        0.103    0.0344",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#テューキーのhsd法",
    "href": "anova.html#テューキーのhsd法",
    "title": "一元配置分散分析",
    "section": "テューキーのHSD法",
    "text": "テューキーのHSD法\n\nemout = emmeans(fullmodel, specs = pairwise ~ site, data=dset, adjust = \"tukey\")\nemout$contrasts |&gt; summary(infer =T)\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.645 15   -3.891   -0.542  -3.438  0.0096\n A - C        1.50 0.645 15   -0.174    3.174   2.327  0.0826\n B - C        3.72 0.645 15    2.042    5.391   5.765  0.0001\n\nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 3 estimates \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nP値はステュデント化範囲の分布21に従います。\n21 Studentized range distribution",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#シェッフェの方法",
    "href": "anova.html#シェッフェの方法",
    "title": "一元配置分散分析",
    "section": "シェッフェの方法",
    "text": "シェッフェの方法\n\nemout = emmeans(fullmodel, specs = pairwise ~ site, data=dset, adjust = \"scheffe\")\nemout$contrasts |&gt; summary(infer =T)\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.645 15   -3.966   -0.467  -3.438  0.0128\n A - C        1.50 0.645 15   -0.249    3.249   2.327  0.0991\n B - C        3.72 0.645 15    1.967    5.466   5.765  0.0002\n\nConfidence level used: 0.95 \nConf-level adjustment: scheffe method with rank 2 \nP value adjustment: scheffe method with rank 2 \n\n\nP値はF分布に従います。",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#ダネットの検定",
    "href": "anova.html#ダネットの検定",
    "title": "一元配置分散分析",
    "section": "ダネットの検定",
    "text": "ダネットの検定\n\nemout = emmeans(fullmodel, specs = trt.vs.ctrl ~ site, ref = 2)\nemout$contrasts |&gt; summary(infer =T)\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.645 15     -3.8   -0.633  -3.438  0.0070\n C - B       -3.72 0.645 15     -5.3   -2.133  -5.765  0.0001\n\nConfidence level used: 0.95 \nConf-level adjustment: dunnettx method for 2 estimates \nP value adjustment: dunnettx method for 2 tests \n\n\nダネットの検定は、各水準は標準水準と比較する方法です。 P値は多変量 t 分布に従います。",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "anova.html#多重比較のおすすめ",
    "href": "anova.html#多重比較のおすすめ",
    "title": "一元配置分散分析",
    "section": "多重比較のおすすめ",
    "text": "多重比較のおすすめ\n\n比較: A – B, A – C, B – C  テューキーのHSD法\n比較: A – B, A – C, A – D  ダネット法",
    "crumbs": [
      "線形モデル",
      "一元配置分散分析"
    ]
  },
  {
    "objectID": "ttest.html",
    "href": "ttest.html",
    "title": "2群の比較：t 検定",
    "section": "",
    "text": "ノコギリモク (Sargassum macrocarpum) は褐藻類ホンダワラ属の海藻です。 通年藻場を形成する海藻であり、海洋動物の住処、餌場、炭素固定の場として機能しています。 かつて、九州に広く分布していましたが、温暖化に伴う環境変動と食害によって、局地的に絶滅しています。 ここでは、ノコギリモクの幼体を資料として、2軍における解析手法を紹介します。\nでは、地点 A と B のノコギリモク幼体の幅は Table 1 の通りです。 各地点から合計６個体採取しました。",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#作業仮説を考えましょう",
    "href": "ttest.html#作業仮説を考えましょう",
    "title": "2群の比較：t 検定",
    "section": "作業仮説を考えましょう",
    "text": "作業仮説を考えましょう\n\n\n\n\n\n\n作業仮設1\n\n\n\nすべての研究は作業仮説 から始まります。\n今回の例について、作業仮説は 「地点毎に対するノコギリモク幼体の幅は異なる」にしました。\n\n\n1 working hypothesis",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#帰無仮説有意生検定が必要とする仮説を決めます",
    "href": "ttest.html#帰無仮説有意生検定が必要とする仮説を決めます",
    "title": "2群の比較：t 検定",
    "section": "帰無仮説有意生検定が必要とする仮説を決めます",
    "text": "帰無仮説有意生検定が必要とする仮説を決めます\n作業仮説を定義したら、つぎは検定のための仮説を定義します。 帰無仮説有意性検定 2\n2 null hypothesis signficance testing\nH_0 (null hypothesis 帰無仮説): 平均値に違いはない (\\mu_{A} = \\mu_{B})\nH_A (alternative hypothesis 対立仮設): 平均値は異なる (\\mu_{A} \\neq \\mu_{B})\n\nつぎのような対立仮説も思いつきます。\n\nH_P (対立仮設): \\mu_A &gt; \\mu_B\nH_N (対立仮設): \\mu_A &lt; \\mu_B\n\n\n\n\n\n\n\n無限に存在する\n\n\n\n帰無仮説と対立仮説はいくらでも考えられますが、 \\mu_A = \\mu_B は一般的な帰無仮説です。 そして、\\mu_A \\neq \\mu_B も一般的な対立仮説です。",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#ナイーブ-な解析手法",
    "href": "ttest.html#ナイーブ-な解析手法",
    "title": "2群の比較：t 検定",
    "section": "ナイーブ 3 な解析手法",
    "text": "ナイーブ 3 な解析手法\n地点 A と B のノコギリモクの大きさの違いが知りたいです。 では、地点同士の大きさの違いを求めます。 地点 A と B の平均値の差を求めてみます。\n\n\\overline{x_A} - \\overline{x_B} = -2.22\n 地点 B のノコギリモクが大きいです。 でも、この大きさはどの程度信用できるかがわかりません。 平均値の差の制度を評価するには、標準誤差 4 を求めないといけないです。 この手法だと、標準誤差は求められません。\n4 standard errorでは、かく地点のサンプル番号ごとの差をとってみます。 この場合、 6 つの差を求められます。 6 つあるので、平均値、標準偏差、標準誤差も求められます。\n\n\n\n\nTable 3: ノコギリモクの幅 (mm) とペア毎の差\n\n\n\nSampleSite ASite BDifference119.922.319.90 - 22.30 = -2.40220.622.920.60 - 22.90 = -2.30320.322.020.30 - 22.00 = -1.70420.423.720.40 - 23.70 = -3.30520.920.920.90 - 20.90 = 0.00618.121.718.10 - 21.70 = -3.60\n\n\n\n\n\n\n\\overline{x} = -2.2\ns = 1.3\n\\text{s.e.} = 0.53\n\n問題は、この差の平均値をどのように評価するのか。",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#平均値の分布",
    "href": "ttest.html#平均値の分布",
    "title": "2群の比較：t 検定",
    "section": "平均値の分布",
    "text": "平均値の分布\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nFigure 2: 求めた平均値と標準誤差から推定した正規分布。\n\n\n\n\n\n\n\n中心極限定理 5によると、平均値の分布は正規分布 6に従います。\n5 central limit theorem6 normal or gaussian distribution7 confidence interval8 95% confidence intervalFigure 2 に示した紫色の部分は 95% の確率密度です。 その幅は 信頼区間 7といいます。 有意水準を \\alpha = 0.05 として定義したとき、 この信頼区間は 95% 信頼区間 8といいます。\n\n\n\n\n\n\n\n\n信頼区間とは？\n\n\n\n[l, u] の区間を定義したとき、l は区間の下限、u は区間の上限です。 このように定義した区間は信頼区間といいます。\nでは、x に対する区間 [l,u] は 1-\\alpha の確率で次のように定義できます。\n\nP(l \\le x \\le u) = 1-\\alpha\n\n\\overline{x} が標本平均であれば、z値 9と呼ぶ統計量を定義できます。\n\nz = \\frac{\\overline{x}-\\mu}{\\sigma}\n\n\\mu は母平均、\\sigma は母分散です。\nつまり、下限と上限を求めるためには\n\nP(l \\le z \\le u) = 1-\\alpha\n\nを解けばいい。\n中心極限定理は次の通りに定義されています。\n\n\\lim_{n\\rightarrow\\infty} \\sqrt{n}\\overbrace{\\left(\\frac{\\overline{x}_n-\\mu}{\\sigma}\\right)}^\\text{この部分は z 値}  \\xrightarrow{d} N(0, 1)\n\nよって、 \\alpha = 0.05　のときの [l, u] は次の通りです。\n\nP\\left(l \\le z \\le u \\right) = 1-0.05 = 0.95\n\n標準化正規分布 N(0,1) のとき、\n\n\\alpha/2=0.05/2=0.025 分位数は l です。\n1-\\alpha/2=1-0.05/2=0.975 分位数は u です。\n\n\n\n9 z-score",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#z-値の分位数を求める",
    "href": "ttest.html#z-値の分位数を求める",
    "title": "2群の比較：t 検定",
    "section": "z 値の分位数を求める",
    "text": "z 値の分位数を求める\n\n\n\n\n\n\n\n\nFigure 3: 標準化正規分布\n\n\n\n\n\n\n[-1 s, 1 s] は 68.3% 区間\n[-2 s, 2 s] は 95.4% 区間\n[-3 s, 3 s] は 99.7% 区間\n\n\n\n\n\nTable 4: 標準化正規分布の分位数\n\n\n\nSignififance levelpercent± quantile0.5000000000050.000000.67448980.3173105078668.268951.00000000.2000000000080.000001.28155160.1000000000090.000001.64485360.0500000000095.000001.95996400.0455002639095.449972.00000000.0250000000097.500002.24140270.0026997960699.730023.00000000.0000633424899.993674.0000000",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#信頼区間の求め方",
    "href": "ttest.html#信頼区間の求め方",
    "title": "2群の比較：t 検定",
    "section": "信頼区間の求め方",
    "text": "信頼区間の求め方\n平均値は \\overline{x}_{A-B} = 21.142 です。 標準誤差は \\text{s.e.} = 0.431 です。 母分散は \\sigma_A = \\sigma_B = 1 です。 有意水準は \\alpha = 0.05 とします。\n95% 信頼区間は次のように定義しています。 \nP\\left(l \\le \\frac{\\overline{x}-\\mu}{\\sigma}\\le u\\right) = 1-\\alpha = 0.95\n\n書き直すと次のとおりです。\n\nP\\left(\\overline{x} +l \\sigma \\le \\mu \\le \\overline{x} + u\\sigma\\right) = 1-\\alpha = 0.95\n\n\\alpha= 0.05　のとき、 l= -1.96 と u= 1.96 です。\n母分散は先程定義しましたが、\\sigma = 1 です。 それぞれの値を式に代入すると、次のとおりです。\n\n\\begin{split}\nP(\n21.142 +  -1.96 \\times 1\n\\le x \\le\n21.142 +  1.96 \\times 1\n) &=\nP(\n\\overbrace{19.182}^{l}\n\\le x \\le\n\\overbrace{23.102}^{u}\n) \\\\\n&= 0.95\n\\end{split}\n\nつまり、 \\overline{x}= 21.142 の 95% 信頼区間は [19.182, 23.102 ] です。\n\n\n\n\n\n\n\n\nFigure 4: 調査を 20 回行ったときの平均値と信頼区間。真の平均値は -2 です。このとき、全ての実験で求めた信頼区間内に真の平均値が存在します。\n\n\n\n\n\n信頼区間内に 0 が含まれるときの、帰無仮説は棄却できません。 ちなみに、このときの帰無仮説は「平均値はゼロ」です。\n\n\n\n\n\n\n\n\nFigure 5: ゼロを含む信頼区間。\n\n\n\n\n\n\n真の平均値は -2 なので、仮定した帰無仮説はそもそも誤りです。\nH_0 を棄却しなかったら、 第2種の誤り 10がおきます。\n8 つの調査の 95% 信頼区間は 0 を含みます。つまり、第2種の誤りは \\beta= 8 / 20 = 0.4 (40%) です。\nこの解析の検出力 (1 - \\beta) は 0.6 です。正しい結果に導く確率は 60% です。\n\n10 Type-II error",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#解析は誤りです",
    "href": "ttest.html#解析は誤りです",
    "title": "2群の比較：t 検定",
    "section": "解析は誤りです!",
    "text": "解析は誤りです!\nz 値は正規分布に従いますが、このとき母平均と母分散は存知です。\n\nz = \\frac{\\overline{x} - \\mu}{\\sigma}\\sim N(0,1)\n\n++ところが、一般的には母平均と母分散は未知です。** 一般的には z 値より、t 値を求めます。\n\nt_{\\overline{x}} = \\frac{\\overline{x} - x_0}{s.e.} = \\frac{\\overline{x} - x_0}{s / \\sqrt{n}}\n\nt 値は t 分布に従います。\n\n\n\n\n\n\n\n\nFigure 6: ｔ分布\n\n\n\n\n\nこの t 分布の 自由度 11は N-1 = 5 です。\n11 degrees-of-freedom\n\n\n\nTable 5: 自由度 5 のときに t 分布の分位数\n\n\n\nSignififance levelpercent± quantile0.5000000050.000000.72668680.3632174763.678251.00000000.2000000080.000001.47588400.1019394889.806052.00000000.1000000090.000002.01504840.0500000095.000002.57058180.0300992596.990083.00000000.0250000097.500003.16338140.0103234298.967664.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: 母分散が未知のときの結果\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFigure 7 は 20 の標本平均とそれぞれの 95% 信頼区間を示しています。 真の平均値が頼区間に含まれている実験は紫色で示しています。 20 の調査のうち、新の平均値を含む回数は 19 回です。\n信頼区間の解釈について\n基本的には、95%　信頼区間を次のように理解できる。 実験を 100 回行い、信頼区間内に真の平均値が含まれる回数は 95 回です。。\n下記で述べた解釈はすべて誤りです。\n\n信頼区間に真の平均値が存在する。\n95% の確率で真の平均値が信頼区間に含まれる。\n95% の確率で次の実験の平均値が信頼区間に含まれる。\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: 信頼区間にゼロが含まれる回数\n\n\n\n\n\n\n信頼区間に 0 を含む実験は 5つあるので、 \\beta= 5 / 20 = 0.25 (25%) です。\nこの実験の検出力 (1 - \\beta) は 0.75　です。",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#対応ありの-t-検定",
    "href": "ttest.html#対応ありの-t-検定",
    "title": "2群の比較：t 検定",
    "section": "対応ありの t 検定",
    "text": "対応ありの t 検定\n\n\n\n\n\n\n\n\nFigure 9: 対応ありの t 検定\n\n\n\n\n\nこのときの第２種の誤りををこす確率は \\beta = 5 / 20 = 25% です。 検出力　(1-\\beta) は 0.75です。",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#分散が異なる-t-検定",
    "href": "ttest.html#分散が異なる-t-検定",
    "title": "2群の比較：t 検定",
    "section": "分散が異なる t 検定",
    "text": "分散が異なる t 検定\n\n\n\n\n\n\n\n\nFigure 10: 分散が異なる t 検定（ウェルチの t 検定ともよびます）\n\n\n\n\n\nこのときの第２種の誤りををこす確率は \\beta = 1 / 20 = 5% です。\n検出力 (1-\\beta) はs 0.95 です。",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#対応ありの-t-検定-1",
    "href": "ttest.html#対応ありの-t-検定-1",
    "title": "2群の比較：t 検定",
    "section": "対応ありの t 検定",
    "text": "対応ありの t 検定\n対応ありのt検定 (paired t-test)\nt 検定の統計量は t 値です。\n\nt^* = \\frac{\\overline{x}_{A-B} - \\mu}{s_{A-B} / \\sqrt{n}}\n\n対応ありの t 検定の自由度は n-1　です。\n観測値がペアとして対応しているときに使います。 たとえば、低い光環境で育て海藻を高い光環境に移した時の成長速度の差を比較するときに使います。",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#標本の-t-検定",
    "href": "ttest.html#標本の-t-検定",
    "title": "2群の比較：t 検定",
    "section": "2標本の t 検定",
    "text": "2標本の t 検定\n2標本 （２群）t 検定には 2 種類あります。\n等分散の t 検定 (equal variance t-test)\n\nt^* = \\frac{\\overline{x}_A - \\overline{x}_B}{s_p \\sqrt{1 / n_A + 1/n_B}}\n \ns_p = \\sqrt{\n\\frac{(n_A-1)s_A^2 + (n_B-1)s_B^2}\n{n_A + n_B -2}}\n 自由度は n_A + n_B - 2　です。\n不等分散の t 検定・ウェルチの t 検定 (Unequal variance, Welch’s t-test)\n\nt^* = \\frac{\\overline{x}_A - \\overline{x}_B}{s_p}\n\n\ns_p = \\sqrt{\n\\frac{s_A^2}{n_A} +\n\\frac{s_B^2}{n_B}}\n 自由度はウェルチ–サタスウェイトの式 (Welch-Satterthwaite Equation) で求めます。\ns は標準偏差、 n サンプル数、 \\overline{x} は平均値、 t^* は t 値。\n\n\\text{degrees-of-freedom} =\n\\frac{\n\\left(\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}\\right)^2\n}\n{\\frac{\\left(s_A^2 / n_A\\right)^2}{n_A-1} + \\frac{\\left(s_B^2 / n_B\\right)^2}{n_B-1}}\n\ndegrees-of-freedom は自由度です。",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#ノコギリモク幼体の幅に対する-t-検定",
    "href": "ttest.html#ノコギリモク幼体の幅に対する-t-検定",
    "title": "2群の比較：t 検定",
    "section": "ノコギリモク幼体の幅に対する t 検定",
    "text": "ノコギリモク幼体の幅に対する t 検定\n\n\n対応ありの t 検定\n\n\\begin{aligned}\nt^* &= \\frac{\\overline{x}_{A-B} - \\mu}{s_{A-B} / \\sqrt{n}} \\\\\nt^* &= \\frac{-2.467}{2.642 / \\sqrt{6}} \\\\\nt^* &= -2.287\n\\end{aligned}\n\n\n\n\n\\overline{x}_{A-B}= -2.467\ns_{A-B}= 2.642\n\\mu=0\nn = 6\n\\alpha = 0.05\nt値: -2.287\nP値: 0.071\n\n\n\nノコギリモク幼体のデータはお互いに対応していないので、対応ありの t 検定の結果は誤りです。\n\n\nノコギリモク幼体の正しい解析はウェルチの t 検定です。\n\n\\begin{aligned}\nt^* &= \\frac{\\overline{x}_A -\\overline{x}_B}{s_p} \\\\\ns_p &= \\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}} \\\\\ns_p &= \\sqrt{\\frac{1.995^2}{6} + \\frac{1.961^2}{6}} \\\\\nt^* &= \\frac{10.05 - 12.517}{1.142} \\\\\nt^* &= -2.16 \\\\\n\\text{d.f.} &= 9.997\n\\end{aligned}\n\n\n\n\n\\alpha = 0.05\nt値-value: -2.16\nP値: 0.056\n\nP\\nless  \\alpha = 0.05 なので、帰無仮説は棄却できません。",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#サンプル数と-p-値の関係",
    "href": "ttest.html#サンプル数と-p-値の関係",
    "title": "2群の比較：t 検定",
    "section": "サンプル数と p 値の関係",
    "text": "サンプル数と p 値の関係\n\n\n\n\n\n分散等しい t 検定の場合、サンプル数が増えると第 2 種の誤りは減少し、検出力は増加します。第 1 種の誤りは変わりません。\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nsite A の真の平均値は \\mu = 20、site B のは \\mu = 22 です。 site A の真の平均値は \\mu = 20、site B のは \\mu = 22 です。 site A と site B の真の標準偏差 (\\sigma) は 1 と 1 です。",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#behavior-of-the-t-test-unequal-variance",
    "href": "ttest.html#behavior-of-the-t-test-unequal-variance",
    "title": "2群の比較：t 検定",
    "section": "Behavior of the t-test (unequal variance)",
    "text": "Behavior of the t-test (unequal variance)\n\n\n\n\n\n分散が異なる t 検定の場合、第 2 種の誤りと検出力の動きは分散が等しい t 検定と似ていますが、標本数も強く影響します。\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nsite A の真の平均値は \\mu = 20、site B のは \\mu = 22 です。 site A の真の標準偏差は \\mu = 1、ですが、site B の標準偏差は \\sigma_B = k\\times\\sigma_A です。 s_A / s_B \\rightarrow\\inftyのとき、第2種の誤りは増加し、検出力は減少します。 さらに、標本数が増えると、不等分散性の影響が下がります。",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#ウェルチ-t-検定の-r-出力と結果",
    "href": "ttest.html#ウェルチ-t-検定の-r-出力と結果",
    "title": "2群の比較：t 検定",
    "section": "ウェルチ t 検定の R 出力と結果",
    "text": "ウェルチ t 検定の R 出力と結果\n\n\n\n    Welch Two Sample t-test\n\ndata:  value by name\nt = -2.16, df = 9.9971, p-value = 0.05612\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -5.01124979  0.07791646\nsample estimates:\nmean in group A mean in group B \n       10.05000        12.51667 \n\n\n\n# パッケージの読み込み\nlibrary(tidyverse)\n\n# 疑似データの作成\nA = c(9.8,11.1,10.7,10.7,11.8,6.2)\nB = c(12.5,13.8,12.0,15.5,9.8,11.5)\ndata = tibble(A, B)\ndata = data %&gt;% pivot_longer(cols = c(A,B))\n\n# ウェルチ t 検定\nt.test(value ~ name, data = data)\n\n\n# ひと書き方\nt.test(A, B)\n\n\n# two-sample, equal variance t-test (等分散 t 検定)\nt.test(value ~ name, data = data, var.equal = TRUE)",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#パッケージの読み込み",
    "href": "ttest.html#パッケージの読み込み",
    "title": "2群の比較：t 検定",
    "section": "パッケージの読み込み",
    "text": "パッケージの読み込み\nt 検定だけしたいなら、次のパッケージの読み込みは不要です。 そう言っても、自分のワークフローでは、つぎのパッケージは必ず読み込んでいます。 パッケージを読み込もうとしたときに、 Error in library(tidyverse) : there is no package called 'tidyverse' のようなエラーがでたら、パッケージのインストールが必要です。\nパッケージのインストールは次のようにできます。\n\ninstall.packages(\"tidyverse\")\n\nでは、パッケージを読み込みます。\n\nlibrary(tidyverse)  # データの操作・処理・作図用メタパッケージ\nlibrary(readxl) 　　# xlsx ファイルの読み込み用\nlibrary(lubridate)　# 時刻データ用\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#データの準備",
    "href": "ttest.html#データの準備",
    "title": "2群の比較：t 検定",
    "section": "データの準備",
    "text": "データの準備\nデータは CSV (Comma Separated Value; コンマ区切り) ファイルに保存しています。 ファイルの内容は次の通りです。 最初の 3 行にはファイルの説明があります。\n\n\n# 6 Sargassum macrocarpum individuals from 2 sites were measured.\n# site: is the collection site (A, B).\n# size: is the width of the individual in mm.\nsite,size\nA,19.9\nA,20.6\nA,20.3\nA,20.4\nA,20.9\nA,18.1\nB,22.3\nB,22.9\nB,22\nB,23.7\nB,20.9\nB,21.7\n\n\nでは、データを R に読み込みます。\n\nfilename = \"./_data/sargassum_t-test_dataset.csv\"\ndset = read_csv(filename)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 15 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): # 6 Sargassum macrocarpum individuals from 2 sites were measured.\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n読み込んだデータの内容を確認しましょう。\n\ndset\n\n# A tibble: 15 × 1\n   `# 6 Sargassum macrocarpum individuals from 2 sites were measured.`\n   &lt;chr&gt;                                                              \n 1 # site: is the collection site (A, B).                             \n 2 # size: is the width of the individual in mm.                      \n 3 site,size                                                          \n 4 A,19.9                                                             \n 5 A,20.6                                                             \n 6 A,20.3                                                             \n 7 A,20.4                                                             \n 8 A,20.9                                                             \n 9 A,18.1                                                             \n10 B,22.3                                                             \n11 B,22.9                                                             \n12 B,22                                                               \n13 B,23.7                                                             \n14 B,20.9                                                             \n15 B,21.7                                                             \n\n\n説明があるので、読み込みに失敗しました。 読み込んだデータのクラス (class) は 15 行 1 列の tibble になっています。 2 列あるはずです。 この場合、read_csv() に説明を無視させないといけない。\nskip = 3 を read_csv() に渡せば、最初の 3 行をスキップできます。\n\nfilename = \"./_data/sargassum_t-test_dataset.csv\"\ndset = read_csv(filename, skip = 3)\n\nRows: 12 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): site\ndbl (1): size\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndset\n\n# A tibble: 12 × 2\n   site   size\n   &lt;chr&gt; &lt;dbl&gt;\n 1 A      19.9\n 2 A      20.6\n 3 A      20.3\n 4 A      20.4\n 5 A      20.9\n 6 A      18.1\n 7 B      22.3\n 8 B      22.9\n 9 B      22  \n10 B      23.7\n11 B      20.9\n12 B      21.7\n\n\n12 行 2 列の tibble になりました。 列1の列名は site 列２の列名は size です。",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#まずデータの平均値や標準偏差などをもとめる",
    "href": "ttest.html#まずデータの平均値や標準偏差などをもとめる",
    "title": "2群の比較：t 検定",
    "section": "まずデータの平均値や標準偏差などをもとめる",
    "text": "まずデータの平均値や標準偏差などをもとめる\nsite ごとの size の平均値、標準偏差、サンプル数、標準誤差は tidyverse パッケージの解析システムをつかいます。\n\ndset |&gt; \n  group_by(site) |&gt; \n  summarise(across(size, list(mean = mean, sd = sd, n = length))) |&gt; \n  mutate(size_se = size_sd / sqrt(size_n))\n\n# A tibble: 2 × 5\n  site  size_mean size_sd size_n size_se\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;int&gt;   &lt;dbl&gt;\n1 A          20.0   1.00       6   0.410\n2 B          22.2   0.971      6   0.396",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#t-検定-1",
    "href": "ttest.html#t-検定-1",
    "title": "2群の比較：t 検定",
    "section": "t 検定",
    "text": "t 検定\nt検定は t.test() でやります。\n\nt.test(size ~ site, data = dset)\n\n\n    Welch Two Sample t-test\n\ndata:  size by site\nt = -3.8886, df = 9.9893, p-value = 0.003022\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -3.486976 -0.946357\nsample estimates:\nmean in group A mean in group B \n       20.03333        22.25000 \n\n\nt.test() の結果をオブジェクトに書き込んだら、t値 (t value)、p値 (p value)、自由度 (degrees of freedom) を抽出できます。\n\ndset_test = t.test(size ~ site, data = dset)\ndset_test$statistic  # t value\n\n        t \n-3.888623 \n\ndset_test$parameter  # degrees of freedom \n\n      df \n9.989348 \n\ndset_test$p.value 　 # p value\n\n[1] 0.003022351",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#結果",
    "href": "ttest.html#結果",
    "title": "2群の比較：t 検定",
    "section": "結果",
    "text": "結果\n\nノコギリモクの幼体において、サイト A から採取した幼体の幅（平均値±標準誤差）は 20.03 ± 0.41 mm でしたが、 サイト B から採取した幼体の幅は 22.25 ± 0.40 mm でした。 ｔ検定の結果、両地点で幼体幅間に有意な差がみられた (t(9.99) = -3.889; P = 0.0030)。\n\n有意水準より低いP値は「P &lt; 0.05」のように書くことも有ります。 つまり、「ｔ検定の結果、両地点で幼体幅間に有意な差がみられた (t(9.99) = -3.889; P &lt; 0.05)」。\nt検定の結果を記述することが重要です。この 3 つの情報を必ず記述しましょう。\n\nt(9.99): 検定に使用した自由度（サンプル数の目安）\n-3.889: t検定の統計量\nP = 0.0030: 結果のP値",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#付録-a-等分散性と正規性の検定",
    "href": "ttest.html#付録-a-等分散性と正規性の検定",
    "title": "2群の比較：t 検定",
    "section": "付録 A: 等分散性と正規性の検定",
    "text": "付録 A: 等分散性と正規性の検定\nデータの正規性と等分散性の検証も必要であれば Levene Test と Shapiro-Wilk Normality Test があります。 Levene Test は car パッケージの leveneTest() 関数でできますが、Shapiro-Wilk Normality Test はベースR に あるので、 パッケージの読み込みは必要ないです。\n等分散性の検定\nLevene Test (ルビーン検定) は2群以上の分散の均質性 (homogeneity) を検定するための検定です。 ルビーン検定の帰無仮説は「各群の分散は等しい」です。 有意水準より低いP値を求めたら、帰無仮説を棄却します。 棄却した場合、各群は均一な分散ではありません。\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nleveneTest(size ~ site, data = dset)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1   0.079 0.7844\n      10               \n\n\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nルビーン検定の統計量は F 値です。 データの等分散性を確認したところ、 F(1,10) = 0.08、 P値は P = 0.7844です。 有意水準より大きいので、帰無仮説を棄却しません。 つまり、等分散性ではないといえません。\n正規性の検定\nShapiro-Wilk Normality Test (シャピロ–ウィルク検定) の帰無仮説は「サンプルが正規分布に従う母集団からとれた」です。 つまりシャピロウィルク検定から得たP値はサンプルの正規性を評価する指標です。 帰無仮説検定論の場合、有意水準より低いP値は帰無仮説を棄却することになり、センプルは正規分布に従わない母集団から得たものだと考えられるようになる。\n\nshapiro.test(dset$size)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dset$size\nW = 0.97575, p-value = 0.9608\n\n\nシャピロウィルク検定の統計量は W値です。 W =0.98、 P = ので、 帰無仮説を棄却しません。 正規性に従わないといえません。\n一般的な手順のコード\n\nlibrary(tidyverse)\nlibrary(car)\nfilename = \"./_data/sargassum_t-test_dataset.csv\"\ndset = read_csv(filename, skip = 3)\n\n# (1) データの可視化\nggplot(dset) + \n  geom_point(aes(x = site, y = size)) +\n  labs(y = \"Width (mm)\",\n       x = \"Site\")\n\n# (2) 等分散性の確認\nleveneTest(size ~ site, data = dset)\n\n# (3) 正規性の確認\nshapiro.test(dset$size)\n\n# (4) t検定\nt.test(size ~ site, data = dset)",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "ttest.html#付録-b-本資料のデータ作成",
    "href": "ttest.html#付録-b-本資料のデータ作成",
    "title": "2群の比較：t 検定",
    "section": "付録 B: 本資料のデータ作成",
    "text": "付録 B: 本資料のデータ作成\n資料に使ったデータは次のコードでつくれます。\n\nlibrary(tidyverse)\nset.seed(2021)\nnA = 6\nnB = 6\nmeanA = 20\nmeanB = 22\nsigmaA = 1\nsigmaB = 1\ngroupA = rnorm(nA, meanA, sigmaA) |&gt;  round(digits = 1)\ngroupB = rnorm(nB, meanB, sigmaB) |&gt;  round(digits = 1)\ndset   = tibble(site = c(\"A\", \"B\"), size = list(groupA, groupB)) |&gt;  unnest(size)\nL1 = \"# 6 Sargassum macrocarpum individuals from 2 sites were measured.\"\nL2 = \"# site: is the collection site (A, B).\"\nL3 = \"# size: is the width of the individual in mm.\"\nfname = \"sargassum_t-test_dataset.csv\"\nwrite_lines(file = fname, list(L1, L2, L3))\nwrite_csv(dset, file = fname, append = TRUE, col_names = TRUE)",
    "crumbs": [
      "線形モデル",
      "2群の比較：t 検定"
    ]
  },
  {
    "objectID": "linearmodels.html",
    "href": "linearmodels.html",
    "title": "ノンパラメトリック法",
    "section": "",
    "text": "Non-parametric statistics1 are a type of statistics that do not assume data is drawn from a probability distribution2.",
    "crumbs": [
      "線形モデル",
      "ノンパラメトリック法"
    ]
  },
  {
    "objectID": "linearmodels.html#example-data",
    "href": "linearmodels.html#example-data",
    "title": "ノンパラメトリック法",
    "section": "Example data",
    "text": "Example data\n\n\nCode\ntitle = \"生データ（左）と順位つけデータ（右）\"\np1 = iris |&gt; \n  ggplot() + geom_point(aes(x = Petal.Length, y = Petal.Width)) +\n  ggtitle(\"Unranked values\")\n\np2 = iris |&gt; \n  mutate(across(matches(\"Petal\"), rank)) |&gt; \n  ggplot() + geom_point(aes(x = Petal.Length, y = Petal.Width)) +\n  ggtitle(\"Ranked values\")\np1 + p2 + \n  plot_annotation(title = title)\n\n\n\n\n\nThe unranked and ranked data from the Iris dataset. Note the change in shape",
    "crumbs": [
      "線形モデル",
      "ノンパラメトリック法"
    ]
  },
  {
    "objectID": "linearmodels.html#correlation",
    "href": "linearmodels.html#correlation",
    "title": "ノンパラメトリック法",
    "section": "Correlation",
    "text": "Correlation\nThe Pearson correlation coefficient3 describes the linear (i.e., straight-line) relationship between two variables. The Spearman correlation describes the monotonic relationship between two variables (Lee Rodgers and Nicewander 1988).\n3 ピアソン相関係数\nLee Rodgers, Joseph, and W. Alan Nicewander. 1988. “Thirteen Ways to Look at the Correlation Coefficient.” The American Statistician 42 (1): 59–66. https://doi.org/10.1080/00031305.1988.10475524.\nPearson correlation\n\nr_{xy} = \\frac{\\sum (x_i - \\overline{x})(y_i - \\overline{y})}{\n\\sqrt{\\sum (x_i - \\overline{x})^2}\\sqrt{\\sum (y_i - \\overline{y})^2}\n} =\n\\frac{S_{xy}}{S_xS_y}\n\nThe Pearson correlation is bounded by -1 and 1 (-1 \\le r_{xy} \\le 1). S_{xy} is the covariance, S_{xx} and S_{yy} are the variances of x and y, respectively.\nManually calculating the Pearson correlation.\n\nx = iris$Petal.Length\ny = iris$Petal.Width\n\nmeanx = mean(x)\nmeany = mean(y)\n\nnumerator = sum((x - meanx) * (y - meany))\ndenominator = sqrt(sum((x - meanx)^2)) * sqrt(sum((y - meany)^2))\nnumerator / denominator\n\n[1] 0.9628654\n\n\nCalculate the Pearson correlation using cor().\n\ncor(x, y, method = \"pearson\")\n\n[1] 0.9628654\n\n\nConduct the Pearson’s product-moment correlation test.\n\ncor.test(x, y, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = 43.387, df = 148, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9490525 0.9729853\nsample estimates:\n      cor \n0.9628654 \n\n\nSpearman rank correlation\n\n\\rho = r_{s} = \\frac{\\sum (R(x_i) - \\overline{R(x)})(R(y_i) - \\overline{R(y)})}{\n\\sqrt{\\sum (R(x_i) - \\overline{R(x)})^2}\\sqrt{\\sum (R(y_i) - \\overline{R(y)})^2}\n}\n\nThe Spearman’s rank correlation coefficient4 is also bounded by -1 and 1 (-1 \\le r_{s} \\le 1). R(x) indicates taking the rank of x.\n4 スピアマン順位相関係数Manually calculate the Spearman’s rank correlation.\n\nx = iris$Petal.Length |&gt; rank()\ny = iris$Petal.Width  |&gt; rank()\n\nmeanx = mean(x)\nmeany = mean(y)\n\nnumerator = sum((x - meanx) * (y - meany))\ndenominator = sqrt(sum((x - meanx)^2)) * sqrt(sum((y - meany)^2))\nnumerator / denominator\n\n[1] 0.9376668\n\n\nCalculate the Spearman’s rank correlation using cor().\n\ncor(x, y, method = \"spearman\")\n\n[1] 0.9376668\n\n\nTest the Spearman’s rank correlation.\n\ncor.test(x, y, method = \"spearman\", exact = F)\n\n\n    Spearman's rank correlation rho\n\ndata:  x and y\nS = 35061, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.9376668 \n\n\nCompare the two correlations coefficients\nComparing the performance of the two correlations using simulation. Assume that the Pearson correlation coefficient is 0.5.\nSample from a multivariate normal distribution.\n\nset.seed(2020)\nSxy = 0.5\nZ = MASS::mvrnorm(n = 10000, \n                  mu = c(0,0),\n                  Sigma = matrix(c(1, Sxy, Sxy, 1), ncol = 2))\n\n\n\nCode\nZ |&gt; as_tibble(.name_repair = ~c(\"V1\", \"V2\")) |&gt; \n  slice_sample(n = 5000) |&gt; \n  ggplot() + geom_point(aes(x = V1, y = V2), alpha = 0.2)\n\n\n\n\n\nMultivariate normal distribution with a mean of (0,0) and a covariance matrix of \\Sigma = \\begin{bmatrix}1&0.5\\\\0.5&1\\end{bmatrix}.\n\n\n\n\nCalculate the Pearson and Spearman rank correlation for n samples. The correlation coefficient should converge to 0.5.\n\nn = c(\n  seq(10, 100, by = 10),\n  seq(100, 1000 - 100, by = 100),\n  seq(1000, 10000 - 1000, by = 1000)\n  )\nsample_from_z = function(n, z, method) {\n    z = z[sample(1:nrow(z), n), ]\n    cor(z[ ,1], z[ ,2], method = method)\n}\ndout = \n  tibble(n = n) |&gt; group_by(n) |&gt; \n  mutate(pearson = map_dbl(n, sample_from_z, z = Z, method = \"pearson\")) |&gt; \n  mutate(spearman = map_dbl(n, sample_from_z, z = Z, method = \"spearman\")) \n\n\n\nCode\ndout |&gt; \n  ggplot() + \n  geom_hline(yintercept = Sxy)  +\n  geom_point(aes(x = n, y = pearson, color = \"Pearson\")) +\n  geom_point(aes(x = n, y = spearman, color = \"Spearman\")) +\n  geom_line(aes(x = n, y = pearson, color = \"Pearson\")) +\n  geom_line(aes(x = n, y = spearman, color = \"Spearman\"))  +\n  scale_color_viridis_d(\"\", end = 0.8) + \n  scale_y_continuous(\"Correlation coefficient\", limits = c(0, 1)) +\n  scale_x_continuous(\"Samples (n)\") +\n  theme(legend.position = c(1,1),\n        legend.justification = c(1,1),\n        legend.background = element_blank())\n\n\n\n\n\nThe Pearson and Spearman rank correlation coefficients for n samples The thick line indicates the true correlation coefficient.",
    "crumbs": [
      "線形モデル",
      "ノンパラメトリック法"
    ]
  },
  {
    "objectID": "linearmodels.html#parametric-and-non-parametric-tests",
    "href": "linearmodels.html#parametric-and-non-parametric-tests",
    "title": "ノンパラメトリック法",
    "section": "Parametric and non-parametric tests",
    "text": "Parametric and non-parametric tests\n\nOne-sample test\nParametric test\nThe (one-sample) t-test.\n\nt.test(Petal.Length ~ 1, data = iris)\n\n\n    One Sample t-test\n\ndata:  Petal.Length\nt = 26.073, df = 149, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 3.473185 4.042815\nsample estimates:\nmean of x \n    3.758 \n\n\nThe lm() function can also be used to conduct the test.\n\nlm(Petal.Length ~ 1, data = iris) |&gt; summary()\n\n\nCall:\nlm(formula = Petal.Length ~ 1, data = iris)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.758 -2.158  0.592  1.342  3.142 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.7580     0.1441   26.07   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.765 on 149 degrees of freedom\n\n\nNon-parametric test\nThe (one-sample) Wilcoxon test\n\nwilcox.test(iris$Petal.Length)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  iris$Petal.Length\nV = 11325, p-value &lt; 2.2e-16\nalternative hypothesis: true location is not equal to 0\n\n\nAlternative version, however we need to determined the signed ranks5 of the observations.\n5 符号順位\nsigned_rank = function(x) sign(x) * rank(abs(x))\n\n\nlm(signed_rank(Petal.Length) ~ 1, data = iris) |&gt; \n  summary()\n\n\nCall:\nlm(formula = signed_rank(Petal.Length) ~ 1, data = iris)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -74.5  -34.5    0.5   37.0   74.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   75.500      3.544   21.31   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 43.4 on 149 degrees of freedom\n\n\n\n\nTwo-sample test\nParametric test\nThis is the Welch’s t-test.\n\ntestdata = iris |&gt; filter(!str_detect(Species, \"setosa\"))\nt.test(Petal.Length ~ Species, data = testdata)\n\n\n    Welch Two Sample t-test\n\ndata:  Petal.Length by Species\nt = -12.604, df = 95.57, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -1.49549 -1.08851\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   4.260                    5.552 \n\n\nThe lm() version of the test.\n\nlm(Petal.Length ~ Species, data = testdata) |&gt; summary()\n\n\nCall:\nlm(formula = Petal.Length ~ Species, data = testdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.260 -0.360  0.044  0.340  1.348 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       4.26000    0.07248   58.77   &lt;2e-16 ***\nSpeciesvirginica  1.29200    0.10251   12.60   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5125 on 98 degrees of freedom\nMultiple R-squared:  0.6185,    Adjusted R-squared:  0.6146 \nF-statistic: 158.9 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nNon-parametric test\nThis is the Mann-Whitney U test (i.e., two-sample Wilcoxon test). One important consideration of this test is shape of the distributions for each group. If the groups have the same shape, then the Mann-Whitney U test is a test of differences in the medians. If the groups have different shapes, then the Mann-Whitney U test is a test of differences in the distributions.\n\nwilcox.test(Petal.Length ~ Species, data = testdata)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  Petal.Length by Species\nW = 44.5, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe lm() version using the signed_rank() function.\n\ntestdata = testdata |&gt; mutate(Petal.Length_srank = signed_rank(Petal.Length))\nlm(Petal.Length_srank ~ Species, data = testdata) |&gt; summary()\n\n\nCall:\nlm(formula = Petal.Length_srank ~ Species, data = testdata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-41.11 -12.18   0.25  12.61  36.11 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        26.390      2.260   11.68   &lt;2e-16 ***\nSpeciesvirginica   48.220      3.196   15.09   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.98 on 98 degrees of freedom\nMultiple R-squared:  0.699, Adjusted R-squared:  0.696 \nF-statistic: 227.6 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nRegarding the U-test\n\nx = rpois(1000, 2) + 5\ny = 14 - x\ndata = tibble(group = c(\"A\", \"B\")) |&gt; \n  mutate(obs = list(x, y)) |&gt; \n  unnest(obs)\n\ndata |&gt; group_by(group) |&gt; summarise(m = median(obs))\n\n# A tibble: 2 × 2\n  group     m\n  &lt;chr&gt; &lt;dbl&gt;\n1 A         7\n2 B         7\n\n\n\nggplot(data) + \n  geom_histogram(aes(x = obs, fill = group),\n                 position = position_dodge())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nwilcox.test(obs ~ group, data = data)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  obs by group\nW = 483552, p-value = 0.1923\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\nPerformance evaluation\nEvaluating the performance of the two-sample tests when the effect size is 1 and the standard deviations are 1.\n\n\nCode\ntestdata = \n  tibble(group = c(\"A\", \"B\"),\n         mu = c(2, 3),\n         sd = c(1, 1)) |&gt; \n  group_by(group) |&gt; \n  mutate(obs = map2(mu, sd, rnorm, n = 500)) |&gt; unnest(obs)\n\ntd2 = \ntibble(n = 3:200) |&gt; \n  mutate(out = map(n, function(n, data) {\n    df = data |&gt; group_by(group) |&gt; slice_sample(n = n)\n    tout = t.test(obs ~ group, data = df)$p.value\n    uout = wilcox.test(obs ~ group, data = df, exact = F)$p.value\n    tibble(toutp = tout, uoutp = uout)\n  }, data = testdata)) |&gt; \n  unnest(out)\n\nggplot(td2) + \n  geom_point(aes(x = n, y = toutp, color = \"t-test\")) +\n  geom_point(aes(x = n, y = uoutp, color = \"U-test\")) +\n  geom_line(aes(x = n, y = toutp, color = \"t-test\")) +\n  geom_line(aes(x = n, y = uoutp, color = \"U-test\")) +\n  scale_color_viridis_d(\"\", end = 0.8) + \n  scale_y_continuous(\"P-value\", limits = c(0, 1)) +\n  scale_x_continuous(\"Samples (n)\") +\n  theme(legend.position = c(1,1),\n        legend.justification = c(1,1),\n        legend.background = element_blank())\n\n\n\n\n\nWhen sample size is large, the P values convege to zero. Note that either test performs well, especially since the data is from a normal distribution.\n\n\n\n\nEvaluating the performance of the two-sample tests when the effect size is 1 and the standard deviations are 1 and 5.\n\n\nCode\ntestdata = \n  tibble(group = c(\"A\", \"B\"),\n         mu = c(2, 3),\n         sd = c(1, 5)) |&gt; \n  group_by(group) |&gt; \n  mutate(obs = map2(mu, sd, rnorm, n = 500)) |&gt; unnest(obs)\n\ntd2 = \n  tibble(n = 3:500) |&gt; \n  mutate(out = map(n, function(n, data) {\n    df = data |&gt; group_by(group) |&gt; slice_sample(n = n)\n    tout = t.test(obs ~ group, data = df)$p.value\n    uout = wilcox.test(obs ~ group, data = df, exact = F)$p.value\n    tibble(toutp = tout, uoutp = uout)\n  }, data = testdata)) |&gt; \n  unnest(out)\n\nggplot(td2) + \n  geom_point(aes(x = n, y = toutp, color = \"t-test\")) +\n  geom_point(aes(x = n, y = uoutp, color = \"U-test\")) +\n  geom_line(aes(x = n, y = toutp, color = \"t-test\")) +\n  geom_line(aes(x = n, y = uoutp, color = \"U-test\")) +\n  scale_color_viridis_d(\"\", end = 0.8) + \n  scale_y_continuous(\"P-value\", limits = c(0, 1)) +\n  scale_x_continuous(\"Samples (n)\") +\n  theme(legend.position = c(1,1),\n        legend.justification = c(1,1),\n        legend.background = element_blank())\n\n\n\n\n\nWhen the variances are different, then both have similar difficulty detecting an effect. Large sample numbers are needed to reduce the P value to zero.\n\n\n\n\n\n\nANOVA like tests\nParametric test\nThe One-way Analysis of Variance (ANOVA).\n\nlm(Petal.Length ~ Species, data = iris) |&gt; summary.aov()\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  437.1  218.55    1180 &lt;2e-16 ***\nResiduals   147   27.2    0.19                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNon-parametric test\nThe Kruskal-Wallis rank sum test6. The test statistic7 is the \\chi^2 statistic8. The null hypothesis is that all of the groups have the same distribution9.\n6 クラスカル=ウォリス検定7 検定統計量8 カイ2乗9 すべてのグループで分布はは同じ\nkruskal.test(Petal.Length ~ Species, data = iris)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Petal.Length by Species\nKruskal-Wallis chi-squared = 130.41, df = 2, p-value &lt; 2.2e-16\n\n\nExamine the performance of the one-way ANOVA and the Kruskal-Wallis test for 3-level factor, where the coefficient of variation is 1 and the means are 1.5, 2.0 and 2.5.\n\n\nCode\ntestdata = \n  tibble(group = c(\"A\", \"B\", \"C\"),\n         mu = c(1.5, 2, 2.5)) |&gt; \n  mutate(sd = rep(1, n())) |&gt; \n  group_by(group) |&gt; \n  mutate(obs = map2(mu, sd, rnorm, n = 500)) |&gt; unnest(obs) |&gt; \n  ungroup()\n\ntd2 = \n  tibble(n = 3:500) |&gt; \n  mutate(out = map(n, function(n, data) {\n    df = data |&gt; group_by(group) |&gt; slice_sample(n = n)\n    tout = summary.aov(lm(obs ~ group, data = df))[[1]][5][[1]][1]\n    uout = kruskal.test(obs ~ group, data = df)$p.value\n    tibble(toutp = tout, uoutp = uout)\n  }, data = testdata)) |&gt; \n  unnest(out)\n\nggplot(td2) + \n  geom_point(aes(x = n, y = toutp, color = \"One-way ANOVA\")) +\n  geom_point(aes(x = n, y = uoutp, color = \"Kruskal-Wallis\")) +\n  geom_line(aes(x = n, y = toutp, color = \"One-way ANOVA\")) +\n  geom_line(aes(x = n, y = uoutp, color = \"Kruskal-Wallis\")) +\n  scale_color_viridis_d(\"\", end = 0.8) + \n  scale_y_continuous(\"P-value\") +\n  scale_x_continuous(\"Samples (n)\") +\n  theme(legend.position = c(1,1),\n        legend.justification = c(1,1),\n        legend.background = element_blank())\n\n\n\n\n\nWhen the variances are the same, both tests quickly converge to P = 0.\n\n\n\n\nExamine the performance of the one-way ANOVA and the Kruskal-Wallis test for 3-level factor, where the coefficient of variation is 3 and the means are 1.5, 2, 2.5.\n\n\nCode\ntestdata = \n  tibble(group = c(\"A\", \"B\", \"C\"),\n         mu = c(1.5, 2, 2.5)) |&gt; \n  mutate(cv = rep(2, n())) |&gt; \n  mutate(sd = mu * cv) |&gt; \n  group_by(group) |&gt; \n  mutate(obs = map2(mu, sd, rnorm, n = 500)) |&gt; unnest(obs) |&gt; \n  ungroup()\n\ntd2 = \n  tibble(n = 3:500) |&gt; \n  mutate(out = map(n, function(n, data) {\n    df = data |&gt; group_by(group) |&gt; slice_sample(n = n)\n    tout = summary.aov(lm(obs ~ group, data = df))[[1]][5][[1]][1]\n    uout = kruskal.test(obs ~ group, data = df)$p.value\n    tibble(toutp = tout, uoutp = uout)\n  }, data = testdata)) |&gt; \n  unnest(out)\n\nggplot(td2) + \n  geom_point(aes(x = n, y = toutp, color = \"One-way ANOVA\")) +\n  geom_point(aes(x = n, y = uoutp, color = \"Kruskal-Wallis\")) +\n  geom_line(aes(x = n, y = toutp, color = \"One-way ANOVA\")) +\n  geom_line(aes(x = n, y = uoutp, color = \"Kruskal-Wallis\")) +\n  scale_color_viridis_d(\"\", end = 0.8) + \n  scale_y_continuous(\"P-value\") +\n  scale_x_continuous(\"Samples (n)\") +\n  theme(legend.position = c(1,1),\n        legend.justification = c(1,1),\n        legend.background = element_blank())\n\n\n\n\n\nWhen the variances are different, then both have similar difficulty detecting an effect. Large sample numbers are needed to reduce the P value to zero.",
    "crumbs": [
      "線形モデル",
      "ノンパラメトリック法"
    ]
  },
  {
    "objectID": "ttest-slides.html#a-juvenile-sargassum-macrocarpum",
    "href": "ttest-slides.html#a-juvenile-sargassum-macrocarpum",
    "title": "Comparing two groups",
    "section": "A juvenile Sargassum macrocarpum",
    "text": "A juvenile Sargassum macrocarpum",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#a-random-sample",
    "href": "ttest-slides.html#a-random-sample",
    "title": "Comparing two groups",
    "section": "A random sample",
    "text": "A random sample\n\n\n\n\nTable 1: Size (mm) of juvenile Sargassum macrocarpum（ノコギリモク）.\n\n\n\n\n  \n\n\n\n\n\n\n\nThe table shows two groups of 6 juvenile Sargassum macrocarpum (ノコギリモク) that were sampled randomly. The two groups of juveniles are from two sites, A and B.\nEach sample is given an sample I.D. of 1 to 6.\nThese data were generated with the R function with a true mean (\\(\\mu\\)) of 20 and 22 for site A and B, respectively. The true standard deviation (\\(\\sigma\\)) is 1 and 1 for site A and B, respectively.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#lets-compare-the-two-groups",
    "href": "ttest-slides.html#lets-compare-the-two-groups",
    "title": "Comparing two groups",
    "section": "Let’s compare the two groups",
    "text": "Let’s compare the two groups\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean (\\(\\overline{x}\\)), standard deviation (\\(s\\)), and the standard error (s.e.) for juvenile S. macrocarpum from site A and B are:\n\n\\(\\overline{x}_A=\\) 20.033, \\(s_A=\\) 1.003, and s.e. = 0.41\n\\(\\overline{x}_B=\\) 22.25 and \\(s_B=\\) 0.971, and s.e. = 0.396",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#what-is-our-question",
    "href": "ttest-slides.html#what-is-our-question",
    "title": "Comparing two groups",
    "section": "What is our question?",
    "text": "What is our question?\n\nIf we want to statistically compare the size from the two sites, we need a question (i.e., a working hypothesis).\n\n\nWorking hypothesis (作業仮設): The size (width) of juvenile S. macrocarpum collected from site A and B are different.\n\n\nWe know that the means for site A and B are different, but the standard deviations and standard errors are similar.\n\n\\(\\overline{x}_A=\\) 20.033; \\(s=\\) 1.003; s.e. = 0.41\n\\(\\overline{x}_B=\\) 22.25; \\(s=\\) 0.971; s.e. = 0.396",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#define-our-hypotheses",
    "href": "ttest-slides.html#define-our-hypotheses",
    "title": "Comparing two groups",
    "section": "Define our hypotheses",
    "text": "Define our hypotheses\nLet’s formally define our statistical hypotheses.\n\n\\(H_0\\) (null hypothesis 帰無仮説): There is no difference in the paired values.\n\\(H_A\\) (alternative hypothesis 対立仮設): There is a difference in the paired values.\n\nOther alternative hypotheses\n\n\\(H_P\\) (alternative hypothesis): The difference in paired values is positive.\n\\(H_N\\) (alternative hypothesis): The difference in paired values is negative.\n\n\nImportant: We can define an infinite number of hypotheses.\nThe most common alternative hypothesis is a test for an effect. For example, \\(H_0\\): there is no effect. \\(H_A\\): There is an effect.\nNote: hypotheses is the plural form (複数形) of hypothesis.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#calculate-the-size-differences-among-pairs",
    "href": "ttest-slides.html#calculate-the-size-differences-among-pairs",
    "title": "Comparing two groups",
    "section": "Calculate the size differences among pairs",
    "text": "Calculate the size differences among pairs\nAssume that we can compare the paired differences (e.g., \\(x_{A,1} - x_{B,1}\\), \\(x_{A,2} - x_{B,2}\\), \\(x_{A,3} - x_{B,3}\\), \\(\\cdots\\), \\(x_{A,6} - x_{B,6}\\)).",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#recall-the-hypotheses",
    "href": "ttest-slides.html#recall-the-hypotheses",
    "title": "Comparing two groups",
    "section": "Recall the hypotheses",
    "text": "Recall the hypotheses\nThe two statistical hypotheses that we defined were:\n\n\\(H_0\\): There is no difference in the paired values.\n\\(H_A\\): There is a difference in the paired values.\n\nThe mean difference (\\(\\overline{x}_{A-B}\\)) is -2.217, the standard deviation (\\(s_{A-B}\\)) is 1.289, and the standard error (\\(\\text{s.e.}_{A-B}\\)) is 0.526\nNote: The true difference \\(\\mu_{A-B}\\) is -2, the true standard deviation \\(\\sigma_A = \\sigma_B\\) is 1.\n\nIf the samples can be paired, then the differences between site A and B are easy to determine.\nTherefore, it is easy to calculate the mean, standard deviation, and standard error.\n\\[\n\\begin{aligned}\n\\overline{x} &= \\frac{1}{n}\\sum x \\qquad \\text{mean} \\\\\ns &= \\sqrt{\\frac{1}{n-1} \\sum \\left(x-\\overline{x}\\right)^2} \\qquad \\text{standard deviation} \\\\\n\\text{s.e.} &= \\frac{s}{\\sqrt{n}} \\qquad \\text{standard error of the}\n\\end{aligned}\n\\]\nHow do we statistically test if the differences are significant?",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#distribution-of-the-mean",
    "href": "ttest-slides.html#distribution-of-the-mean",
    "title": "Comparing two groups",
    "section": "Distribution of the mean",
    "text": "Distribution of the mean\nRecall that the central limit theorem (中心極限定理) states that the distribution of the mean has a Gaussian (normal) distribution (正規分布).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\overline{x}_{A-B} =\\) -2.217 (mean)\n\\(s_{A-B}=\\) 1.289 (standard deviation)\ns.e.A-B = 0.526 (standard error)\n\nThe shaded area is the 95% probability region. The width of the shaded area is called a confidence interval (信頼区間). If the significance level (有意水準) is \\(\\alpha = 0.05\\), then the confidence interval is called a 95% confidence interval (95% 信頼区間).\n\n\n\nThe green vertical line is the sample mean (\\(\\overline{x}\\)).\nThe thick line and the shaded region indicates the 95% confidence interval of the sample mean.\nThe thin line indicates a Gaussian distribution with a sample mean of -2.217.\nThe standard deviation is 0.526, which is the standard error of the mean.\nWe use the standard error because we are interested in the distribution of the mean values. If we were interested in the distribution of the observations, then we would use the standard deviation.\nSince the value of zero is not included in the 95% confidence interval, we can reject the null hypothesis that \\(\\overline{x}_A = \\overline{x}_B\\) or \\(x_{A-B}=0\\).\nSo what is a confidence interval (信頼区間)?",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#developing-the-confidence-interval",
    "href": "ttest-slides.html#developing-the-confidence-interval",
    "title": "Comparing two groups",
    "section": "Developing the confidence interval",
    "text": "Developing the confidence interval\nThe confidence interval is an interval \\([l, u]\\) with a lower bound of \\(l\\) and an upper bound of \\(u\\).\nFor a probability \\(1-\\alpha\\), the interval \\([l, u]\\) for \\(x\\) is\n\\[\nP(l \\le x \\le u) = 1-\\alpha\n\\] If \\(\\overline{x}\\) is a sample mean, then the z-score (z値) is\n\\[\nz = \\frac{\\overline{x}-\\mu}{\\sigma}\n\\]\nwhere \\(\\mu\\) is the population mean and \\(\\sigma\\) is the population standard deviation. Then, to find \\(l\\) and \\(u\\), we need to solve\n\\[\nP(l \\le z \\le u) = 1-\\alpha\n\\]\nfor the interval \\([l, u]\\) of \\(z\\) given a probability of \\(1-\\alpha\\).",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#central-limit-theorem-中心極限定理",
    "href": "ttest-slides.html#central-limit-theorem-中心極限定理",
    "title": "Comparing two groups",
    "section": "Central limit theorem (中心極限定理)",
    "text": "Central limit theorem (中心極限定理)\nRecall that the central limit theorem states that:\n\\[\n\\lim_{n\\rightarrow\\infty} \\sqrt{n}\\overbrace{\\left(\\frac{\\overline{x}_n-\\mu}{\\sigma}\\right)}^{z}  \\xrightarrow{d} N(0, 1)\n\\]\nTherefore, for \\(\\alpha = 0.05\\), we can define an \\([l, u]\\)\n\\[\nP\\left(l \\le z \\le u \\right) = 1-0.05 = 0.95\n\\]\nFor the standard normal distribution (\\(N(0,1)\\))\n\n\\(l\\) is the \\(\\alpha/2=0.05/2=0.025\\) quantile.\n\\(u\\) is the \\(1-\\alpha/2=1-0.05/2=0.975\\) quantile.\n\n\nThe z-score appears in the definition of the central limit theorem.\n\\[\nz = \\frac{\\overline{x}-\\mu}{\\sigma}\n\\]\nAs the number of samples increase, the distribution of \\(\\sqrt{n}\\left(\\frac{\\overline{x}_n-\\mu}{\\sigma}\\right)\\) converges to a standard normal distribution.\nThe symbol \\(\\xrightarrow{d}\\) in the model means, “converges in distribution.”\nThere are many different types of intervals.\n\nrange (レンジ・範囲)\nprediction interval (予測区間)\ncredible interval (信用区間)\n\nThe confidence interval provides information about the precision of our estimate of the mean.\nThe confidence interval is also a range of observations defined by the quantile of a distribution. For the mean values, a normal distribution is usually assumed because mean values follow the central limit theorem.\nHow do we find the \\(l\\) and \\(u\\) quantiles?",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#determining-the-lower-and-upper-quantiles-of-n0-1",
    "href": "ttest-slides.html#determining-the-lower-and-upper-quantiles-of-n0-1",
    "title": "Comparing two groups",
    "section": "Determining the lower and upper quantiles of \\(N(0, 1)\\)",
    "text": "Determining the lower and upper quantiles of \\(N(0, 1)\\)\n\nNote that \\([-1 s, 1 s]\\) is the 68.3% interval, \\([-2 s, 2 s]\\) is the 95.4% interval, and \\([-3 s, 3 s]\\) is the 99.7% interval.\n\nThe 50%, 68.3%, 95%, and 99% confidence intervals are indicated by the shaded region.\nThe shaded area indicates the probability.\nAlong the x-axis are the z-scores or a multiple of the standard deviation.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#table-of-quantiles-for-n0-1",
    "href": "ttest-slides.html#table-of-quantiles-for-n0-1",
    "title": "Comparing two groups",
    "section": "Table of quantiles for \\(N(0, 1)\\)",
    "text": "Table of quantiles for \\(N(0, 1)\\)\n\n\n\n\n\nSignificance level, α\nConfidence interval (%)\n± quantile\n\n\n\n\n0.500\n50.000\n0.674\n\n\n0.317\n68.269\n1.000\n\n\n0.200\n80.000\n1.282\n\n\n0.100\n90.000\n1.645\n\n\n0.050\n95.000\n1.960\n\n\n0.046\n95.450\n2.000\n\n\n0.025\n97.500\n2.241\n\n\n0.003\n99.730\n3.000\n\n\n0.000\n99.994\n4.000",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#calculating-the-confidence-interval",
    "href": "ttest-slides.html#calculating-the-confidence-interval",
    "title": "Comparing two groups",
    "section": "Calculating the confidence interval",
    "text": "Calculating the confidence interval\nLet \\(x = \\overline{x}_{A-B} =\\) -2.217, \\(s = \\text{s.e.} =\\) 0.526, \\(\\sigma_A = \\sigma_B =\\) 1, and and \\(\\alpha = 0.05\\).\n\\[\nP\\left(l \\le \\frac{\\overline{x}-\\mu}{\\sigma}\\le u\\right) = 1-\\alpha = 0.95\n\\]\n\\[\nP\\left(\\overline{x} +l \\sigma \\le \\mu \\le \\overline{x} + u\\sigma\\right) = 1-\\alpha = 0.95\n\\]\nWhen \\(\\alpha= 0.05\\), the \\(l\\) and \\(u\\) quantiles are \\(l=\\) -1.96 and \\(u=\\) 1.96, and \\(\\sigma = 1\\).\n\\[\nP(\n-2.217 +  -1.96 \\times 1\n\\le x \\le\n-2.217 +  1.96 \\times 1\n)\n\\]\n\\[\nP(\n-4.177\n\\le x \\le\n-0.257\n) = 0.95\n\\]\nThe 95% confidence interval of \\(\\overline{x}=\\) -2.217 is \\([-4.177, -0.257 ]\\).",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#confidence-intervals-for-each-experiment-when-sigma-is-known",
    "href": "ttest-slides.html#confidence-intervals-for-each-experiment-when-sigma-is-known",
    "title": "Comparing two groups",
    "section": "Confidence intervals for each experiment when \\(\\sigma\\) is known",
    "text": "Confidence intervals for each experiment when \\(\\sigma\\) is known\n\n\\(\\sigma_A = \\sigma_B=\\) 1, \\(H_0:\\) \\(\\overline{x}_A = \\overline{x}_B\\) or \\(\\overline{x}_{A-B}=0\\)",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#how-often-does-the-confidence-interval-contain-0",
    "href": "ttest-slides.html#how-often-does-the-confidence-interval-contain-0",
    "title": "Comparing two groups",
    "section": "How often does the confidence interval contain 0?",
    "text": "How often does the confidence interval contain 0?\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe true difference is -2, therefore \\(H_0\\) is false.\nIf we do not reject \\(H_0\\), we are making a Type-II Error (第2種の誤り).\nThe 95% confidence intervals of 8 experiments include 0.\nThe error rate is \\(\\beta=\\) 8 / 20 = 0.4 or 40%.\nThe power of this analysis (\\(1 - \\beta\\)) is 0.6",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#we-made-some-wrong-assumptions",
    "href": "ttest-slides.html#we-made-some-wrong-assumptions",
    "title": "Comparing two groups",
    "section": "We made some wrong assumptions",
    "text": "We made some wrong assumptions\nThe z-score when population mean \\(\\mu\\) and population standard deviation \\(\\sigma\\) is known follows a standard normal distribution.\n\\[\nz = \\frac{\\overline{x} - \\mu}{\\sigma}\\sim N(0,1)\n\\] However, if you do not know the population standard deviation, we must calculate the t-value.\n\\[\nt_{\\overline{x}} = \\frac{\\overline{x} - x_0}{s.e.} = \\frac{\\overline{x} - x_0}{s / \\sqrt{n}}\n\\]\nWhich follows a t-distribution. \\(x_0\\) is a constant, and is often set to zero.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#determining-the-lower-and-upper-quantiles-of-td.f.",
    "href": "ttest-slides.html#determining-the-lower-and-upper-quantiles-of-td.f.",
    "title": "Comparing two groups",
    "section": "Determining the lower and upper quantiles of \\(t(d.f.)\\)?",
    "text": "Determining the lower and upper quantiles of \\(t(d.f.)\\)?\n\nNote that the degrees-of-freedom (d.f., 自由度) for the t-distribution is \\(N -1\\) = 5.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#table-of-quantiles-for-td.f.-5",
    "href": "ttest-slides.html#table-of-quantiles-for-td.f.-5",
    "title": "Comparing two groups",
    "section": "Table of quantiles for \\(t(d.f. = 5)\\)",
    "text": "Table of quantiles for \\(t(d.f. = 5)\\)\n\n\n\nQuantiles of the t distribution for d.f. = 5.\n\n\n\n\n\n\n\nSignificance level, \\(\\alpha\\)\nConfidence interval (%)\n\\(\\pm\\) quantile\n\n\n\n\n0.500\n50.000\n0.727\n\n\n0.363\n63.678\n1.000\n\n\n0.200\n80.000\n1.476\n\n\n0.102\n89.806\n2.000\n\n\n0.100\n90.000\n2.015\n\n\n0.050\n95.000\n2.571\n\n\n0.030\n96.990\n3.000\n\n\n0.025\n97.500\n3.163\n\n\n0.010\n98.968\n4.000",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#confidence-intervals-for-each-experiment-when-sigma-is-unknown",
    "href": "ttest-slides.html#confidence-intervals-for-each-experiment-when-sigma-is-unknown",
    "title": "Comparing two groups",
    "section": "Confidence intervals for each experiment when \\(\\sigma\\) is unknown",
    "text": "Confidence intervals for each experiment when \\(\\sigma\\) is unknown\n\n\\(H_0:\\) \\(\\overline{x}_A = \\overline{x}_B\\) or \\(\\overline{x}_{A-B}=0\\)\n\nThis figure shows 20 sample means and their confidence intervals. The dashed horizontal line indicates zero and the solid horizontal line indicates the true difference 0.\nThe dots indicate the sample mean and the vertical lines indicate the range of the 95% confidence interval.\nThe green symbols indicates that the interval includes the true mean.\nOut of 20 trials, the 95% confidence included the true difference 19 times.\nWhat does the confidence interval mean?\nA 95% confidence interval implies that after 100 experiments, about 95 of the confidence intervals will include the true mean.\n\nIt does not mean that the true mean is in the interval.\nIt does not mean that there is a 95% chance that the true mean in in the interval, since the size of the confidence interval varies with the sample.\nIt also does not mean that there is a 95% chance that the mean of the next experiment will be in the confidence interval.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#how-often-does-the-confidence-interval-contain-0-1",
    "href": "ttest-slides.html#how-often-does-the-confidence-interval-contain-0-1",
    "title": "Comparing two groups",
    "section": "How often does the confidence interval contain 0?",
    "text": "How often does the confidence interval contain 0?\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe true difference is -2, therefore \\(H_0\\) is false.\nIf we do not reject \\(H_0\\), we are making a Type-II Error (第2種の誤り).\nThe 95% confidence intervals of 5 experiments include 0. So for 5 experiments, we do not reject \\(H_0\\).\nThe error rate is \\(\\beta=\\) 5 / 20 = 0.25 or 25%.\nThe power of this analysis (\\(1 - \\beta\\)) is 0.75\n\n\n\n\nThese are informal ways of testing differences, howerver in practice we use more sophisticated formalized methods.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#the-paired-t-test",
    "href": "ttest-slides.html#the-paired-t-test",
    "title": "Comparing two groups",
    "section": "The paired t-test",
    "text": "The paired t-test\n\nType-II error rate \\(\\beta\\) = 5 / 20 = 25% and power (\\(1-\\beta\\)) is 0.75.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#t-test-unpaired-assuming-unequal-variance",
    "href": "ttest-slides.html#t-test-unpaired-assuming-unequal-variance",
    "title": "Comparing two groups",
    "section": "t-test (unpaired assuming unequal variance)",
    "text": "t-test (unpaired assuming unequal variance)\n\nType-II error rate \\(\\beta\\) = 1 / 20 = 5% and power (\\(1-\\beta\\)) is 0.95.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#the-null-hypothesis-of-the-t-test",
    "href": "ttest-slides.html#the-null-hypothesis-of-the-t-test",
    "title": "Comparing two groups",
    "section": "The null hypothesis of the t-test",
    "text": "The null hypothesis of the t-test\n\\(H_0\\) null hypothesis (帰無仮説): \\(\\overline{x}_A - \\overline{x}_B = \\overline{x}_{A-B}=0\\)",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#paired-t-test",
    "href": "ttest-slides.html#paired-t-test",
    "title": "Comparing two groups",
    "section": "Paired t-test",
    "text": "Paired t-test\nPaired t-test (対応ありのt検定)\nWe need to calculate the t-value, which is the statistic for the t-test.\n\\[\nt^* = \\frac{\\overline{x}_{A-B} - \\mu}{s_{A-B} / \\sqrt{n}}\n\\]\nAnd determine the degrees-of-freedom (自由度) which is \\(n-1\\).\nUsed when observations can be paired. For example the length of the left and right fin of a fish.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#independent-two-sample-t-test",
    "href": "ttest-slides.html#independent-two-sample-t-test",
    "title": "Comparing two groups",
    "section": "Independent two sample t-test",
    "text": "Independent two sample t-test\nThere are two versions.\n\n\nEqual variance (等分散)\n\\[\nt^* = \\frac{\\overline{x}_A - \\overline{x}_B}{s_p \\sqrt{1 / n_A + 1/n_B}}\n\\] \\[\ns_p = \\sqrt{\n\\frac{(n_A-1)s_A^2 + (n_B-1)s_B^2}\n{n_A + n_B -2}}\n\\] Degrees-of-freedom is \\(n_A + n_B - 2\\).\n\nUnequal variance, Welch’s t-test (ウェルチのt検定)\n\\[\nt^* = \\frac{\\overline{x}_A - \\overline{x}_B}{s_p}\n\\]\n\\[\ns_p = \\sqrt{\n\\frac{s_A^2}{n_A} +\n\\frac{s_B^2}{n_B}}\n\\] Degrees-of-freedom is calculated with the Welch-Satterthwaite Equation.\n\n\n\\(s\\) is the sample standard deviation. \\(n\\) is the number of samples. \\(\\overline{x}\\) is the mean. \\(t^*\\) is the t-value.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#welch-satterthwaite-equation",
    "href": "ttest-slides.html#welch-satterthwaite-equation",
    "title": "Comparing two groups",
    "section": "Welch-Satterthwaite Equation",
    "text": "Welch-Satterthwaite Equation\n\\[\n\\text{degrees-of-freedom} =\n\\frac{\n\\left(\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}\\right)^2\n}\n{\\frac{\\left(s_A^2 / n_A\\right)^2}{n_A-1} + \\frac{\\left(s_B^2 / n_B\\right)^2}{n_B-1}}\n\\]",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#the-unpaired-t-test",
    "href": "ttest-slides.html#the-unpaired-t-test",
    "title": "Comparing two groups",
    "section": "The unpaired t-test",
    "text": "The unpaired t-test\n\n\n\\[\n\\begin{aligned}\nt^* &= \\frac{\\overline{x}_{A-B} - \\mu}{s_{A-B} / \\sqrt{n}} \\\\\nt^* &= \\frac{-2.467}{2.642 / \\sqrt{6}} \\\\\nt^* &= -2.287\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\overline{x}_{A-B}=\\) -2.467\n\\(\\mu=0\\)\n\\(n\\) = 6\n\\(s_{A-B}=\\) 2.642\n\n\n\n\\(\\alpha\\) = 0.05\nt-value: -2.287\nOne-sided P-value: 0.035\nTwo-sided P-value: 0.071\n\n\n\nThe juvenile S. macrocarpum size observations cannot be paired, so this is the wrong test.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#the-correct-test-is-welchs-t-test",
    "href": "ttest-slides.html#the-correct-test-is-welchs-t-test",
    "title": "Comparing two groups",
    "section": "The correct test is Welch’s t-test",
    "text": "The correct test is Welch’s t-test\n\n\n\\[\n\\begin{aligned}\nt^* &= \\frac{\\overline{x}_A -\\overline{x}_B}{s_p} \\\\\ns_p &= \\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}} \\\\\ns_p &= \\sqrt{\\frac{1.995^2}{6} + \\frac{1.961^2}{6}} \\\\\nt^* &= \\frac{10.05 - 12.517}{1.142} \\\\\nt^* &= -2.16 \\\\\n\\text{d.f.} &= 9.997\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha\\) = 0.05\nt-value: -2.16\nOne-sided P-value: 0.028\nTwo-sided P-value: 0.056\n\n\n\nThe P-value decreases, but \\(0.056 \\ge \\alpha= 0.05\\). We can’t reject \\(H_0\\).",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#behavior-of-the-t-test-equal-variance",
    "href": "ttest-slides.html#behavior-of-the-t-test-equal-variance",
    "title": "Comparing two groups",
    "section": "Behavior of the t-test (equal variance)",
    "text": "Behavior of the t-test (equal variance)\n\nIncreasing the number of observations decrease the Type-II error rate and increases the power of the test. The Type-I error rate is fixed at \\(\\alpha=0.05\\).\n\nThe true mean (\\(\\mu\\)) is for site A and B is 20 and 22, respectively. The true standard deviation (\\(\\sigma\\)) for site A and B is 1 and 1, respectively.\nFor each sample size of {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 142, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, and 16} a total of 2000 simulated samples were created.\nWhen the sample size (\\(N\\)) is small, the Type-II error rate is high and the power of the t-test is low.\nIncreasing the sample size increases the power of the t-test and decreases the Type-II error rate.\nWhen the \\(N\\) = 6, the Type-II error rate is 0.138 and the power of the t-test is 0.862.\nWhen the \\(N\\) = 10, the Type-II error rate is 0.011 and the power of the t-test is 0.99.\nThe Type-I error rate is predefined and does not change with the number of observations.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#behavior-of-the-t-test-unequal-variance",
    "href": "ttest-slides.html#behavior-of-the-t-test-unequal-variance",
    "title": "Comparing two groups",
    "section": "Behavior of the t-test (unequal variance)",
    "text": "Behavior of the t-test (unequal variance)\n\nUnbalanced variances (\\(s^2\\)) increase the risk of a Type-II error rate (\\(\\beta\\)) and decrease the power (\\(1-\\beta\\)) of the t-test. The Type-I error rate is fixed at \\(\\alpha=0.05\\).\n\nThe true mean (\\(\\mu\\)) is for site A and B is 20 and 22, respectively. The true standard deviation (\\(\\sigma\\)) for site A is 1 and for site B is \\(k\\times\\sigma_B\\), respectively. \\(k\\) is a multiplier.\nWhen the standard deviations (variances) are not equal, the Type-II error rate and power of the t-test varies.\nWhen \\(s_A / s_B \\rightarrow\\infty\\), the Type-II error rate increases and the power decreases.\nWhen the sample size is high, the t-test is less sensitive to unequal variances.\nThe Type-I error rate is predefined and does not change with the number of observations.",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "ttest-slides.html#welchs-t-test-r-code",
    "href": "ttest-slides.html#welchs-t-test-r-code",
    "title": "Comparing two groups",
    "section": "Welch’s t-test R code",
    "text": "Welch’s t-test R code\n\nCodeOutput\n\n\n\nlibrary(tidyverse)\nA = c(9.8,11.1,10.7,10.7,11.8,6.2)\nB = c(12.5,13.8,12.0,15.5,9.8,11.5)\ndata = tibble(A, B)\ndata = data %&gt;% pivot_longer(cols = c(A,B))\nt.test(value ~ name, data = data) # ウェルチ t 検定\n# t.test(A, B) # Alternative method\n\n# two-sample, equal variance t-test (等分散 t 検定)\n# t.test(value ~ name, data = data, var.equal = TRUE) \n\nWelch’s t-test does not require equal variances or equal sample size.\nThe two-sample t-test requires equal variances.\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  value by name\nt = -2.16, df = 9.9971, p-value = 0.05612\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -5.01124979  0.07791646\nsample estimates:\nmean in group A mean in group B \n       10.05000        12.51667",
    "crumbs": [
      "基礎統計学用",
      "2群の分析（t 検定）"
    ]
  },
  {
    "objectID": "piping.html",
    "href": "piping.html",
    "title": "パイプ演算子の詳細",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n中間のオブジェクトをつくりながら、処理をする。 中間のオブジェクトの結果は参照できるのが大きなメリット。 Rコードの伝統的な組み方。\n\ny = rnorm(50)\nx = seq_along(y)\nz = cbind(x,y)\nplot(z)\n\n\n\n\n\n\n\n\n処理内容を入れ子構造でも処理できます。 とても読みづらいが、中間オブジェクトを作る必要はない。\n\nplot(\n  cbind(\n    x = seq(1, 50),\n    y = rnorm(50)\n  )\n)\n\n\n\n\n\n\n\n\nパイプ演算子を用いて処理する方法は magrittr パッケージが誕生してからできるようになりました。 このときも中間オブジェクトを作る必要はないです。 パイプ演算子の左側にある結果を右側にある関数に流し込む仕組みです。\n\ntibble(y = rnorm(50)) |&gt; \n  mutate(x = seq_along(y)) |&gt; \n  plot()",
    "crumbs": [
      "その前に",
      "パイプ演算子の詳細"
    ]
  },
  {
    "objectID": "piping.html#解析の処理",
    "href": "piping.html#解析の処理",
    "title": "パイプ演算子の詳細",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n中間のオブジェクトをつくりながら、処理をする。 中間のオブジェクトの結果は参照できるのが大きなメリット。 Rコードの伝統的な組み方。\n\ny = rnorm(50)\nx = seq_along(y)\nz = cbind(x,y)\nplot(z)\n\n\n\n\n\n\n\n\n処理内容を入れ子構造でも処理できます。 とても読みづらいが、中間オブジェクトを作る必要はない。\n\nplot(\n  cbind(\n    x = seq(1, 50),\n    y = rnorm(50)\n  )\n)\n\n\n\n\n\n\n\n\nパイプ演算子を用いて処理する方法は magrittr パッケージが誕生してからできるようになりました。 このときも中間オブジェクトを作る必要はないです。 パイプ演算子の左側にある結果を右側にある関数に流し込む仕組みです。\n\ntibble(y = rnorm(50)) |&gt; \n  mutate(x = seq_along(y)) |&gt; \n  plot()",
    "crumbs": [
      "その前に",
      "パイプ演算子の詳細"
    ]
  },
  {
    "objectID": "piping.html#magrittr-と-native-pipe-の違い",
    "href": "piping.html#magrittr-と-native-pipe-の違い",
    "title": "パイプ演算子の詳細",
    "section": "magrittr と native pipe の違い",
    "text": "magrittr と native pipe の違い\ntidyverse は magrittr パッケージをデフォルトでつかていますが、 R4.1.0 からは R native のパイプ演算子が導入されました。\n\nmagrittr / tidyverse デフォルトパイプ演算子： x %&gt;% f()\nR 4.1.0 native pipe : x |&gt; f()\n\nそれぞれのパイプは右辺の関数の扱いが違いので注意しましょう。\n\nx = rnorm(10)\ny = x * 5 + rnorm(10)\nz = data.frame(x, y)\n\nmagrittr パイプの場合は、 . を place holder として使えます。 つまり、左辺の値を第1引数以外の引数に渡せます。\n\nz %&gt;% lm(y ~ x, data = .)\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n     -0.207        4.636  \n\n\nnative パイプは第1引数以外の引数に渡せない。\n\nz |&gt; lm(y ~ x, data = . )\n\nError in eval(mf, parent.frame()): object '.' not found\n\n\nところが、次のように無名関数を用いて、native パイプでも magrittr パイプと同じ用なコードが作れます。\n\n# 括弧の組み方に注意\nz |&gt; (\\(dataset) lm(y ~ x, data = dataset))()\n\n\nCall:\nlm(formula = y ~ x, data = dataset)\n\nCoefficients:\n(Intercept)            x  \n     -0.207        4.636  \n\n\nUPDATE\nR 4.2.0 からは native パイプにも place holder が使えるようになりました。 native パイプ演算子の place holder は _ です。 ところが、必ず指定した引数名に _ を渡しましょう。\n\nz |&gt; lm(y ~ x, data = _)\n\n\nCall:\nlm(formula = y ~ x, data = z)\n\nCoefficients:\n(Intercept)            x  \n     -0.207        4.636",
    "crumbs": [
      "その前に",
      "パイプ演算子の詳細"
    ]
  },
  {
    "objectID": "piping.html#native-パイプをデフォルトに設定する",
    "href": "piping.html#native-パイプをデフォルトに設定する",
    "title": "パイプ演算子の詳細",
    "section": "Native パイプをデフォルトに設定する",
    "text": "Native パイプをデフォルトに設定する\nRStudio で R 4.1.0 の native パイプ演算子を使いたいなら、Tools/Global Options/Code の Use native pipe operator にチェックを入れましょう。\n\nmagick::image_read(\"Images/part00/globalops02.png\")",
    "crumbs": [
      "その前に",
      "パイプ演算子の詳細"
    ]
  },
  {
    "objectID": "openwater_nep_method.html",
    "href": "openwater_nep_method.html",
    "title": "開放型溶存酸素法",
    "section": "",
    "text": "[1] \"en_US.UTF-8\"",
    "crumbs": [
      "研究室用",
      "開放型溶存酸素法"
    ]
  },
  {
    "objectID": "openwater_nep_method.html#紹介したデータの結果",
    "href": "openwater_nep_method.html#紹介したデータの結果",
    "title": "開放型溶存酸素法",
    "section": "紹介したデータの結果",
    "text": "紹介したデータの結果\n\n\nCode\nfitdataset = function(dset) {\n  m1 = gam(mgl ~ s(H, k = 24, bs = \"cr\"), data = dset)\n  pdata = dset |&gt; add_fitted(model = m1, value = \"fit\")\n  rate = derivatives(m1, data = pdata, type = \"central\") |&gt;\n    pull(.derivative)\n  pdata = pdata |&gt; \n    mutate(rate = rate * depth) |&gt; \n    mutate(deltat = 24 / n()) |&gt; \n    mutate(mt = masstransfer(wind, temperature, 35, mgl, hpa, 1))\n  pdata  \n}\n\ndataset = dataset |&gt; \n  group_nest(date) |&gt; \n  mutate(data2 = map(data, fitdataset))\n\ndataset2 = dataset |&gt; \n  unnest(data2) |&gt; \n  mutate(nep = rate - mt) |&gt; \n  group_by(date) |&gt; \n  summarise(nep = sum(nep))\n\n\n\n\nCode\nylabel = \"NEP (g O&lt;sub&gt;2&lt;/sub&gt; m&lt;sup&gt;-2&lt;/sup&gt; d&lt;sup&gt;-1&lt;/sup&gt;)\"\n\nggplot(dataset2) + \n  geom_point(aes(x = date, y = nep)) +\n  geom_segment(aes(x = date, xend = date,\n                   y = 0, yend = nep)) +\n  scale_y_continuous(ylabel,\n                     limits = c(-40, 40)) + \n  scale_x_date(\"Date\", \n               breaks = \"7 days\") +\n  theme(axis.title.y = element_markdown())\n\n\n\n\n\n\n\n\nFigure 2: 一日あたりの生態系純一次生産量。",
    "crumbs": [
      "研究室用",
      "開放型溶存酸素法"
    ]
  },
  {
    "objectID": "tic_memo.html",
    "href": "tic_memo.html",
    "title": "TIC分析機器のメモ",
    "section": "",
    "text": "PV = nRT\n\n\nP : 圧力 Pressure (Pa)\nV : 体積 Volume (m3; 1000 L = 1 m3)\nn : 物質量 Amount of material (mol)\nR : モル気体定数 gas constant (8.314 J (mol K)-1)\nT : 絶対温度 Absolute temperature (K)",
    "crumbs": [
      "研究室用",
      "TIC分析機器のメモ"
    ]
  },
  {
    "objectID": "tic_memo.html#conversion-factor",
    "href": "tic_memo.html#conversion-factor",
    "title": "TIC分析機器のメモ",
    "section": "Conversion factor",
    "text": "Conversion factor\n\n1 J is 1 Pa m3.\n1000 L is m3.\n20 °C is 293.15 K.",
    "crumbs": [
      "研究室用",
      "TIC分析機器のメモ"
    ]
  }
]